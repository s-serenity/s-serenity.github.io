{"posts":[{"title":"第十九篇 深度学习(1)-梯度下降","text":"","link":"/2021/09/25/DL/"},{"title":"第十三篇 机器学习(2)-朴素贝叶斯","text":"朴素贝叶斯朴素贝叶斯（Naive Bayes）是一种基于贝叶斯定理的分类算法，被广泛用于文本分类和其他分类问题。它被称为”朴素”是因为它假设每个特征与其他特征之间都是相互独立的，这是一个较为简化的假设，但在实践中，朴素贝叶斯通常表现得相当好。 在朴素贝叶斯中，我们考虑一个分类问题，其中 A 是类别，而 B 是特征。贝叶斯定理用于计算给定特征的情况下某个类别的概率。我们可以使用训练数据中的频率估计概率，并计算每个类别的概率。然后，给定一个新的特征向量，我们可以使用贝叶斯定理计算每个类别的后验概率，并选择具有最高概率的类别作为预测结果。","link":"/2021/08/29/ML-1/"},{"title":"Elasticsearch","text":"ELK StackElasticsearch, Logstash and Kibana ElasticsearchElasticsearch is a NoSQL database.When you feed data into Elasticsearch, the data is placed into Apache Lucene indexes. Apache LuceneApache Lucene™ is a high-performance, full-featured search engine library written entirely in Java. APILogstashUsing more than 50 input plugins for different platforms, databases and applications, Logstash can be defined to collect and process data from these sources and send them to other systems for storage and analysis. projecthttps://trecpodcasts.github.io/https://doc.yonyoucloud.com/doc/mastering-elasticsearch/chapter-2/21_README.htmlhttps://cloud.tencent.com/developer/article/1600163https://www.elastic.co/cn/blog/how-to-improve-elasticsearch-search-relevance-with-boolean-querieshttps://www.elastic.co/guide/en/app-search/current/relevance-tuning-guide.htmlhttps://medium.com/mlearning-ai/enhancing-information-retrieval-via-semantic-and-relevance-matching-64973ff81818https://www.elastic.co/cn/blog/how-to-improve-elasticsearch-search-relevance-with-boolean-querieshttps://bigdataboutique.com/blog/optimizing-elasticsearch-relevance-a-detailed-guide-c9efd3NDCG：https://www.javatips.net/api/MyMediaLiteJava-master/src/org/mymedialite/eval/measures/NDCG.java","link":"/2023/04/02/Elasticsearch/"},{"title":"第十四篇 机器学习(3)-支持向量机","text":"支持向量机支持向量机（Support Vector Machine，SVM）是一种用于分类和回归分析的监督学习算法。它的主要目标是找到一个超平面，将数据集划分成两个类别，同时使得两个类别中距离最近的数据点到该超平面的距离最大化。这两个最近的数据点被称为支持向量。 SVM 可以使用核函数来处理非线性可分的数据。核函数可以将输入特征映射到高维空间，从而在高维空间中找到一个线性超平面来解决原始空间中的非线性问题。","link":"/2021/08/29/ML-2/"},{"title":"第十五篇 机器学习(4)-决策树","text":"决策树决策树是一种用于分类和回归问题的监督学习算法。它通过树状图的结构来表示和推断决策规则。每个内部节点表示一个特征或属性，每个分支代表一个决策规则，而每个叶节点表示一个类别标签或一个数值。 决策树的学习过程形成了一个递归的分治算法，其中每个节点都对应于一个特征，并且通过节点上的决策规则将数据集分割成更纯的子集。在决策树的构建过程中，选择最佳特征和分割数据的目标是提高每个节点的纯度，使得决策树在训练数据上达到最佳的拟合效果。","link":"/2021/08/29/ML-3/"},{"title":"第二十篇 机器学习(4)-uplift模型","text":"uplift模型uplift模型中文为增益模型，是工业界因果推断与机器学习结合最成熟的算法之一。传统的监督学习模型，往往是对输入x去预测一个y，而增益模型注重于x的变化对y的影响，以广告为例，传统的监督学习往往是给定这个特征去预测用户是否会点击，而增益模型注重的是给这个客户投放广告与否对客户是否购买广告商品所产生的影响。 因果推断因果推断是从观察到的数据中推断出变量之间的因果关系的过程。在统计学和数据科学中，因果推断涉及到尝试理解一个事件或行为是什么导致了另一个事件或行为。这与相关性或关联不同，因果推断试图确定一个变量的变化是否直接导致另一个变量的变化。","link":"/2021/09/26/ML-4/"},{"title":"第十二篇 机器学习(1)-逻辑回归","text":"逻辑回归逻辑回归（Logistic Regression）是一种用于解决二分类问题的监督学习算法，尽管名称中包含”回归”一词，但实际上它用于分类任务。逻辑回归使用一个假设函数（sigmoid函数），将输入特征的线性组合映射到一个在0和1之间的概率值。逻辑回归将概率值转换为二分类的决策，通常使用一个阈值（例如0.5）。逻辑回归使用交叉熵损失函数来衡量预测概率与实际标签之间的差异。损失函数的目标是最小化误差。","link":"/2021/08/29/ML/"},{"title":"第二十二篇 机器学习(5)--不平衡的分类问题","text":"什么是不平衡的分类问题在机器学习中，不平衡的分类问题指的是类别之间的样本分布不均匀，其中某一类的样本数量远远超过另一类。这种不平衡可能会对模型训练和性能评估产生影响，因为模型可能更倾向于预测样本数更多的类别，而对样本数较少的类别进行较差的预测。 如何解决为了解决不平衡分类问题，可以考虑以下方法：增加少数类别的样本数或减少多数类别的样本数，以平衡类别分布。这包括上采样（增加少数类别样本）和下采样（减少多数类别样本）。调整分类阈值，使模型更倾向于识别少数类别。这可以通过调整模型输出的概率阈值来实现。","link":"/2021/11/15/ML-5/"},{"title":"第一篇 Markdown","text":"第一篇 Markdown （Typora）基础语法目录标题篇级数个#加空格，如# 表示后面的文字作为一级标题，## 表示后面的文字作为二级标题。 字体篇加粗：将想要加粗的文字在左右用两个*号括起来 ： **加粗** 斜体：将想要斜体的文字在左右用一个*号括起来 ： 斜体加粗：将想要斜体的文字在左右用三个*号括起来 删除线：将想要加删除线的文字在左右用两个~号括起来波浪线 段落篇 生成目录：[toc]， 但是hexo上传博客用此方法显示目录不成功ORZ。 分割线：三个及以上的-或者*，试一试 无序列表：就是这个前面的小黑点，一个+-或者*加上空格，后面跟上文字。 有序列表：数字标号的。数字加点加上空格 这是第一个 这是第二个 插入类 插入图片：![图片下方的文字](图片地址 &quot;图片标题&quot;) 拿一张试试： 插入超链接：[超链接名称](超链接地址 &quot;标题&quot;) 试试：我的博客 插入公式：$$包含TeX 或 LaTeX 格式的数学公式。 插入表格：|分列，-分割表头和内容，左右两边加：文字居中，只加一边文字偏哪边，默认偏左。 举例 1234表头|表头 --|--内容|内容内容|内容 ​ 试一个：没有成功诶，需要在两端也加上|才会显示表格，可是把最后一行作为表头显示了，emmm，应该是编辑器导致的差异。不过Typora可以很方便地直接右击插入表格。。。 表头 表头 内容 内容 内容 内容 插入代码块 单行代码：一个反引号`括起来，反引号就是Esc下面那个键，记得换成英文输入法。 print('hello world!') 多行代码：三个反引号`括起来。 12import numpy as npimport pandas as pd ​","link":"/2020/03/14/Markdown/"},{"title":"My first blog","text":"This is my first blog which is just for test. Happy~I will present my learning process in this blog from today on. Fighting~","link":"/2018/12/21/My-first-blog/"},{"title":"NP and reduction","text":"PP is the set of all decision problems that can be solved inPolynomial time. Extended Church-Turing ThesisAny realistic model of computation can be efficientlysimulated by a Turing machine.(The ECT is probably false!Probable counter-example: Quantum computing,However, the ECT is true for the classic models of computationunderlying our laptops) NP problemWe do not know of polynomial-time algorithms for these problems, and we cannot prove that nopolynomial-time algorithm exists. NP stands for Nondeterministic Polynomial time. NP is a set of problems that there is a polynomial-time algorithm that verify if a solution to the problem is correct.Note that showing that a problem is in NP does not mean that the problem can be solved in polynomial time. It only means that a proposed solution can be verified in polynomial time. All problems in P are also in NP, we do not know if P=NP. NP-hardnessWe say that a problem Y is NP-hard if every problem in NP can be Karp-reduced to Y. To prove NP-hardness for Y we only have to findone other NP-hard problem X and reduce X to Y. NP-complete problemA large class of problems in this “gray area” has been characterized,and it has been proved that they are equivalent in the following sense: a polynomial-time algorithm for any one of them would imply the existence of a polynomial-time algorithm for all of them. These are the NP-complete problems. Every problem in NP can be reduced to X. We will call such an X an NP-complete problem. To prove a problem is np-complete, we need to prove it lies in Np and it is np-hard. In NP is proved by verifying the solution in polynomial time. Np-hard is proved by using Karp-reduction from known NP-hard problem. the NP-complete problems are the hardest problems in NP The Cook-Levin TheoremThe Sat problem is NP-hard. Hamiltonian CycleReduction from Sat to Hamiltonian Cycle. Travelling SalesmanReduction from Hamiltonian Cycle Graph ColoringGraph Coloring is NP-hard.2-coloring is not NP-hard. Vertex Coverwe say that a set of nodes S is a vertex cover if every edge e has at least one end in S. Set CoverGiven a set U of n elements, a collection S1,…, Sm of subsets of U, anda number k, does there exist a collection of at most k of these sets whoseunion is equal to all of U? CoNPThe complexity class CoNP consists of all problems where a “no” answer can be efficiently verified.For every NP-complete problem there is acorresponding CoNP-complete problem. We do not know if NP=CoNP. PSPACEThe complexity class PSPACE consists of all problems that canbe solved by an algorithm using at most a polynomial amount ofspace.It is strongly believed that NP != PSPACE, but we do not evenknow with certainty whether P != PSPACE or not! For many 2-player games like Geography, deciding if there is awinning strategy from a given position is a PSPACE problem BPP and ZPPBPPBPP (Bounded-error Probabilistic Polynomial-time)consists of all decision problems for which there is apolynomial-time randomized algorithms that is correct withprobability at least 2=3 on all instances. ZPPZPP (Zero-error Probabilistic Polynomial Time)consists of all decision problems for which there is a Las Vegasalgorithm running in expected polynomial time.It is widely believed that P = ZPP = BPP but all three could bedifferent.","link":"/2023/03/28/NP/"},{"title":"Network Representation Learning","text":"backgroundRecording studying during KTH. First blog about network representation learning, a project belongs to machine learning, advanced course. LINEReproduce paper “LINE: Large-scale Information Network Embedding”. Alias Table MethodIt’s a method of effiently drawing samples from discrete distribution.reference:https://www.keithschwarz.com/darts-dice-coins/https://blog.csdn.net/haolexiao/article/details/65157026 Negative Samplingword2vecOriginal paper:Efficient estimation of word representations in vector space.reference:word2vec Explained: Deriving Mikolov et al.’sNegative-Sampling Word-Embedding Method Skip-Gram ModelOriginal papaer:Distributed Representations of Words and Phrasesand their Compositionality.The idea behind the word2vec models is that the words that appear in the same context (near each other) should have similar word vectors. Therefore, we should consider some notion of similarity in our objective when training the model. This is done using the dot product since when vectors are similar, their dot product is larger.reference:https://www.baeldung.com/cs/nlps-word2vec-negative-sampling graphSage","link":"/2022/12/23/Network-Representation-Learning/"},{"title":"Neural Networks","text":"training nerual networkssuggestionsThe first step to training a neural net is to not touch any neural net code at all and instead begin by thoroughly inspecting your data. Our next step is to set up a full training + evaluation skeleton and gain trust in its correctness via a series of experiments. At this stage it is best to pick some simple model that you couldn’t possibly have screwed up somehow. Tips &amp; tricks for this stage:Always use a fixed random seed to guarantee that when you run the code twice you will get the same outcome.Make sure to disable any unnecessary fanciness. As an example, definitely turn off any data augmentation at this stage.When plotting the test loss run the evaluation over the entire (large) test set. Do not just plot test losses over batches and then rely on smoothing them in Tensorboard. We are in pursuit of correctness and are very willing to give up time for staying sane.Monitor metrics other than loss that are human interpretable and checkable (e.g. accuracy). Whenever possible evaluate your own (human) accuracy and compare to it.Train an input-independent baseline, (e.g. easiest is to just set all your inputs to zero). This should perform worse than when you actually plug in your data without zeroing it out. Does it? i.e. does your model learn to extract any information out of the input at all? The approach I like to take to finding a good model has two stages: first get a model large enough that it can overfit (i.e. focus on training loss) and then regularize it appropriately (give up some training loss to improve the validation loss). The reason I like these two stages is that if we are not able to reach a low error rate with any model at all that may again indicate some issues, bugs, or misconfiguration. In the early stages of setting baselines I like to use Adam with a learning rate of 3e-4. In my experience Adam is much more forgiving to hyperparameters, including a bad learning rate. referenceshttp://karpathy.github.io/2019/04/25/recipe/","link":"/2023/07/20/Neural-Networks/"},{"title":"PCA","text":"关于PCA为什么要中心化因为不做zero mean，根本做不了PCA。从线性变换的本质来说，PCA就是在线性空间做一个旋转（数据矩阵右乘协方差矩阵的特征向量矩阵），然后取低维子空间（实际上就是前ncomponents个特征向量张成的子空间）上的投影点来代替原本的点，以达到降维的目的，注意我说的，只做了旋转，没有平移，所以首先你要保证原本空间里的点是以原点为中心分布的，这就是zero mean的目的。另外如果自己手撸过PCA的算法就知道了，explained_variance和explainedvariance_ratio是怎么实现的？explainedvariance就是协方差矩阵的每个特征值除以sample数，而explained_variance_ratio是每个特征值除以所有特征值之和。为什么这么简单呢？这也和zero mean有关，如果你用最大投影长度的证法去证明PCA就会在过程中很自然的发现这一点，在这里我就不展开了。 链接：https://www.zhihu.com/question/40956812/answer/848527057 PCA 去中心化不一定是必需的，这一结论成立的前提是严格使用协方差矩阵的定义式 S=XXT−nμμT，而不是用 XXT 来当作协方差矩阵。 Centering is an important pre-processing step because it ensures that the resulting components are only looking at the variance within the dataset, and not capturing the overall mean of the dataset as an important variable (dimension). Without mean-centering, the first principal component found by PCA might correspond with the mean of the data instead of the direction of maximum variance. https://towardsdatascience.com/tidying-up-with-pca-an-introduction-to-principal-components-analysis-f876599af383 Why is minimizing squared residuals equivalent to maximizing variance? Consider a datapoint (row of ). Then the contribution of that datapoint to the variance is , or equivalently the squared Euclidean length . Applying the Pythagorean theorem shows that this total variance equals the sum of variance lost (the squared residual) and variance remaining. Thus, it is equivalent to either maximize remaining variance or minimize lost variance to find the principal components. http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/ PCA can only be interpreted as the singular value decomposition of a data matrix when the columns have first been centered by their means. https://stats.stackexchange.com/questions/29781/when-conducting-multiple-regression-when-should-you-center-your-predictor-varia PCA focuses on “explaining” the data matrix using the sample means plus the eigencomponents. When the column mean is far from the origin, the first right singular value is usually quite highly correlated with column mean - thus using PCA concentrates on the second, third and sometimes higher order singular vectors. This is a loss of information when the mean is informative for the process under study. On the other hand, when the scatterplot of the data is roughly elliptical, the PCs typically align with the major axes of the ellipse. Due to the uncorrelatedness constraint, if the mean is far from the origin, the first singular vector will be close to the mean and the others will be tilted away form the major axes of the ellipse. Thus the first singular vector will not be informative about the spread of the data, and the second and third singular values will not be in the most informative directions. Generally, PCA will be more informative, particularly as a method for plotting the data, than uncentered SVD. https://online.stat.psu.edu/stat555/node/94/ Since X is zero centered we can think of them as capturing the spread of the data around the mean in a sense reminiscent of PCA. https://intoli.com/blog/pca-and-svd/ that reconstruction error is minimized by taking as columns of W some k orthonormal vectors maximizing the total variance of the projection. https://stats.stackexchange.com/questions/130721/what-norm-of-the-reconstruction-error-is-minimized-by-the-low-rank-approximation PCA is a regressional model without intercept1. Thus, principal components inevitably come through the origin. If you forget to center your data, the 1st principal component may pierce the cloud not along the main direction of the cloud, and will be (for statistics purposes) misleading.https://stats.stackexchange.com/questions/22329/how-does-centering-the-data-get-rid-of-the-intercept-in-regression-and-pca Centering brings in a big difference. PCA with centering maximizes SS deviations from the mean (i.e. variance); PCA on raw data maximizes SS deviations from the zero point.https://stats.stackexchange.com/questions/489037/principal-components-with-and-without-centering?noredirect=1&amp;lq=1 SVD and PCA https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca singular value decomposition SVD is basically a matrix factorization technique, which decomposes any matrix into 3 generic and familiar matrices. Eigenvalues and Eigenvectors The concept of eigenvectors is applicable only for square matrices. The vector space spanned by an eigenvector is called an eigenspace. A square matrix is called a diagonalizable matrix if it can be written in the format: $ A=PDP^{-1} $, D is the diagonal matrix comprises of the eigenvalues as diagonal elements A Symmetric Matrix where the matrix is equal to the transpose of itself. Special properties of a Symmetric Matrix with respect to eigenvalues and eigenvectors:Has only Real eigenvalues;Always diagonalizable;Has orthogonal eigenvectors. A matrix is called an Orthogonal Matrix if the transpose of the matrix is the inverse of that matrix.ince the eigenvectors of a Symmetric matrix are orthogonal to each other, matrix P in the diagonalized matrix A is an orthogonal matrix. So we say that any Symmetric Matrix is Orthogonally Diagonalizable. https://towardsdatascience.com/singular-value-decomposition-and-its-applications-in-principal-component-analysis-5b7a5f08d0bd % For the PCA derived from maximal preserved variance \\cite{lee2007nonlinear}, we have the covariance% of $\\mathbf{y}$, which is% \\begin{equation}% \\mathbf{C}_{\\mathbf{y} \\mathbf{y}}=E\\left{\\mathbf{y} \\mathbf{y}^T\\right}% \\end{equation}% This equation is valid only when $\\mathbf{y}$ is centered. The goal of PCA is to maximize the variance of the data along each of the principal components. Centering is an important step because it ensures that the resulting components are only looking at the variance of features, and not capturing the means of the features as important. Without mean-centering, the first principal component found by PCA might correspond with the mean of the data instead of the direction of maximum variance.","link":"/2022/12/29/PCA/"},{"title":"第十七篇 强化学习(1)-马尔可夫决策过程","text":"马尔可夫决策过程马尔可夫性质：当前状态可以完全表征过程。 对于任意有限的马尔可夫决策过程，都存在一个最优策略，不差于其他所有可能的策略。 贝尔曼方程","link":"/2021/09/02/RL/"},{"title":"Randomized Algorithms","text":"Randomized Algorithms and approximation algorithmA randomized algorithm is an algorithm that uses randomness tomake some of its choices. Las Vegas AlgorithmsA Las Vegas algorithm is a randomized algorithm that always findsthe correct answer, but the running time of the algorithm mightvary significantly depending on the random choices made. Monte Carlo AlgorithmsA Monte Carlo algorithm is a randomized algorithm where theoutput of the algorithm may be incorrect, but we have a guaranteethat this only happens with small probability. Random Number Generator True RNG: gets randomness from measuring physical phenomena(e.g. background radiation) that we believe is sufficiently randomPseudo-RNG: starting from a small random seed, new numbers aregenerated in a completely deterministic way. Randomized Min-Cut Algorithmwhile G has more than 2 vertices:Pick a uniformly random edge of G and contract it.the total runtime is $O(n^2m)$. However this algorithm can be refined and running time improved to $O(n^2log(n))$ (relatively simple algorithm) or$O(m log^3(n))$ (more complicated algorithm). approximation algorithmThe Maximum Cut ProblemPartition of vertices of G into two non-empty sets A and B such that number of edges between A and B is maximized. It is a NP-hard problem. Let us lower the ambition and try to find a fast algorithm that finds a “reasonably good” cut: Random algorithm:Put each vertex in either A or B independently with probability 1/2.The random assignment algorithm cuts (in expectation) at least half of all the edges in the graph.This is an example of an approximation algorithm. finding “good but maybe not optimal” solutionsHeuristic AlgorithmsWork well in some or even many cases, but with no guaranteesabout how well they perform. Approximation AlgorithmsAlgorithms with provable guarantee that the solution found is relatively good compared to the optimum solution, for all instances.For a minimization problem, an algorithm has approximation ratio $\\alpha ≥ 1$ if for every instance it holds that $ Alg ≤ \\alpha Opt$.For a maximization problem, the inequality goes the other way: we have approximation ratio $\\alpha ≤ 1$ if $ Alg ≥ \\alpha Opt$. alpha-approximation algorithmapproximation ratio: $\\alpha$. Minimum Load BalancingNP-hard problem. Given Lengths t1; : : : ; tn of n jobs to be run on m machines, to find Minimum possible makespan of a schedule for the n jobs.Approximation Algorithm for Load Balancing:Assign job i to the machine j with the smallest load. how to prove itThe main difficulty in analyzing approximationalgorithms is to get some handle on the Opt value. We need to find Opt, but finding Opt is NP-hard, so we find lower bounds on Opt. Minimum Vertex CoverA vertex cover in a graph G is a set of vertices that “touches” every edge of G.What is the size of a minimum vertex cover of G? Approximation Algorithm for Minimum Vertex Cover:while there exists an edge e = (u; v) such that u /∈ S and v /∈ S, add (u,v) into S. The algorithm is a 2-approximation algorithm.“Unique Games Conjecture”: it is known that Vertex Cover cannot be approximated better thanwithin a factor 2. Minimum Set CoverA collection of sets S1; : : : ; Sm ⊆ U over some universe U, What is minimum number of Si’s whose union equals U? Analysis of Greedy Set Cover Finale?","link":"/2023/04/25/Randomized-Algorithms/"},{"title":"Regular Expressions","text":"Regular expressionsA formal language for specifying text strings rulesDisjunctions:Letters inside square brackets[]: [A-Z]pipe |: a|b|cNegation in Disjunction: Ss ?: When placed after a character or a group, the question mark makes it optional, meaning that the character or group can occur zero or one time.When placed after a quantifier, such as , +, or ?, it modifies the quantifier to be non-greedy or lazy. A non-greedy quantifier matches as few characters as possible, while a greedy quantifier matches as many characters as possible. :0 or more of previous char+:1 or more of previous char.:any char Anchors:^: The begining. $: The end.","link":"/2023/07/11/Regular-Expressions/"},{"title":"第十篇 SQL(1)-数据统计","text":"","link":"/2021/08/07/SQL/"},{"title":"Turing Machines and Undecidability","text":"mathematical model of computationTuring MachinesTuring machines are primarily a mathematical construction. A Turing machine (TM) consists of:A finite set of states: initial state(when the TM starts, it is in this state) and accepting states(if the TM ends in one of these states, it says yes); Transition rules that determine what to do based on current state and content of memory tape at current position; An alphabet, the set of symbols that can be stored on the memory tape. The Church-Turing HypothesisAnything which can be computed by any automatic method, can also be computed by a Turing machine. If a model can also do everything that a Turing machine can, then it is called a Turing-complete model of computation. The RAM ModelModels the way modern computers operate, with registers and memory cells that can be immediately accessed. Comparison-Based SortingWe will prove that any such algorithm must make Ω(n log n)comparisons (and thus take Ω(n log n) time) Undecidabilitydecision problemsThe answer is just a single bit of information,“yes” or “no”.A decision problem is decidable if there exists an algorithm for solving the problem without any efficiency considerations. The Halting ProblemTuring machine M, and input x to M. Does M halt when run on the input x?The Halting Problem is undecidable. Halting On Empty InputIt is undecidable. Halting on All InputsIt is undecidable. Recursively Enumerable ProblemsThere exists an “algorithm” which terminates whenever the answer is“yes” but does not terminate on “no” instances.Problems which have algorithms like this are called recursively enumerable. Turing reductionA Turing reduction (also called Cook reduction) from a problemX to a problem Y is an efficient algorithm for problem X which isallowed to assume an algorithm for Y as a black box. Vertex CoverIndependent SetSet Cover3-SatKarp ReductionsX ≤p Y if given any instance A of problem X, we can inpolynomial time construct an instance f (A) of problem Y such thatthe answer to A is the same as the answer to f (A). Turing reduction is a type of reduction that is stronger than Karp reduction. A problem A is Turing-reducible to problem B if there exists a Turing machine that can solve problem A by making a polynomial number of calls to a black-box subroutine for problem B. In other words, if we can use an algorithm for problem B to solve problem A, then A is Turing-reducible to B. Turing reduction preserves the complexity class of a problem, meaning that if A is Turing-reducible to B and B is in a certain complexity class, then A is also in that complexity class. Karp reduction, also known as polynomial-time reduction, is a weaker form of reduction than Turing reduction. A problem A is Karp-reducible to problem B if there exists a polynomial-time algorithm that can transform any instance of problem A into an instance of problem B such that the answer to the transformed instance is the same as the answer to the original instance of problem A. In other words, if we can use an algorithm for problem B to solve a transformed version of problem A in polynomial time, then A is Karp-reducible to B. Karp reduction preserves the complexity class up to polynomial factors, meaning that if A is Karp-reducible to B and B is in a certain complexity class, then A is also in that complexity class up to polynomial factors. Good referencehttps://www.cs.rochester.edu/u/nelson/courses/csc_173/computability/undecidable.htmlhttp://www2.lawrence.edu/fast/GREGGJ/CMSC515/chapt05/Reducibility.htmlhttps://www.cs.princeton.edu/courses/archive/spring05/cos423/lectures.php","link":"/2023/04/25/Turing-Machines/"},{"title":"bigdata - Distributed file systems","text":"Distributed file systemsrequirements of a distributed file systemGoing back to our capacity-throughput-latency view of storage, a distributed file system is designed so that, in cruise mode, its bottleneck will be the data flow (throughput), not the latency. We saw that capacity increased much faster than throughput, and that this can be solved with parallelism. We saw that throughput increased much faster than latency decreased, and that this can be solved with batch processing. Distributed file systems support both parallelism and batch processing natively, forming the core part of the ideal storage system accessed by MapReduce or Apache Spark.The origins of such a system come back to the design of GoogleFS, the Google File System. Later on, an open source version of it was released as part of the Hadoop project, initiated by Doug Cutting at Yahoo, and called HDFS, for Hadoop Distributed File System. HDFSHDFS does not follow a key-value model: instead, an HDFS cluster organizes its files as a hierarchy, called the file namespace. Files are thus organized in directories, similar to a local file system. Unlike in S3, HDFS files are furthermore not stored as monolithic blackboxes, but HDFS exposes them as lists of blocks. As for the block size: HDFS blocks are typically 64 MB or 128 MB large, and are thus considerably larger than blocks on a local hard drive (around 4 kB).HDFS is designed to a run on a cluster of machines. architectureHDFS is implemented on a fully centralized architecture, in which one node is special and all others are interchangeable and connected to it.In the case of HDFS, the central node is called the NameNode and the other nodes are called the DataNodes. Every file is divided into chunks called blocks. All blocks have a size of exactly 128 MB, except the last one which is usually smaller. Each one of the blocks is then replicated and stored on several DataNodes. How many times? This is a parameter called the replication factor. By default, it is 3. The NameNode is responsible for the system-wide activity of the HDFS cluster. It store in particular three things: • the file namespace, that is, the hierarchy of directory names and file names, as well as any access control (ACL) information similar to Unix-based systems. • a mapping from each file to the list of its blocks. Each block, in this list, is represented with a 64-bit identifier; the content of the blocks is not on the NameNode. • a mapping from each block, represented with its 64-bit identifier, to the locations of its replicas, that is, the list of the DataNodes that store a copy of this block. The DataNodes store the blocks themselves. These blocks are stored on their local disks. DataNodes send regular heartbeats to the NameNode. The frequency of these heartbeats is configurable and is by default a few seconds (e.g., 3s, but this value may change across releases). This is a way to let the NameNode know that everything is alright.Finally, the DataNode also sends, every couple of hours (e.g., 6h, but this value may change across releases), a full report including all the blocks that it contains. A NameNode never initiates a connection to a DataNode. Finally, DataNodes are also capable of communicating with each other by forming replication pipelines. A pipeline happens whenever a new HDFS file is created. The client does not send a copy of the block to all the destination DataNodes, but only to the first one. This first DataNode is then responsible for creating the pipeline and propagating the block to its counterparts. When a replication pipeline is ongoing and a new block is being written to the cluster, the content of the block is not sent in one single 128 MB packet. Rather, it is sent in smaller packets (e.g., 64 kB) in a streaming fashion via a network protocol. replicasHaving this in mind, the first replica of the block, by default, gets written to the same machine that the client is running on. The second replica is written on a DataNode sitting in a different rack than the client, that we call B. The third replica is written to another DataNode on the same rack B.And further replicas are written mostly at random, but respecting two simple rules for resilience: at most one replica per node, and at most two replicas per rack. Fault toleranceHDFS has a single point of failure: the NameNode. If the metadata stored on it is lost, then all the data on the cluster is lost, because it is not possible to reassemble the blocks into files any more.For this reason, the metadata is backed up. More precisely, the file namespace containing the directory and file hierarchy as well as the mapping from files to block IDs is backed up to a so-called snapshot. What is done is that updates to the file system arriving after the snapshot has been made are instead stored in a journal, called edit log, that lists the updates sorted by time of arrival. The snapshot and edit log are stored either locally or on a networkattached drive (not HDFS itself). Logging and importing dataTwo tools are worth mentioning: Apache Flume lets you collect, aggregate and move log data to HDFS. Apache Sqoop lets you import data from a relational database management system to HDFS. referenceshttps://ghislainfourny.github.io/big-data-textbook/https://hadoop.apache.org/docs/r3.3.0/hadoop-project-dist/hadoop-hdfs/HdfsDesign.html#Data_Replication","link":"/2023/09/19/bigdata2/"},{"title":"bigdata - introduction","text":"big dataBig Data is a portfolio of technologies that were designed to store, manage and analyze data that is too large to fit on a single machine while accommodat-ing for the issue of growing discrepancy between capacity, throughput and latency. data independenceData independence means that the logical view on the data is cleanly separated, decoupled, from its physical storage. architectureStack: storage, compute, model ,language data modelwhat data looks like and what you can do with it. data shapestables, trees, cubes tableRow(Tuple), Column(Attribute), Primary Key,Value. relational tablesschema: A set of attributes.extension: A set/bag/list of tuples.Three constraints: Relational integrity, domain integrity and atomic integrity.Superkey, Candidate key(minimal superkey). Database NormalizationIn database management systems (DBMS), normal forms are a series of guidelines that help to ensure that the design of a database is efficient, organized, and free from data anomalies.First Normal Form (1NF):In 1NF, each table cell should contain only a single value, and each column should have a unique name.Second Normal Form (2NF): 2NF eliminates redundant data by requiring that each non-key attribute be dependent on the primary key.Third Normal Form (3NF): 3NF builds on 2NF by requiring that all non-key attributes are independent of each other. DenormalizationDenormalization is a database optimization technique in which we add redundant data to one or more tables. This can help us avoid costly joins in a relational database. Note that denormalization does not mean ‘reversing normalization’ or ‘not to normalize’. It is an optimization technique that is applied after normalization. SQLSQL was originally named SEQUEL, for Structured English QUEry Language. SQL is a declarative language, which means that the user specifies what they want, and not how to compute it: it is up to the underlying system to figure out how to best execute the query. View and TableThe view is a result of an SQL query and it is a virtual table, whereas a Table is formed up of rows and columns that store the information of any object and be used to retrieve that data whenever required. A view contains no data of its own but it is like a ‘window’ through which data from tables can be viewed or changed. The view is stored as a SELECT statement in the data dictionary. Creating a view fulfills the requirement without storing a separate copy of the data because a view does not store any data of its own and always takes the data from a base table. as the data is taken from the base table, accurate and up-to-date information is required. SQL:1999 added the with clause to define “statement scoped views”. They are not stored in the database schema: instead, they are only valid in the query they belong to. This makes it possible to improve the structure of a statement without polluting the global namespace.With is not a stand alone command like create view is: it must be followed by select. Natural Join and Inner JoinNatural Join joins two tables based on the same attribute name and datatypes. The resulting table will contain all the attributes of both the table but keep only one copy of each common column while Inner Join joins two tables on the basis of the column which is explicitly specified in the ON clause. The resulting table will contain all the attributes from both tables including the common column also. data storageStack: Storage, Encoding, Syntax, Data models, Validation, Processing, Indexing, Data stores,Querying, User interfaces. database and data laketwo main paradigms for storing and retrieving data:database and data lake. Data can be imported into the database (this is called ETL, for Extract-Transform-Load. ETL is often used as a verb).The data is internally stored as a proprietary format that is optimized to make queries faster. This includes in particular building indices on the data.On the other hand, data can also just be stored on some file system.This paradigm is called the data lake paradigm and gained a lot of popularity in the past two decades. It is slower, however users can start querying their data without the effort of ETLing. scaling up and scaling outFirst, one can buy a bigger machine: more memory, more or faster CPU cores, a larger disk, etc. This is called scaling up. Second, one can buy more, similar machines and share the work across them. This is called scaling out. Object storesAmazon’s object storage system is called Simple Storage Service, abbreviated S3. From a logical perspective, S3 is extremely simple: objects are organized in buckets. Buckets are identified with a bucket ID, and each object within a bucket is identified with an Object ID. CAP theoremConsistency: at any point in time, the same request to any server returns the same result, in order words, all nodes see the same data;Availability: the system is available for requests at all times with very high availability.Partition tolerance: the system continues to function even if the network linking its machines is occasionally partitioned.The CAP theorem is basically an impossibility triangle: a system cannot guarantee at the same time: usually are CP,AP or AC. REST APIsREST (Representational State Transfer) is an architectural style for designing networked applications.RESTful services often use HTTP as the communication protocol. A client and server communicated with the HTTP protocol interact in terms of methods applied to resources.A resource is referred to with what is called a URI. URI stands for Uniform Resource Identifier. A client can act on resources by invoking methods, with an optional body. The most important methods are: GET, PUT,DELETE,POST. REST is not a standard or protocol, this is an approach to or architectural style for writing API.REST is an architectural style, and RESTful is the interpretation of it. That is, if your back-end server has REST API and you make client-side requests (from a website/application) to this API, then your client is RESTful. All requests you make have their HTTP status codes. There are a lot of them and they are divided into 5 classes. The first number indicates which of them a code belongs to:1xx - informational2xx - success3xx - redirection4xx - client error5xx - server error Amazon S3 and Azure Blob Storage Key-value storeA key-value store differs from a typical relational database in three aspects: • Its API is considerably simpler than that of a relational database (which comes with query languages) • It does not ensure atomic consistency; instead, it guarantees eventual consistency, which we covered earlier in this Chapter. • A key-value store scales out well, in that it is very fast also at large scales. Amazon DynamoIt is itself based (with some modifications) on the Chord protocol, which is a Distributed Hash Table.On the physical level, a distributed hash table is made of nodes (the machines we have in a data center, piled up in racks) that work following a few design principles. The first design principle is incremental stability. This means that new nodes can join the system at any time, and nodes can leave the system at any time, sometimes gracefully, sometimes in a sudden crash.The second principle is symmetry: no node is particular in any way The third principle is decentralization: there is no “central node” that orchestrates the others.The fourth principle is heterogeneity: the nodes may have different CPU power, amounts of memory, etc. A central aspect of the design of a distributed hash table, and part in particular of the Chord protocol, is that every logical key is hashed to bits that we will call IDs. In the case of Dynamo, the hash is made of 128 bits (7 bytes).In the chord protocol, a technology called a finger table is used. Each node knows the next node clockwise, and the second node, and the 4th node, and the 8th node. Dynamo changes this design to so-called “preference lists”: each node knows, for every key (or key range), which node(s) are responsible (and hold a copy) of it. This is done by associating every key (key range) with a list of nodes, by decreasing priority (going down the ring clockwise). Distributed hash tables, including Dynamo, are typically AP. A fundamental conceptual tool in AP systems is the use of vector clocks.Vector clocks are a way to annotate the versions when they follow a DAG structure.A vector clock can logically be seen as a map from nodes (machines) to integers, i.e., the version number is incremented per machine rather than globally. vector clockLamport’s logical clockLamport’s Logical Clock was created by Leslie Lamport. It is a procedure to determine the order of events occurring. It provides a basis for the more advanced Vector Clock Algorithm. Due to the absence of a Global Clock in a Distributed Operating System Lamport Logical Clock is needed. Implementation Rules：[IR1]: If a -&gt; b [‘a’ happened before ‘b’ within the same process] then, Ci(b) =Ci(a) + d[IR2]: Cj = max(Cj, tm + d) [If there’s more number of processes, then tm = value of Ci(a), Cj = max value between Cj and tm + d] Vector Clocks in Distributed SystemsVector Clock is an algorithm that generates partial ordering of events and detects causality violations in a distributed system.How does the vector clock algorithm work : Initially, all the clocks are set to zero.Every time, an Internal event occurs in a process, the value of the processes’s logical clock in the vector is incremented by 1.Every time, a process receives a message, the value of the processes’s logical clock in the vector is incremented by 1, and moreover, each element is updated by taking the maximum of the value in its own vector clock and the value in the vector in the received message (for every element). To sum up, Vector clocks algorithms are used in distributed systems to provide a causally consistent ordering of events but the entire Vector is sent to each process for every message sent, in order to keep the vector clocks in sync. partial order relationNote that vector clocks can be compared to each other with a partial order relation ≤. A partial order relation is any relation that is reflexive, antisymmetric, and transitive. A total order relation is a partial order in which every element of the set is comparable with every other element of the set. All total order relations are partial order relations, but the converse is not always true. referenceshttps://www.geeksforgeeks.org/normal-forms-in-dbms/https://www.geeksforgeeks.org/denormalization-in-databases/https://www.geeksforgeeks.org/difference-between-view-and-table/https://modern-sql.com/feature/withhttps://www.geeksforgeeks.org/sql-natural-join/https://mlsdev.com/blog/81-a-beginner-s-tutorial-for-understanding-restful-apihttps://ghislainfourny.github.io/big-data-textbook/https://www.geeksforgeeks.org/lamports-logical-clock/","link":"/2023/09/19/bigdata1/"},{"title":"bigdata - Syntax","text":"syntaxCSVCSV is a textual format, in the sense that it can be opened in a text editor. CSV means comma-separated values. The main challenge with CSV files is that, in spite of a standard (RFC 4180), in practice there are many different dialects and variations, which limits interoperability. For example, another character can be used instead of the comma (tabulation, semi-colons, etc). Also, when a comma (or the special character used in its stead) needs to actually appear in a value, it needs to be escaped. Data denormalizationData denormalization makes a lot of sense in read-intensive scenarios in which not having to join brings a significant performance improvement. The difference with CSV is that, in JSON, the attributes appear in every tuple, while in CSV they do not appear except in the header line. JSON is appropriate for data denormalization because including the attributes in every tuple allows us to drop the identical support requirement. The generic name for denormalized data (in the same of heterogeneous and nested) is “semi-structured data”. Textual formats such as XML and JSON have the advantage that they can both be processed by computers, and can also be read, written and edited by humans. Another very important and characterizing aspect of XML and JSON is that they are standards: XML is a W3C standard. W3C, also known as the World Wide Web consortium, is the same body that also standardizes HTML, HTTP, etc. JSON is now an ECMA standard, which is the same body that also standardizes JavaScript. Whichever syntax is used, they have in common the concept of well-formedness. A string is said to be well-formed if it belongs to the language. Concretely, when a document is well-formed XML, it means that it can be successfully opened by an editor as XML with no errors. JSONJSON stands for JavaScript Object Notation because the way it looks like originates from JavaScript syntax, however it is now living its own life completely independently of JavaScript. JSON is made of exactly six building blocks: strings, numbers, Booleans, null, objects, and arrays. Strings are simply text. In JSON, strings always appear in double quotes. Obviously, strings could contain quotes and in order not to confuse them with the surrounding quotes, they need to be differentiated. This is called escaping and, in JSON, escaping is done with backslash characters (). JSON generally supports numbers, without explicitly naming any types nor making any distinction between numbers apart from how they appear in syntax. The way a number appears in syntax is called a lexical representation, or a literal. JSON places a few restrictions: a leading + is not allowed. Also, a leading 0 is not allowed except if the integer part is exactly 0. There are two Booleans, true and false. Arrays are simply lists of values. The concept of list is abstract and mathematical. The concept of array is the syntactic counterpart of a list. Objects are simply maps from strings to values. The concept of object is the syntactic counterpart of a map,i.e., an object is a physical representation of an abstract map that explicitly lists all string-value pairs. The keys of an object must be strings. The JSON standard recommends for keys to be unique within an object. XMLXML stands for eXtensible Markup Language. It resembles HTML, except that it allows for any tags and that it is stricter in what it allows. XML’s most important building blocks are elements, attributes, text and comments. XML is a markup language, which means that content is “tagged”. Tagging is done with XML elements. An XML element consists of an opening tag, and a closing tag. What is “tagged” is everything inbetween the opening tag and the closing tag. ags consist of a name surrounded with angle brackets &lt; … &gt;, and the closing tag has an additional slash in front of the name. We use a convenient shortcut to denote the empty element with a single tag and a slash at the end. For example, \\ is equal to\\\\.Unlike JSON keys, element names can repeat at will. Attributes appear in any opening elements tag and are basically keyvalue pairs. Values can be either double-quoted or single-quoted. The key is never quoted, and it is not allowed to have unquoted values. Within the same opening tag, there cannot be duplicate keys. Attributes can also appear in an empty element tag. Attributes can never appear in a closing tag. It is not allowed to create attributes that start with XML or xml, or any case combination. Text, in XML syntax, is simply freely appearing in elements and without any quotes (attribute values are not text!). Within an element, text can freely alternate with other elements. This is called mixed content and is unique to XML. Comments in XML look like so: \\. XML documents can be identified as such with an optional text declaration containing a version number and an encoding, like \\&lt;?xml version=”1.0” encoding=”UTF-8”?&gt;. The version is either 1.0 or 1.1. Another tag that might appear is the doctype declaration, like \\&lt;!DOCTYPE person&gt;. Remember that in JSON, it is possible to escape sequences with a backslash character. In XML, this is done with an ampersand (&amp;) character. There are exactly five possible escape sequences pre-defined in XML:. Escape sequences can be used anywhere in text, and in attribute values. At other places (element names, attribute names, inside comments), they will not be recognized.There are a few places where they are mandatory:&amp; and &lt; MUST be escaped. ” and ‘ should also be escaped in quoted qttribute values. Namespaces are an extension of XML that allows users to group their elements and attributes in packages, similar to Python modules, Java packages or C++ namespaces. A namespace is identified with a URI. A point of confusion is that XML namespaces often start with http://, but are not meant to be entered as an address into a browser. A namespace declaration is like: \\. If you remember, we saw that attributes starting with xml are forbidden, and this is because this is reserved for namespace declarations. What about documents that use multiple namespaces? This is done by associating namespaces with prefixes, which act as shorthands for a namespace. Then, we can use the prefix shorthand in every element that we want to have in this namespace.So, given any element, it is possible to find its local name, its (possibly absent) prefix, and its (possibly absent) namespace. The triplet (namespace, prefix, localname) is called a QName Attributes can also live in namespaces, that is, attribute names are generally QNames. However, there are two very important aspects to consider. First, unprefixed attributes are not sensitive to default namespaces: unlike elements, the namespace of an unprefixed attribute is always absent even if there is a default namespace. Second, it is possible for two attributes to collide if they have the same local name, and different prefixes but associated with the same namespace (but again, we told you: do not do that!). referenceshttps://ghislainfourny.github.io/big-data-textbook/","link":"/2023/10/11/bigdata3/"},{"title":"chatgpt","text":"chatgpt apiparameterstwo important parameters that you can use with OpenAI’s GPT API to help control text generation behavior: temperature and top_p sampling.Temperature is a parameter that controls the “creativity” or randomness of the text generated by GPT-3. A higher temperature (e.g., 0.7) results in more diverse and creative output, while a lower temperature (e.g., 0.2) makes the output more deterministic and focused.In practice, temperature affects the probability distribution over the possible tokens at each step of the generation process. A temperature of 0 would make the model completely deterministic, always choosing the most likely token. Top_p sampling is an alternative to temperature sampling. Instead of considering all possible tokens, GPT-3 considers only a subset of tokens (the nucleus) whose cumulative probability mass adds up to a certain threshold (top_p). chatgpt methodsWe trained this model using Reinforcement Learning from Human Feedback (RLHF), using the same methods as InstructGPT, but with slight differences in the data collection setup. We trained an initial model using supervised fine-tuning: human AI trainers provided conversations in which they played both sides—the user and an AI assistant. We gave the trainers access to model-written suggestions to help them compose their responses. We mixed this new dialogue dataset with the InstructGPT dataset, which we transformed into a dialogue format. To create a reward model for reinforcement learning, we needed to collect comparison data, which consisted of two or more model responses ranked by quality. Using these reward models, we can fine-tune the model using Proximal Policy Optimization. We performed several iterations of this process. referencehttps://community.openai.com/t/cheat-sheet-mastering-temperature-and-top-p-in-chatgpt-api-a-few-tips-and-tricks-on-controlling-the-creativity-deterministic-output-of-prompt-responses/172683 https://openai.com/blog/chatgpt","link":"/2023/06/28/chatgpt/"},{"title":"第四篇 git相关(2)-git&amp;github","text":"远程仓库远程仓库是指托管在因特网或其他网络中的你的项目的版本库。 githubGitHub是一个面向开源及私有软件项目的托管平台，因为只支持git作为唯一的版本库格式进行托管，故名GitHub。 一次练习最近在学java，也想巩固一下之前学习过的算法和学习新的算法，就打算用java实现，就用这个项目来加强git的使用练习。 首先在IDEA下java学习的项目里，新建了一个algorithms module，在此文件下，进入Git Bash，新建本地仓库，此时只有一个用来测试排序算法的java文件和一个iml项目配置文件。 目前java文件的内容还是空的， 编辑一下，加入main函数，文件被编辑之后，再次使用git status查看文件状态，会发现文件状态已经变成了modified，再次add，然后开始commit。 commit之后，会显示此次提交的一些信息。 前面展示了一些Git本地的基本操作，现在假如本地文件修改好了，或者工作暂停了，准备放到github上，那么先去github上Create a new repository，最初创建的时候只有一个readme文件，下面将本地仓库同步到远程仓库上。 建立完之后，在本地仓库，将远程仓库的URL复制下来，添加远程仓库。 现在本地仓库里是没有readme文件的，如果此时想要直接push，将本地仓库推送到远程仓库的话，看看会发生什么。 跟随这个报错信息的指示，使用pull，看又会发生什么。这个原因是因为目前本地仓库和远程仓库没有任何相同的文件，根本不相干，所以会被告知无法合并，更加方便的流程是先从远程仓库拉取下来，再把本地文件加入到远程仓库下载到本地的库，然后再提交。 那就没有解决办法了嘛？不是的，可以使用一个强制的方法，添加一个可选项—allow-unrelated-histories，问题终于得以解决。 现在再去github上看看，就会发现提交成功而且push成功啦，开森，撒花~之后就要坚持练习写代码啦，刚把得勒~","link":"/2020/03/29/git-1/"},{"title":"第六篇 git相关(3)-分支与合并","text":"git分支git分支是git的一大特性，git 的分支本质上仅仅是指向提交对象的可变指针，是包含所指对象校验和（长度为 40 的 SHA-1 值字符串）的文件，这也是为什么git分支的创建和销毁都异常高效的原因，创建新分支时，就是在当前所在的提交对象上创建一个指针，而git是通过一个名为HEAD的特殊指针来记录自己的当前位置的。 与git分支相关的命令有： 1234567git branch -- 查看本地分支git branch -v -- 查看本地分支每个分支的最后一次提交git branch -vv -- 查看本地分支每个分支跟踪的远程分支git checkout -b dev -- 新建dev分支，并切换到dev分支(此后HEAD会指向dev分支,工作目录也会变为dev分支指向的快照内容)git checkout master -- 切换到master分支(此后HEAD会指向master分支,工作目录也会变为master分支指向的快照内容)git merge dev -- 合并dev分支到当前分支git branch -d dev -- 删除dev分支 分支切换注意在切换分支时，git会重置工作目录，自动添加、删除、修改文件以确保此时你的工作目录和这个分支最后一次提交时的样子一模一样，因此在切换分支前，要注意暂存当前的工作进度，可以使用git stash命令暂存当前工作。 分支合并在合并时，可能会看到fast-forward这个词，这是指当前分支是合并分支的直接上游，两者合并不会产生冲突，而只是指针向前移动了，所以叫做fast-forward。 如果两个分支已经分叉开来，进行合并时，git会根据两个分支的共同祖先来做一个三方合并，此时的合并就不是简单的指针向前移动了，而是将三方合并的结果生成一个新的快照并创建一个新的提交指向它。 有时候合并会发生冲突，靠git自动合并已经无法解决，就需要人工去解决冲突，git会在有冲突的文件中加入标记，使用&lt;&lt;&lt;&lt;&lt;&lt;&lt;和&gt;&gt;&gt;&gt;&gt;&gt;&gt;、=======标识了冲突的位置，=======将两个分支的冲突位置内容分隔开来，为了解决冲突，只能选择其中一个，手动地将标识的片段改为你选择的内容即可，修改完冲突之后，还要使用git add来表示冲突已经解决，git commit来完成合并提交。 远程分支远程分支以 (remote)/(branch) 形式命名，本地分支与远程分支交互相关的命令如下： 1234git push (remote) (branch):(remote branch) --推从到远程分支git fetch (remote) --拉取远程仓库的内容，不会修改工作目录git pull (remote) --拉取并合并远程仓库的内容到本地git push (remote) --delete (branch) --删除远程分支","link":"/2021/07/03/git-2/"},{"title":"第三篇 git相关(1)-git基础原理与命令","text":"什么是gitGit is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. 使用Git假设已经在电脑上安装好了Git，并配好了环境，在windows下，文件夹下右击出现了 Git Bash Here，就说明已经安装好了。 使用git需要首先建立一个仓库，之后就可以在这个仓库中对代码进行各种操作，过程中会使用到各种git命令，下面就介绍一下每个git命令的具体作用。 Git 基础命令git init用于初始化一个仓库。在某个文件夹下打开Git Bash，运行完这个命令，文件夹下会生成一个.git文件夹，这个文件夹会记录以后的变更行为，但想要真正地追踪这些变更，还需要更多的操作，只有处于tracked状态下的文件，git才会追踪，后面会具体介绍。 git status用于查看仓库中所有文件的状态。Git中文件有4种状态：untracked， unmodified , modified, staged，后三种状态属于tracked，这几个状态体现了git的原理。 新建一个文件的时候，这个文件处于untracked状态，对这个文件使用了 git add之后，这个文件进入staged状态，也就是暂存区，使用了git commit之后，这个文件进入unmodified状态，这才是实际提交了改动，如果编辑了这个文件，对这个文件进行了更改，便进入了modified状态。 git add在上部分有介绍过，但是这个命令并不是添加文件到某个项目中的意思，准确地说这个命令是把想要提交的内容添加到准备提交的集合里，可以用这个命令来追踪新文件(add the file)，暂存文件（stage the file），或者其它在commit之前的操作。如果在运行了git add之后又修改了文件，没有再次运行git add，就运行了commit，commit的是修改之前的内容。那么是不是每一次都要反复操作git add呢，其实还有别的方法可以跳过暂存区这一步，下面会说明。 git commit是提交变化到仓库里，提交到仓库里的几乎总是可以恢复的，后面会介绍如何恢复。git commit -a就可以跳过暂存区这一步，因为-a包含了所有改动过的文件。git commit -m可以加上这次提交的描述， git rm用于删除文件，这里的删除有两种含义，从Git中删除和从工作目录中删除，如果只是想要Git不在追踪这个文件，需要使用git rm --cached。 git log用于回顾提交历史，运行这个命令可以看到提交的SHA-1 校验和，作者，提交时间和具体提交的内容。如果想要复原到某次提交时候的版本，这个命令是非常重要的，通过拿到每次提交的SHA-1 校验和，可以追踪到对应的版本。 git reset HEAD 用于取消暂存文件。 git checkout --用于撤销所作的修改。 Git 分支分支就是与主线相对的，每个人都可以使用各自的分支进行工作，而不影响主线。在许多版本控制系统中，是需要创建一个完整的源项目副本来创建分支的，而Git不是这样的，Git处理分支的方式非常轻量，分支之间的操作非常迅速。 要理解Git是如何处理分支的，就要理解Git是如何实现对文件的追踪的。Git保存的是不同时刻的快照（Snapshot）,进行提交操作时，Git会保存一个提交对象，这个对象包含了一个指向暂存内容快照的指针，还包含了作者、邮箱等内容以及指向它的父对象的指针，如果是第一次提交，是没有父对象的，而之后的提交，其父对象就是上一次提交。Git的分支，本质上就是指向提交对象的可变指针，所以不同的分支可以指向不同的内容，从而互不影响，而且Git的分支实质上就是一个包含所致对象校验和的文件，所以其创建和销毁都非常高效。 git branch就是创建分支的命令，这个命令会在当前的提交快照上创建一个指针。Git通过一个HEAD的特殊指针指向当前所在的本地分支，从而可以知道自己当前在哪一个分支上。 git branch -d用于删除分支。 git checkout是切换分支的命令。 git merge用于合并分支。当合并分支产生冲突时，Git会停下来，这时候需要手动解决这些冲突，解决完冲突之后，使用git add命令将冲突文件标记为冲突已解决。 远程仓库之前所述的内容，都是基于本地的操作，而如果想要在Git项目上进行协作，就需要一个公共的空间供项目参与者进行共同编辑，这就是远程仓库。远程仓库是指托管在因特网或其他网络中的你的项目的版本库，使用命令新建远程仓库的操作与本地是一致的，在github上create repository就可以新建一个远程仓库，本地和远程仓库之间通过SSH连接，完成SSH的公钥设置之后， 想要实现本地与远程仓库的内容交换，使用以下介绍的命令。 git remote add &lt;shortname&gt; &lt;url&gt;用于添加一个新的远程仓库。 git fetch用于从远程仓库中获取本地没有的数据，但是这个命令指挥把数据下载到本地仓库，并不会与本地仓库自动合并，想要实现自动合并，需要使用git pull命令。git clone可以把远程仓库的内容克隆到本地，并将远程仓库默认命名为”origin”。 git push &lt;remote&gt; &lt;branch&gt;命令用于将本地内容推送到远程仓库上，只有具备远程仓库的写入权限，并且没有人推送过之前，这条命令才会生效，如果别人先推送了，需要先抓取别人的工作并合并到自己的工作中之后才能推送。 git remote show &lt;remote&gt;用于查看某一个远程仓库的具体新息。 git 别名上述说了很多命令，有些命令也比较长，命令很多也比较难记下来，git提供了将这些命令起个别名的功能方便使用。 git config --global alias.co checkout就将checkout起了别名co，现在使用git co就相当于git checkout。","link":"/2020/03/15/git/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2018/12/21/hello-world/"},{"title":"hexo upgrade","text":"hexowhat is hexo“Hexo is a fast, simple and powerful blog framework. You write posts in Markdown (or other markup languages) and Hexo generates static files with a beautiful theme in seconds.” how to use hexoHexo is dependent on Git and Node.js. After installed the two, if you are lucky, you can start to use Hexo as the official documentation(https://hexo.io/docs/) does. Here I will focus on the problems I have met. I accidently uninstalled Node.js and installed newest Node.js on my laptop, however, this action caused a series of problems. I spent 3 hours to fix the problem. In this opportunity, I will document the encountered issues and their solutions. At the same time, I will clarify the dependencies among these software tools. problemsHere we assume that we have installed Git and Node.js successfully. We can check this by running1git --version 1node -v The first issue I encountered was when I ran1hexo deployI got “TypeError [ERR_INVALID_ARG_TYPE]: The “mode” argument must be integer”. As https://github.com/hexojs/hexo/issues/4281 suggests, I upgraded Hexo, but it didn’t work, and there was another problem, I ran12npm install -g hexo-clinpm install -g hexoHexo is still older version. After I modified package.json under the Hexo path and using command like this, it upgraded.1npm i hexo@5.2.0 Unfortunately, even I upgraded Hexo to the newest version, I still got the error, so I decided to degraded Node.js. This is where I knew about nvm, a command-line tool that allows you to easily manage multiple versions of Node.js on a single machine. With nvm, you can install, switch between, and manage different Node.js versions for your development projects. Remember to uninstall the Node.js first, I didn’t, so when I use nvm to install an older version, I still got the installed version.Finally got the older version, I got “YAMLException: Specified list of YAML types (or a single Type object) contains a non-Type object.”, then as https://github.com/hexojs/hexo/issues/4917 suggests, I degraded Node.js and installed yaml-js. Running hexo again, I got “TypeError: Cannot read property ‘length’ of undefined” and “Error [ERR_PACKAGE_PATH_NOT_EXPORTED]: Package subpath ‘./lib/js-yaml/type’ is not defined by “exports””, then I upgraded the theme I use and ran npm install to fix the problem, but there are still some problems with the theme. I reinstalled the theme as https://github.com/ppoffice/hexo-theme-icarus. As the theme upgrading, there are more dependencies needed, just install them as the message shows. referencehttps://hexo.io/docs/https://github.com/ppoffice/hexo-theme-icarus","link":"/2023/10/12/hexo-upgrade/"},{"title":"Introduction to deep learning in computer vision","text":"Basic architectureCNNConvolutional networks are simply neural networks that use convolution in place of general matrix multiplication in at least one of their layers.Convolution leverages three important ideas that can help improve a machine learning system: sparse interactions, parameter sharing and equivariant representations. Moreover, convolution provides a means for working with inputs of variable size. We assume that the size of the input image is nn, and the size of the filter is ff (note that f is generally an odd number). The size of the output image after convolution is (n-f+1)* (n-f+1).During the convolution process, padding is sometimes necessary to avoid information loss. Additionally, adjusting the stride allows for compression of some information.If we want to perform convolution on a three-channel RGB image, the corresponding filter group would also have three channels. The process involves convolving each individual channel with its corresponding filter, summing up the results, and then adding the sums of the three channels together. The resulting sum of the 27 multiplications is considered as one pixel value of the output image. The filters for different channels can be different. When the input has specific height, width, and channel dimensions, the filters can have different height and width, but the number of channels must match the input.Pooling layers are commonly included in many CNNs. The purpose of pooling layers is to reduce the size of the model, improve computational speed, and simultaneously decrease noise to enhance the robustness of the extracted features. Important networks in the history of computer visionLeNet-5LeNet-5, developed by Yann LeCun et al. in 1998, was one of the first successful convolutional neural networks (CNNs) for handwritten digit recognition. It laid the foundation for modern CNN architectures and demonstrated the power of deep learning in computer vision tasks. “Gradient-Based Learning Applied to Document Recognition” by Yann LeCun et al. (1998). LeNet’s network architecture has seven layers: convolutional layer (Convolutions, C1), pooling layer (Subsampling, S2), convolutional layer (C3), pooling layer (S4), fully connected convolutional layer ( C5), fully connected layer (F6), Gaussian connected layer (output).The input layer is a 28x28 one-dimensional image, and the Filter size is 5x5. The output channels of the first Filter and the second Filter are 6 and 16 respectively, and both use Sigmoid as the activation function.The window of the pooling layer is 2x2, the stride is 2, and the sampling is performed using average pooling. The number of neurons in the last fully connected layer is 120 and 84, respectively.The last output layer is the Gaussian connection layer, which uses the RBF function (radial Euclidean distance function) to calculate the Euclidean distance between the input vector and the parameter vector. AlexNetAlexNet, introduced by Alex Krizhevsky et al. in 2012, was a breakthrough CNN architecture that won the ImageNet competition and popularized deep learning in computer vision. It demonstrated the effectiveness of deep CNNs for image classification tasks and paved the way for subsequent advancements.”ImageNet Classification with Deep Convolutional Neural Networks” by Alex Krizhevsky et al. (2012).AlexNet’s architecture has eight layers, using a total of five convolutional layers and three fully connected layers, which is deeper than the LeNet model.The first to fifth layers are convolutional layers, where the first, second, and fifth convolutional layers are followed by pooling layers, and Maxpooling with a size of 3x3 and a stride of 2 is used.The sixth to eighth layers are fully connected layers. Changing the Sigmoid used by LeNet to ReLU can avoid the problem of vanishing gradient due to too deep neural network layers or too small gradients. VGGNetThe VGGNet, proposed by Karen Simonyan and Andrew Zisserman in 2014, is known for its simplicity and depth. It consisted of deep networks with stacked 3x3 convolutional layers, showing that increasing network depth led to improved performance on image classification tasks.”Very Deep Convolutional Networks for Large-Scale Image Recognition” by Karen Simonyan and Andrew Zisserman (2014).Compared with AlexNet, VGGNet adopts a deeper network. It is characterized by repeated use of the same set of basic modules, and uses small convolution kernels instead of medium and large convolution kernels in AlexNet. Its architecture consists of n VGG Blocks and 3 full connections composed of layers.The structure of VGG Block is composed of 3x3 convolutional layers (kernel size=3x3, stride=1, padding=”same”) of different numbers (the number is hyperparameters), and 2x2 Maxpooling (pool size=2, stride=2).VGGNet has many different structures, such as VGG11, VGG13, VGG16, VGG19, the difference lies in the number of layers of the network (the number of convolutional layers and the number of fully connected layers). The common VGGNet refers to VGG16. Network in Network“Network in Network” (NiN) refers to a neural network architecture proposed by Lin et al. in their paper titled “Network In Network” published in 2014. NiN is designed to enhance the expressive power of deep neural networks by incorporating micro neural networks called “MLPs (Multi-Layer Perceptrons)” or “1x1 Convolutions” within the network structure. The key idea behind NiN is to replace traditional convolutional layers with what they call “MLP Convolutional Layers” or “1x1 Convolutional Layers.” These layers consist of a series of fully connected layers (MLPs) applied at every pixel location of the input. The purpose is to capture complex local feature interactions and enable more non-linear transformations.By using 1x1 convolutions, NiN can model non-linear relationships within the channels of the input feature map. This allows for richer and more powerful representations compared to standard convolutional layers.The 1x1 convolutional layer not only integrates the information of different channels at the same position, but also can reduce or increase the dimension of the channel. GoogLeNet (Inception-v1)GoogLeNet, presented by Christian Szegedy et al. in 2015, introduced the Inception module and demonstrated the importance of multi-scale feature extraction. It achieved high accuracy while maintaining computational efficiency, inspiring subsequent Inception versions and influencing network designs.”Going Deeper with Convolutions” by Christian Szegedy et al. (2015).GoogLeNet was designed to address the challenges of deep neural networks, such as computational efficiency and overfitting, while maintaining high accuracy in image classification tasks. It introduced several novel concepts and architectural innovations that made it stand out from previous CNN architectures at the time. The key feature of GoogLeNet is the Inception module, which utilizes parallel convolutional filters of different sizes (1x1, 3x3, 5x5) to capture features at various scales. This allows the network to learn and represent both local and global features effectively. Additionally, it incorporates 1x1 convolutions for dimensionality reduction and introduces a technique called “bottleneck” layers to reduce the computational complexity. InceptionIn the context of computer vision, “inception” refers to the Inception module or the Inception architecture used in deep convolutional neural networks (CNNs). The Inception module was introduced in the GoogLeNet architecture (also known as Inception-v1) as a key component for efficient and effective feature extraction.The Inception module aims to capture multi-scale features by employing multiple parallel convolutional filters of different sizes within the same layer. By using a combination of 1x1, 3x3, and 5x5 convolutional filters, the Inception module allows the network to learn and extract features at various spatial scales. The Inception module extracts different features through convolution of three different sizes and 3x3 Maxpooling, and then concatenates these four results together with the channel axis. This way of increasing the width of the network can capture more features and details of the picture.But if the sizes of these four results are different, both the convolutional layer and the pooling layer use padding=”same” and stride=1 to ensure the size of the input feature map. ResNetResNet, developed by Kaiming He et al. in 2015, introduced the concept of residual learning. It utilized skip connections or shortcuts to address the vanishing gradient problem and enabled training of extremely deep networks, leading to significant performance gains in image classification and other tasks.”Deep Residual Learning for Image Recognition” by Kaiming He et al. (2015). DenseNet DenseNet, introduced by Gao Huang et al. in 2016, focused on dense connectivity patterns between layers. It aimed to alleviate the vanishing gradient problem, promote feature reuse, and encourage better gradient flow. DenseNet achieved competitive results while reducing the number of parameters compared to other architectures. “Densely Connected Convolutional Networks” by Gao Huang et al. (2016).’ ResNeXtResNeXt is a convolutional neural network (CNN) architecture that builds upon the concepts introduced by the ResNet (Residual Network) model. ResNeXt was proposed by Xie et al. in their paper titled “Aggregated Residual Transformations for Deep Neural Networks” in 2017. The main idea behind ResNeXt is to leverage the concept of “cardinality” to improve the representational power of the network. Cardinality refers to the number of independent pathways or branches within a block of the network. In ResNeXt, instead of using a single pathway in each block, multiple parallel pathways are employed. referenceshttps://juejin.cn/post/7104845694225088525https://www.showmeai.tech/article-detail/221https://medium.com/ching-i/%E5%8D%B7%E7%A9%8D%E7%A5%9E%E7%B6%93%E7%B6%B2%E7%B5%A1-cnn-%E7%B6%93%E5%85%B8%E6%A8%A1%E5%9E%8B-lenet-alexnet-vgg-nin-with-pytorch-code-84462d6cf60c","link":"/2023/06/09/image-task/"},{"title":"第二十一篇 java(2)- 集合框架","text":"集合框架java集合框架包括 Collection 和 Map 两种，Collection 存储着对象的集合，而 Map 存储着键值对(两个对象)的映射表。 CollectionCollection下包括Set、List、Queue，各自又包含了使用不同方式的实现。 SetSet下包括TreeSet，HashSet，LinkedHashSet。其中TreeSet基于红黑树实现。","link":"/2021/09/26/java-1/"},{"title":"第八篇 java(1)-面向对象","text":"类与对象类就是用户定义好的原型，可以通过类创建对象，类中定义好了一系列的属性或者是函数。对象是真正的实体。当为一个类创建了对象，也可以说是实例化了这个类，java中有几种创建类的方式。 当只是简单地声明一个类变量时，如Object a，不同于原始变量int、double等声明变量时就分配好了内存，这样的声明方式并没有创建好一个对象，需要通过new关键字来触发类构造器，并为这个对象分配好内存。所有类都至少有一个构造函数，如果没有定义，Java编译器会自动创建一个无参构造函数。这个构造器会调用其父类的无参构造函数。 封装、继承与多态修饰符在介绍封装、继承与多态之前，需要先了解java中的修饰符，修饰符有两类： 一类是控制访问权限的，一类是实现其他功能的。控制访问权限的修饰符有 12345678// 以下为类修饰符public --任何类均可访问。default --没有指定修饰符时的默认修饰符，只有同一个包中的类可以访问。// 以下为属性、方法修饰符public --任何类均可访问。private --只有声明的类中可以访问。protected --只有同一个包中的类和其子类可以访问。default --没有指定修饰符时的默认修饰符，只有同一个包中的类可以访问。 实现其他功能的修饰符有： 12345678910// 以下为类修饰符final --此类不能被其他类继承。abstract --抽象类，此类不能用来创建对象。// 以下为属性、方法修饰符final --属性、方法不能被重载。static --属性、方法属于类而不是对象。abstract -- 只能用在抽象类中的方法上。transient -- 序列化对象时跳过此属性和方法。synchronized -- 此方法一次只能被一个线程访问。volatile --此属性的值不是线程内部缓存，而是从主内存中读取。 封装封装可以将实现细节隐藏起来，其最主要的功能在于我们能修改自己的实现代码，而不用修改那些调用我们代码的程序片段。 继承在继承中，先了解两个概念：父类和子类，父类就是被继承的类，子类就是继承其他类的类，在jva中，继承使用extends关键字或implements关键字。java中类的继承是单一继承，一个子类只能拥有一个父类。所有 Java 的类均是由java.lang.Object 类继承而来的。通过使用关键字 extends ，子类可以继承父类的除 private 属性外所有的属性。Implements 关键字在类继承接口的情况下使用，可以多继承接口。 重写(Override)与重载(Overload)重写(Override)重写是子类对父类的允许访问的方法的实现过程进行重新编写！返回值和形参都不能改变。 子类在声明变量时，可以使用父类类型，这是因为在编译阶段，只是检查参数的引用类型，然而在运行时，Java 虚拟机 (JVM) 指定对象的类型并且运行该对象的方法。需要注意的是构造方法不能被重写。需要在子类中调用父类的被重写方法时，要使用 super 关键字。 重载(Overload)重载 (overloading) 是在一个类里面，方法名字相同，而参数不同。返回类型可以相同也可以不同。构造函数是可以重载的。 多态通过继承、重写、重载可以以多种不同的方式实现某个操作，便可以实现多态。除此之外，java中还有接口和抽象类以实现多态。 接口接口通常以interface来声明。一个实现接口的类，必须实现接口内所描述的所有方法，否则就必须声明为抽象类。","link":"/2021/07/16/java/"},{"title":"第七篇 Linux相关(2)-进程管理与性能分析","text":"进程管理相关命令进程启动12nohup --用于在系统后台不挂断地运行命令&amp; --放在命令后，表示后台执行 进程查看1234567ps -aux --显示进程状态top --实时显示进程状态pstree --树状图展示进程关系nice --调整进程优先级pidof --查询进程PIDkill --终止进程killall --终止某服务名称对应的所有进程 历史命令12history --显示执行过的命令历史!编码数字 --重复执行某一次的命令 性能分析系统状态1234uname --查看系统内核版本与系统架构,详细系统版本使用cat /etc/redhat-releaseuptime --查看系统的负载信息free --显示当前系统中内存的使用量信息who --查看当前登入主机的用户终端信息 网络状态123ifconfig --查看网卡配置和网络状态ifconfigping --测试主机之间的网络连通性netstat --显示如网络连接、路由表、接口状态等的网络相关信息 时间123date -- 显示当前日期date &quot;+%Y-%m-%d %H:%M:%S&quot; --以给定格式显示日期date &quot;+%j&quot; --今年第几天","link":"/2021/07/09/linux-1/"},{"title":"Large Language Model","text":"basic ideasZero-Shot Learningzero-shot learning, in which your model learns how to classify classes that it hasn’t seen before. Contrastive Language-Image Pretraining (CLIP)Just like traditional supervised models, CLIP has two stages: the training stage (learning) and the inference stage (making predictions).In the training stage, CLIP learns about images by “reading” auxiliary text (i.e. sentences) corresponding to each image. CLIP aims to minimize the difference between the encodings of the image and it’s corresponding text.In the inference stage, we setup the typical classification task by first obtaining a list of all possible labels.Each label will then be encoded by the pretrained text encoder from Step 1.Now that we have the label encodings, T₁ to Tₙ, we can take the image that we want to classify, feed it through the pretrained image encoder, and compute how similar the image encoding is to each text label encoding using a distance metric called cosine similarity. contrastive learningContrastive learning is a machine learning technique used to learn the general features of a dataset without labels by teaching the model which data points are similar or different.It looks at which pairs of data points are “similar” and “different” in order to learn higher-level features about the data, before even having a task such as classification or segmentation. SimCLRv2The entire process can be described concisely in three basic steps: For each image in our dataset, we can perform two augmentation combinations (i.e. crop + resize + recolor, resize + recolor, crop + recolor, etc.). We want the model to learn that these two images are “similar” since they are essentially different versions of the same image. To do so, we can feed these two images into our deep learning model (Big-CNN such as ResNet) to create vector representations for each image. The goal is to train the model to output similar representations for similar images. Lastly, we try to maximize the similarity of the two vector representations by minimizing a contrastive loss function. Meta-learningThe idea of meta-learning is to learn the learning process. In-context Learninguring in-context learning, we give the LM a prompt that consists of a list of input-output pairs that demonstrate a task. At the end of the prompt, we append a test input and allow the LM to make a prediction just by conditioning on the prompt and predicting the next tokens. Instruction learningInstruction learning is an idea proposed by the team led by Quoc V. Le at Google DeepMind in a paper titled ‘Finetuned Language Models Are Zero-Shot Learners’ in 2021. The purpose of instruction learning and prompt learning is to explore the knowledge inherent in language models. The difference is that prompts aim to stimulate the completion ability of the language model, such as generating the second half of a sentence based on the first half or filling in the blanks. Instructions aim to stimulate the understanding ability of the language model by providing more explicit instructions, enabling the model to take correct actions. The advantage of instruction learning is that after fine-tuning through multitask learning, it can also perform zero-shot learning on other tasks, while prompt learning is specific to one task. Its generalization ability is not as strong as instruction learning. Diffusion ModelIn machine learning, the Diffusion Model refers to a class of algorithms or models that utilize diffusion processes for various tasks, such as data clustering, image segmentation, or graph-based learning. The basic principle of the Diffusion Model in machine learning is to propagate information or labels through the connections or edges of a graph or network. The diffusion process starts with initial information or labels assigned to some nodes in the graph, and it gradually spreads and influences the neighboring nodes based on certain rules or algorithms. Stable DiffusionStable Diffusion is a deep learning, text-to-image model released in 2022. It is primarily used to generate detailed images conditioned on text descriptions. Prompt engineeringPrompt engineering is a relatively new discipline for developing and optimizing prompts to efficiently use language models (LMs) for a wide variety of applications and research topics.Prompt engineering focuses on crafting the optimal textual input by selecting the appropriate words, phrases, sentence structures, and punctuation. RLHF(Reinforcement Learning from Human Feedback)generation Auto-regressive language generation is based on the assumption that the probability distribution of a word sequence can be decomposed into the product of conditional next word distributions. The length T of the word sequence is usually determined on-the-fly and corresponds to the timestept=T the EOS token is generated from the probability distribution. decoding methodsGreedy searchGreedy search is the simplest decoding method. It selects the word with the highest probability as its next word. Beam searchBeam search reduces the risk of missing hidden high probability word sequences by keeping the most likely num_beams of hypotheses at each time step and eventually choosing the hypothesis that has the overall highest probability.Beam search will always find an output sequence with higher probability than greedy search, but is not guaranteed to find the most likely output.The most common n-grams penalty makes sure that no n-gram appears twice by manually setting the probability of next words that could create an already seen n-gram to 0. Nevertheless, n-gram penalties have to be used with care. An article generated about the city New York should not use a 2-gram penalty or otherwise, the name of the city would only appear once in the whole text!When using transformers library:beam_output = model.generate(**model_inputs,max_new_tokens=40,num_beams=5,no_repeat_ngram_size=2,early_stopping=True) Another important feature about beam search is that we can compare the top beams after generation and choose the generated beam that fits our purpose best.In transformers, we simply set the parameter num_return_sequences to the number of highest scoring beams that should be returned. Make sure though that num_return_sequences &lt;= num_beams! samplingIn its most basic form, sampling means randomly picking the next word according to its conditional probability distribution. temperaturea temperatureparameter to adjust the probability distribution of the output. The larger the parameter value, the smoother the distribution looks, that is, the gap between high probability and low probability is narrowed (not so sure about the output); of course, the smaller it is, the more obvious the gap between high probability and low probability (more sure about the output). If it tends to 0, it is the same as Greedy Search. Top-KIn Top-K sampling, the K most likely next words are filtered and the probability mass is redistributed among only those K next words.GPT2 adopted this sampling scheme, which was one of the reasons for its success in story generation. Top-PInstead of sampling only from the most likely K words, in Top-p sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability p. The probability mass is then redistributed among this set of words. While in theory, Top-p seems more elegant than Top-K, both methods work well in practice. Top-p can also be used in combination with Top-K, which can avoid very low ranked words while allowing for some dynamic selection. sample_outputs = model.generate(**model_inputs, max_new_tokens=40,do_sample=True,top_k=50,top_p=0.95,num_return_sequences=3) models:LLaMALLaMA, a collection of foundation language models ranging from 7B to 65B parameters. FastChatother modelshttps://github.com/baichuan-inc/baichuan-7B LLM benchmarksMMLUThe MMLU benchmark covers 57 general knowledge areas such as “Humanities”, “Social Sciences”, and “STEM”. Each question in it contains four possible options, and each question has only one correct answer.there are two main ways to get information from a model to evaluate it:Get the output probabilities for a particular set of tokens and compare them to the alternatives in the sample;Take the text generated by the model (iteratively generated one by one using the method described above), and compare these texts with the alternatives in the sample. C-EvalA Chinese knowledge and reasoning test set covering four major fields: humanities, social sciences, natural sciences, and other disciplines. It consists of 52 subjects, including calculus, linear algebra, and more, covering topics from secondary school to university-level studies, graduate studies, and professional examinations. The test set comprises a total of 13,948 questions. code generation benchmarksHumanEvalHumanEval is proposed to evaluate the functional correctness on a set of 164 handwritten programming problems with unit tests.Functional correctness is measured for synthesizing programs from docstrings.Each problem includes a function signature, docstring, body, and several unit tests. pass@k metric, is used where k code samples are generated per problem see if any sample passes the unit tests. MBPP (Mostly Basic Python Programming)The benchmark consists of around 1,000 crowd-sourced Python programming problems, designed to be solvable by entry-level programmers, covering programming fundamentals, standard library functionality, and so on. Each problem consists of a task description, code solution and 3 automated test cases. APPS(Automated Programming Progress Standard)The APPS dataset consists of 5000 training and 5000 test examples of coding problems. Most of the APPS tests problems are not formulated as single-function synthesis tasks, but rather as full-program synthesis.The APPS benchmark attempts to mirror how humans programmers are evaluated by posing coding problems in unrestricted natural language and evaluating the correctness of solutions. MultiPL-EMultiPL-E is a multi-programming language benchmark for evaluating the code generation performance of large language model (LLMs) of code. DS-1000a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas. referenceshttps://towardsdatascience.com/understanding-zero-shot-learning-making-ml-more-human-4653ac35ccabhttps://towardsdatascience.com/understanding-contrastive-learning-d5b19fd96607http://ai.stanford.edu/blog/understanding-incontext/https://www.8btc.com/article/6813626https://en.wikipedia.org/wiki/Stable_DiffusionLLaMA: Open and Efficient Foundation Language Modelshttps://www.promptingguide.ai/https://medium.com/geekculture/list-of-open-sourced-fine-tuned-large-language-models-llm-8d95a2e0dc76https://nl2code.github.io/https://yaofu.notion.site/C-Eval-6b79edd91b454e3d8ea41c59ea2af873https://huggingface.co/blog/zh/evaluating-mmlu-leaderboardhttps://github.com/datawhalechina/hugging-llm/blob/main/content/ChatGPT%E5%9F%BA%E7%A1%80%E7%A7%91%E6%99%AE%E2%80%94%E2%80%94%E7%9F%A5%E5%85%B6%E4%B8%80%E7%82%B9%E6%89%80%E4%BB%A5%E7%84%B6.mdhttps://huggingface.co/blog/how-to-generate","link":"/2023/06/19/large-model/"},{"title":"第二篇 Linux相关(1)-文件管理与文本编辑","text":"文件操作相关命令查看目录和文件12345678ls -- 当前目录下文件展示ll -- 当前目录下文件展示 详细tree --树状图形式展示pwd --显示当前工作目录find --按照指定条件来查找文件所对应的位置cd -- 打开目录，.表示当前目录，..表示上一级目录，-表示回到刚才所在的路径下。which 命令 --查看命令所在路径file 文件名 --查看文件类型 查看文件内容1234567891011cat 文件名 -- 从第一行开始显示文件内容head [-n number] 文件名 -- 只看头几行tail [-n number] 文件名 -- 只看尾巴几行，tail -f 可以实时查看文件更新内容more 文件名 -- 一页一页显示内容less 文件名 -- 一页一页显示内容,相比less可以向上翻页nl 文件名 -- 显示行号stat 文件名 --查看文件的具体存储细节和时间等信息wc 文件名 --统计指定文本文件的行数、字数或字节数grep 文件名 --按行提取文本内容cut 文件名 --按列提取文本内容diff [参数] 文件名称A 文件名称B --比较多个文件之间内容的差异 处理目录和文件12345678mkdir -- 创建目录rmdir -- 删除空目录cp source dest --复制source到destmv source dest -- 移动source到dest/重命名rm -- 删除touch --创建空白文件tar -czvf 压缩包名称.tar.gz 要打包的目录 -- 把指定的文件进行打包压缩tar -xzvf 压缩包名称.tar.gz -C 解压到的路径 --解压到指定路径 磁盘管理12345df -- 列出文件系统的整体磁盘使用量du -- 检查磁盘空间使用量du -ah --max-depth=1 查看当前目录下的第一级使用量mount --磁盘挂载umount --磁盘卸载 文本编辑器vi/vim命令模式(:)12345678910111213:i -- 进入编辑模式:wq -- 保存并退出:q -- 退出:q! --强制退出:w [filename] --另存为:set nu --显示行号:set nonu --取消行号:1,$s/word1/word2/g -- 从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 :n1,n2s/word1/word2/g -- 在第 n1 与 n2 行之间寻找 word1 这个字符串，并将该字符串取代为 word2 :10,20s#^#//#g -- 在 10 - 20 行添加 // 注释:10,20s#^//##g -- 在 10 - 20 行删除 // 注释:10,20s/^/#/g -- 在 10 - 20 行添加 # 注释::10,20s/#//g -- 在 10 - 20 行删除 # 注释 操作键:1234567891011gg -- 转到第一行G -- 转到最后一行nG -- 转到第n行dd -- 删除光标所在行yy -- 复制光标所在行[Ctrl] + [f] -- 往上翻页[Ctrl] + [b] -- 往下翻页/ -- 搜索，如/word就是搜索wordu -- 撤销上一步[Ctrl]+r -- 重复上一步p,P -- 粘贴复制内容,p是复制到光标所在下一行，P是复制到光标所在上一行。","link":"/2020/03/14/linux/"},{"title":"object tracking","text":"object trackingMultiple Object Tracking(MOT) is the task of detecting various objects of interest in a video, tracking these detected objects in subsequent frames by assigning them a unique ID, and maintaining these unique IDs as the objects move around in a video in successive frames.Generally, multiple object tracking happens in two stages: object detection and object association. Object detection is the process of identifying all potential objects of interest in the current frame using object detectors such as Faster-RCNN or YOLO. Object association is the process of linking objects detected in the current frame with its corresponding objects from previous frames, referred to as tracklets. Object or instance association is usually done by predicting the object’s location at the current frame based on previous frames’ tracklets using the Kalman Filter followed by one-to-one linear assignment typically using the Hungarian Algorithm to minimise the total differences between the matching results. MetricsMOTP (Multiple Object Tracking Precision)MOTP (Multi-Object Tracking Precision) expresses how well exact positions of the object are estimated. It is the total error in estimated position for matched ground truth-hypothesis pairs over all frames, averaged by the total number of matches made. This metric is not responsible for recognizing object configurations and evaluating object trajectories. MOTA (Multiple Object Tracking Accuracy)MOTA (Multi-Object Tracking Accuracy) shows how many errors the tracker system has made in terms of Misses, False Positives, Mismatch errors, etc. Therefore, it can be derived from three error ratios: the ratio of Misses, the ratio of False positives, and the ratio of Mismatches over all the frames. IDF1 score (IDF1)IDF1 score (IDF1) is the ratio of correctly identified detections over the average of ground truth and predicted detections. BenchmarksOTBKITTIMOT16Methods(models)IOU trackerThe Intersection-Over-Union (IOU) tracker uses the IOU values among the detector’s bounding boxes between the two consecutive frames to perform the association between them or assign a new target ID if no match found. Simple Online And Realtime Tracking (SORT)Simple Online And Realtime Tracking (SORT) is a lean implementation of a tracking-by detection framework.SORT uses the position and size of the bounding boxes for both motion estimation and data association through frames. SORT combines location and motion cues by adopting a Kalman filter to predict the location of the tracklets in the new frame, then computes the IoU between the detection boxes and the predicted boxes as the similarity. DeepSORTDeepSORT replaces the association metric with a more informed metric that combines motion and appearance information. In particular, a “deep appearance” distance metric is added. The core idea is to obtain a vector that can be used to represent a given image. DeepSort adopts a stand-alone RE-ID model to extract appearance features from the detection boxes. After similarity computation matching strategy assigns identities to the objects. This can be done by the Hungarian Algorithm or greedy assignment. FairMOTFairMOT is a new tracking approach built on top of the anchor-free object detection architecture CenterNet.It has a simple network structure that consists of two homogeneous branches for detecting objects and extracting re-ID features. TransMOTTransMOT is a new spatial-temporal graph Transformer that solves all these issues. It arranges the trajectories of all the tracked objects as a series of sparse weighted graphs that are constructed using the spatial relationships of the targets. TransMOT then uses these graphs to create a spatial graph transformer encoder layer, a temporal transformer encoder layer, and a spatial transformer decoder layer to model the spatial-temporal relationships of the objects. ByteTrackBYTE is an effective association method that utilizes all detection boxes from high scores to low ones in the matching process.BYTE is built on the premise that the similarity with tracklets provides a strong cue to distinguish the objects and background in low score detection boxes. BYTE first matches the high score detection boxes to the tracklets based on motion similarity. It uses Kalman Filter to predict the location of the tracklets in the new frame. The motion similarity is computed by the IoU of the predicted box and the detection box. Then, it performs the second matching between the unmatched tracklets. The primary innovation of BYTETrack is keeping non-background low confidence detection boxes which are typically discarded after the initial filtering of detections and use these low-score boxes for a secondary association step. Typically, occluded detection boxes have lower confidence scores than the threshold, but still contain some information about the objects which make their confidence score higher than purely background boxes. Hence, these low confidence boxes are still meaningful to keep track of during the association stage. Comparison of DeepSort and ByteTrackDeepSort uses a pre-trained object detection model to detect objects in each frame and a Siamese network to match the detected objects based on their appearance features. It also uses Kalman filters to predict the locations of the objects in the next frame. ByteTrack, on the other hand, uses a lightweight Siamese network architecture that takes in two input frames and outputs a similarity score. It also uses a simple but effective data augmentation technique to improve its performance on challenging datasets. using ByteTrackByteTracker initiates a new tracklet only if a detection is not matched with any previous tracklet and the bounding box score is higher than a threshold. referenceshttps://www.datature.io/blog/introduction-to-bytetrack-multi-object-tracking-by-associating-every-detection-boxhttps://pub.towardsai.net/multi-object-tracking-metrics-1e602f364c0chttps://learnopencv.com/object-tracking-and-reidentification-with-fairmot/https://medium.com/augmented-startups/top-5-object-tracking-methods-92f1643f8435https://medium.com/@pedroazevedo6/object-tracking-state-of-the-art-2022-fe9457b77382","link":"/2023/08/03/object-tracking/"},{"title":"opencv","text":"basic functionsreading and writingcv2.VideoWriter_fourcc(‘M’, ‘P’, ‘4’, ‘V’)cv2.VideoWriter(filename,fourcc,fps,frameSize[,isColor])cv2.VideoWriter.write(image)","link":"/2023/08/10/opencv/"},{"title":"Probabilistic Artificial Intelligence - Bayesian Linear Regression","text":"Bayesian Linear RegressionLinear RegressionGiven a set of (x, y) pairs, linear regression aims to find a linear model that fits the data optimally.Given the linear model $y=w^Tx$, we want to find optimal weights $w$.There are many ways of estimating w from data, the most common being the least squares estimator: \\hat{w}_\\mathrm{ls}\\doteq\\arg\\min_{\\boldsymbol{w}\\in\\mathbb{R}^d}\\sum_{i=1}^n(y_i-\\boldsymbol{w}^\\top\\boldsymbol{x}_i)^2=\\arg\\min_{\\boldsymbol{w}\\in\\mathbb{R}^d}\\|\\boldsymbol{y}-\\boldsymbol{X}\\boldsymbol{w}\\|_2^2,A slightly different estimator is used for ridge regression: \\hat{w}_{\\mathrm{ridge}}\\doteq\\arg\\min_{w\\in\\mathbb{R}^d}\\|y-Xw\\|_2^2+\\lambda\\|w\\|_2^2As the formula shows, the squared $l_2$ regularization term $\\lambda|w|_2^2$ penalizes large $w$ and thus reduces the complexity of the resulting model,so Ridge regression is more robust than standard linear regression in the presence of multicollinearity. Multicollinearity occurs when multiple independent inputs are highly correlated. In this case, their individual effects on the predicted variable cannot be estimated well. Classical linear regression is highly volatile to small input changes. The regularization of ridge regression reduces this volatility by introducing a bias on the weights towards 0. uncertaintyIn practice, our data D is merely a sample of the process we are modeling. In these cases, we are looking for models that generalize to unseen data.Therefore, it is useful to express the uncertainty about our model due to the lack of data. This uncertainty is commonly referred to as the epistemic uncertainty.Usually, there is another source of uncertainty called aleatoric uncertainty, which originates directly from the process that we are modeling. This uncertainty is the noise in the labels that cannot be explained by the inputs. Weight-space ViewThe most immediate and natural Bayesian interpretation of linear regression is to simply impose a prior on the weights $w$. Assume that prior $w\\sim\\mathcal{N}(0,\\sigma_p^2I)$ and likelihood $y_i\\mid x_i,w\\sim\\mathcal{N}(w^\\top x_i,\\sigma_n^2).$ are both Gaussian, we will get the posterior distribution over the weights as: \\begin{aligned} &\\log p(w\\mid x_{1:n},y_{1:n}) \\\\ &=\\log p(w)+\\log p(y_{1:n}\\mid x_{1:n},w)+\\mathrm{const} \\\\ &=\\log p(w)+\\sum_{i=1}^n\\log p(y_i\\mid x_i,w)+\\mathrm{const}\\\\ &=-\\frac1{2\\sigma_p^2}\\|w\\|_2^2-\\frac1{2\\sigma_n^2}\\sum_{i=1}^n(y_i-w^\\top x_i)^2+\\mathrm{const} \\\\ &=-\\frac1{2\\sigma_p^2}\\|w\\|_2^2-\\frac1{2\\sigma_n^2}\\|y-Xw\\|_2^2+\\mathrm{const} \\\\ &=-\\frac1{2\\sigma_p^2}w^\\top w-\\frac1{2\\sigma_n^2}\\Big(w^\\top X^\\top Xw-2y^\\top Xw+y^\\top y\\Big)+\\mathrm{const.}\\\\ &=-\\frac12(w-\\mu)^\\top\\Sigma^{-1}(w-\\mu)+\\mathrm{const} \\end{aligned}As the above is a quadratic form in $w$, it follows that the posterior distribution is a Gaussian.This also shows that Gaussians with known variance and linear likelihood are self-conjugate. A distribution is said to be self-conjugate (or a conjugate prior is self-conjugate) if, when used as a prior distribution, it results in a posterior distribution that belongs to the same family of distributions. It can be shown more generally that Gaussians with known variance are self-conjugate to any Gaussian likelihood. For general distributions the posterior will not be closed-form. This is a very special property of Gaussians.We can compute the MAP estimate for the weights, \\begin{gathered} \\hat{w}_{\\mathrm{MAP}} =\\underset{w}{\\operatorname*{\\arg\\max}}\\log p(y_{1:n}\\mid x_{1:n},w)+\\log p(w) \\\\ =\\arg\\min_w\\|y-Xw\\|_2^2+\\frac{\\sigma_n^2}{\\sigma_p^2}\\|w\\|_2^2. \\end{gathered}we can find that this is simply the MLE loss with an additional $l_2$ regularization term and this coincides with the optimization objective of ridge regression with weight decay $\\lambda=\\frac{\\sigma_n^2}{\\sigma_p^2}$. Also, recall that the MAP estimate corresponds to the mode of the posterior distribution, which in the case of a Gaussian is simply its mean.(The mode of a probability distribution is the value where the distribution reaches its maximum point. In the context of the posterior distribution, the mode corresponds to the most probable value of the parameter given the observed data.). A commonly used alternative to ridge regression is the least absolute shrinkage and selection operator (or lasso), which uses $l_1$ regularization.It turns out that lasso can also be viewed as Bayesian learning, using a Laplace prior. Note that using point estimates like the MAP estimate does not quantify uncertainty in the weights. The MAP estimate simply collapses all mass of the posterior around its mode.This is especially harmful when we are unsure about the best model. Recursive Bayesian UpdatesAs data arrives online (i.e., in “real-time”), we can obtain the new posterior and use it to replace our prior. Non-linear RegressionWe can use linear regression not only to learn linear functions. The trick is to apply a non-linear transformation $\\phi:\\mathbb{R}^d\\to\\mathbb{R}^e$.In Polynomial regression, to learn polynomials of degree m in d input dimensions, we need to apply the nonlinear transformation \\begin{aligned} \\boldsymbol{\\phi}(x)& =[1,x_1,\\ldots,x_d,x_1^2,\\ldots,x_d^2,x_1\\cdot x_2,\\ldots,x_{d-1}\\cdot x_d, \\\\ &x_{d-m+1}\\cdot\\cdot\\cdot\\cdot\\cdot x_d]. \\end{aligned}the dimension of the feature space grows exponentially in the degree of polynomials and input dimensions. Even for relatively small m and d, this becomes completely unmanageable. Function-space ViewPreviously, we have been interpreting it as a distribution over the weights $w$ of a linear function $\\hat{f}=\\boldsymbol{\\Phi}w.$, we can equivalently consider a distribution directly over the estimated function values. Instead of considering a prior over the weights${w\\sim\\mathcal{N}}(0,\\sigma_p^2I)$, we now impose a prior directly on the values of our model at the observations.Using that Gaussians are closed under linear maps, we obtain the equivalent prior: f\\mid X\\sim\\mathcal{N}(\\boldsymbol{\\Phi}\\mathbb{E}[\\boldsymbol{w}],\\boldsymbol{\\Phi}\\mathrm{Var}[\\boldsymbol{w}]\\boldsymbol{\\Phi}^\\top)=\\mathcal{N}(\\boldsymbol{0},\\underbrace{\\sigma_p^2\\boldsymbol{\\Phi}\\boldsymbol{\\Phi}^\\top}_{\\boldsymbol{\\kappa}})K is the kernel matrix.The kernel function is:$k(x,x^{\\prime})\\doteq\\sigma_p^2\\cdot\\phi(x)^\\top\\phi(x^{\\prime})$The kernel matrix is a covariance matrix and the kernel function measures the covariance of the function values $k(x,x^{\\prime})=\\mathrm{Cov}\\big[f(x),f(x^{\\prime})\\big].$Moreover, note that we have reformulated the learning algorithm such that the feature space is now implicit in the choice of kernel, and the kernel is defined by inner products of (nonlinearly transformed) inputs. In other words, the choice of kernel implicitly determines the class of functions that f is sampled from, and encodes our prior beliefs. This is known as the kernel trick. reference[1] A. Krause, “Probabilistic Artificial Intelligence”.","link":"/2023/10/13/pai0/"},{"title":"Probabilistic Artificial Intelligence - Gaussian Process","text":"Gaussian ProcessGaussian distributionUnivariate Gaussian Distribution is simple, here we focus on multivariate graussian distribution, where each random variable is distributed normally and their joint distribution is also Gaussian. The multivariate Gaussian distribution is defined by a mean vector $\\mu$ and a covariance matrix $\\Sigma$. The covariance matrix is always symmetric and positive semi-definite. why is Gaussian distribution so important? because under the assumptions of the central limit theorem, we can use it to model many events in the real world. Moreover, Gaussian distributions have the nice algebraic property of being closed under conditioning and marginalization. Being closed under conditioning and marginalization means that the resulting distributions from these operations are also Gaussian, which makes many problems in statistics and machine learning tractable. Conditioning is the cornerstone of Gaussian processes since it allows Bayesian inference. Grassian processwhat is GPA Gaussian process is an infinite set of random variables such that any finite number of them are jointly Gaussian.A Gaussian process is characterized by a mean function $\\mu$ and a covariance function (or kernel function) k. Intuitively, a Gaussian process can be interpreted as a normal distribution over functions and is therefore often called an infinite-dimensional Gaussian. Here’s an analogy: Consider a multivariate normal distribution over a set of points in 2D space. Each draw from this distribution corresponds to a vector of values, one for each point. Now, extend this idea to an infinite number of points, and you get a function. The Gaussian process is like having a normal distribution over all possible functions that could describe your data. Mean and covariance functionsThe prior mean function $m(⋅)$ describes the average function under the GP distribution before seeing any data. Therefore, it offers a straightforward way to incorporate prior knowledge about the function we wish to model. In the absence of this type of prior knowledge, a common choice is to set the prior mean function to zero, i.e., $m(⋅)≡0$. The covariance function $k(x,x’)$ computes the covariance $cov[f(x),f(x′)]$ between the corresponding function values by evaluating the covariance functionk at the corresponding inputs x,x′(kernel trick ).Practically, the covariance function encodes structural assumptions about the class of functions we wish to model. These assumptions are generally at a high level and may include periodicity or differentiability. Practically, the covariance function encodes structural assumptions about the class of functions we wish to model. These assumptions are generally at a high level and may include periodicity or differentiability. How GP worksFor a given set of training points, there are potentially infinitely many functions that fit the data. Gaussian processes offer an elegant solution to this problem by assigning a probability to each of these functions. The goal of Gaussian processes is to learn this underlying distribution from training data. Respective to the test data X, we will denote the training data as Y. As we have mentioned before, the key idea of Gaussian processes is to model the underlying distribution ofX together with Y as a multivariate normal distribution. The essential idea of Bayesian inference is to update the current hypothesis as new information becomes available. In the case of Gaussian processes, this information is the training data. Thus, we are interested in the conditional probability$P(X∣Y)$. In Gaussian processes we treat each test point as a random variable. A multivariate Gaussian distribution has the same number of dimensions as the number of random variables. Since we want to predict the function values at $∣X∣=N$ test points, the corresponding multivariate Gaussian distribution is alsoN -dimensional. Making a prediction using a Gaussian process ultimately boils down to drawing samples from this distribution. We then interpret the i-th component of the resulting vector as the function value corresponding to the i-th test point. Marginal likelihood and GP trainingA marginal likelihood is a likelihood function that has been integrated over the parameter space. In Bayesian statistics, it represents the probability of generating the observed sample from a prior and is therefore often referred to as model evidence or simply evidence. The likelihood function represents the probability of observing the given data X given a specific set of parameter values $\\theta$ in a statistical model. It expresses how well the parameters explain the observed data. The likelihood function is a key component in frequentist statistics. It is used to estimate the maximum likelihood estimates (MLE) of the parameters. The marginal likelihood represents the probability of observing the data X without specifying a particular set of parameters. It is obtained by integrating (or summing) the likelihood function over all possible values of the parameters, weighted by the prior distribution of the parameters. The marginal likelihood is a key concept in Bayesian statistics. It serves as a normalizing constant, ensuring that the posterior distribution integrates (or sums) to 1. It is also used in Bayesian model comparison, where different models are compared based on their marginal likelihoods. To train the GP, we maximize the marginal likelihood with respect to the GP hyperparameters,i.e., the parameters of the mean and covariance functions, which we summarize by $\\theta$. \\begin{aligned} p(\\mathbf{y}|\\mathbf{X},\\theta)& =\\int p(\\mathbf{y}|f,\\mathbf{X})p(f|\\mathbf{X})df \\\\ &=\\int\\mathcal{N}(\\mathbf{y}|f(\\mathbf{X}),\\sigma_n^2\\mathbf{I})\\mathcal{N}(f(\\mathbf{X})|\\mathbf{0},\\mathbf{K})df(\\mathbf{X}) \\\\ &=\\mathcal{N}(\\mathbf{y}|\\mathbf{0},\\mathbf{K}+\\sigma_n^2\\mathbf{I}), \\end{aligned}Maximizing the marginal likelihood behaves much better than finding maximum likelihood $\\operatorname*{argmax}{f(\\mathbf{X}),\\sigma_n}p(\\mathbf{y}|f(\\mathbf{X}),\\sigma_n^2\\mathbf{I})$ or maximum a-posteriori point estimates $\\mathop{\\mathrm{argmax}}{f(\\mathbf{X}),\\sigma_n}p(\\mathbf{y}|f(\\mathbf{X}),\\sigma_n^2\\mathbf{I})p(f(\\mathbf{X})|\\theta)$.These two approaches would lead to overfitting, since it is possible to get arbitrarily high likelihoods by placing the function values $f(X)$ on top of the observations y and letting the the noise $\\sigma_n$ tend to zero. In contrast, the marginal likelihood does not fit function values directly, but integrates them out.By averaging (integrating out) the direct model parameters, i.e., the function values, the marginal likelihood automatically trades off data fit and model complexity.Choose a model that is too inflexible, and the marginal likelihood$p(y∣X,θ)$ will be low because few functions in the prior fit the data. A model that is too flexible spreads its density over too many datasets, and so $p(y∣X,θ)$ will also be low. what is kernelIf an algorithm is defined solely in terms of inner products in input space then it can be lifted into feature space by replacing occurrences of those inner products by k(x, x′); this is sometimes called the kernel trick. This technique is kernel trick particularly valuable in situations where it is more convenient to compute the kernel than the feature vectors themselves. The kernel k, which is often also called covariance function, pairwise on all the points. The kernel receives two points$t,t’ \\in \\mathbb{R}^n$ as an input and returns a similarity measure between those points in the form of a scalar. k: \\mathbb{R}^n \\times \\mathbb{R}^n \\rightarrow \\mathbb{R},\\quad \\Sigma = \\text{Cov}(X,X’) = k(t,t’)We evaluate this function for each pairwise combination of the test points to retrieve the covariance matrix. Kernels can be separated into stationary and non-stationary kernels. Stationary kernels, such as the RBF kernel or the periodic kernel, are functions invariant to translations, and the covariance of two points is only dependent on their relative position. Non-stationary kernels, such as the linear kernel, do not have this constraint and depend on an absolute location. The kernel is used to define the entries of the covariance matrix. Consequently, the covariance matrix determines which type of functions from the space of all possible functions are more probable. A kernel function coulde be stationary or isotropic. A kernel function is stationary if $k(x,x’)=k(x-x’)$. A kernel function is isotropic is $k(x,x’)=k(||x-x’||_2)$. Stationarity implies that the covariance function only depends on distances$∥x−x’∥$ of the corresponding inputs, and not on the location of the individual data points. This means that if the inputs are close to each other, the corresponding function values are strongly correlated. Interpretation of the hyperparametersStationary covariance functions typically contain the term $\\frac\\tau l=\\frac{|\\mathbf{x}-\\mathbf{x}^{\\prime}|}l$. where$l$ is a lengthscale parameter. Longer lengthscales cause long-range correlations, whereas for short lengthscales, function values are strongly correlated only if their respective inputs are very close to each other. This allows functions to vary strongly and display more flexibility in the range of the data. The signal variance parameter $\\sigma_f^2$ allows us to say something about the amplitude of the function we model. training tipsThe marginal likelihood is non-convex with potentially multiple local optima. Therefore, we may end up in (bad) local optima when we choose a gradient-based optimization method. In order to initialize these parameters to reasonable values when we optimize the marginal likelihood, we need to align them with what we know about the data, either empirically or using prior knowledge. Assume, we have training inputsX and training targets y. We will see that the signal and noise variances can be initialized using statistics of the training targets, whereas the lengthscale parameters can be initialized using statistics of the training inputs. A reasonable intialization that works well in practice is to set the signal variance to the empirical variance of the observed function values, and the noise variance to a smaller value. Local optima are the largest problem that prevent good lengthscales from being selected through gradient-based optimisation. Generally, we can observe two different types of local optima: Long lengthscale, large noise. Often the lengthscale is so long that the prior only allows nearly linear functions in the posterior. As a consequence, a large amount of noise is required to account for the residuals, leading to a small signal-to-noise ratio. This looks like underfitting, as non-linearities in the data are modelled as noise instead of being learned as part of the function.Short lengthscale, low noise. Short lengthscales allow the posterior mean to fit to small variations in the data. Often such solutions are accompanied by small noise, and therefore a high signal-to-noise ratio. Such solutions look like they overfit, since the means fit the data by making drastic and fast changes, while generalizing poorly. However, the short lengthscale also prevents the predictive error bars from being small, so all predictions will be made with high uncertainty. In the probabilistic sense, this also looks like underfitting. Which optimum we end up in, depends on the initialization of our lengthscale as we are likely to end up in a local optimum nearest to our initial choice. In both cases, the optimizer is more likely to get stuck in a local optimum if the situations are a somewhat plausible explanations of the data. In practice, it is harder to get out of a long lengthscale situation since the optimizer often struggles to get beyond the (typically) huge plateau that is typical for very long lengthscales. How to choose a kernelThe choice of kernel (a.k.a. covariance function) determines almost all the generalization properties of a GP model.​In fact, you might decide that choosing the kernel is one of the main difficulties in doing inference - and just as you don’t know what the true parameters are, you also don’t know what the true kernel is. Probably, you should try out a few different kernels at least, and compare their marginal likelihood on your training data. othersThe GP does not require any input normalization, but it can make sense to do so for numerical reasons. referencehttps://distill.pub/2019/visual-exploration-gaussian-processes/https://www.cs.toronto.edu/~duvenaud/cookbook/ \\https://infallible-thompson-49de36.netlify.app/ \\A. Krause, “Probabilistic Artificial Intelligence”.","link":"/2023/10/12/pai1/"},{"title":"第十一篇 python(2)-flask+gunicorn+supervisor的python服务部署","text":"过程部署一个 Python 服务可以通过 Flask 框架、Gunicorn 服务器和 Supervisor 进程管理工具来完成。 1.安装 Flask、Gunicorn 和 Supervisor：1pip install flask gunicorn supervisor 创建一个 Python 脚本，例如 app.py，并添加一个简单的 Flask 应用。12345678910from flask import Flaskapp = Flask(__name__)@app.route('/')def hello(): return &quot;Hello, World!&quot;if __name__ == '__main__': app.run() 3.创建 Gunicorn 配置文件：创建一个名为 gunicorn_config.py 的文件，配置 Gunicorn 服务器：12bind = '0.0.0.0:8000'workers = 4 4.运行 Gunicorn 服务器：使用 Gunicorn 运行 Flask 应用：1gunicorn -c gunicorn_config.py app:app 5.创建 Supervisor 配置文件：创建一个名为 flask_app.conf 的配置文件，配置 Supervisor：1234567[program:flask_app]directory=/path/to/your/appcommand=/path/to/gunicorn -c /path/to/gunicorn_config.py app:appautostart=trueautorestart=truestderr_logfile=/var/log/flask_app.err.logstdout_logfile=/var/log/flask_app.out.log 6.启动 Supervisor：启动 Supervisor 并加载配置文件：1sudo supervisord -c /etc/supervisor/supervisord.conf 7.检查应用程序状态：使用 Supervisorctl 命令检查应用程序的状态：1sudo supervisorctl status","link":"/2021/08/08/python-1/"},{"title":"第零篇 概率论相关(1)-先验概率、后验概率与似然","text":"先验概率、后验概率与似然今天看到一个比较好的关于先验、后验和似然的通俗解释，先验概率就是基于历史数据的统计经验，后验概率是在已知结果发生时推断原因的概率，似然概率是已知原因推断结果的概率。 根据上述解释，假设我们有一个数据集，这个数据集服从某一种分布，也可以理解为是一个黑盒子模型，黑盒子模型里面包含了很多参数，则似然概率就是已知参数得到某样本的概率，后验概率就是已知某样本得到参数的概率。 为了更理解这一概念，再来看一下著名的贝叶斯公式： P(\\theta \\mid x)=\\frac{p(x \\mid \\theta) p(\\theta)}{p(x)}其中$p(\\theta)$是先验概率， $P(\\theta \\mid x)$是后验概率，$p(x \\mid \\theta)$是似然函数。 这里区分一下两个概念，对于$p(x \\mid \\theta)$如果$\\theta$已知且不变，x是变量，则此函数称为概率函数，而如果x已知且保持不变，$\\theta$是变量，则此函数称为似然函数。 最大似然估计(MLE)最大似然，也就是说要让似然最大，则在数据集上的学习过程就是求模型参数使得当前观察到的样本概率最大，所以最大似然估计的目的就是根据已知的样本结果，反推最有可能导致这个结果的参数值。最大似然估计的适用场景是”模型已定、参数未知”，一个重要前提是样本集中的样本都是独立同分布的随机变量，因为只有独立同分布，样本集的似然函数才能等于各样本似然函数的乘积。 假设一个用于学习的样本集是：$D=\\left{x{1}, x{2}, \\cdots, x{N}\\right}$，来估计参数向量θ，则$l(\\theta)=p(D \\mid \\theta)=p\\left(x{1}, x{2}, \\cdots, x{N} \\mid \\theta\\right)=\\prod{i=1}^{N} p\\left(x{i} \\mid \\theta\\right)$，则使得似然函数最大的参数值求解过程为： \\hat{\\theta}=\\arg \\max _{\\theta} l(\\theta)=\\arg \\max _{\\theta} \\prod_{i=1}^{N} p\\left(x_{i} \\mid \\theta\\right)最大后验估计(MAP)最大后验估计与最大似然估计的不同之处在于最大后验估计中引入了先验概率，因此结合贝叶斯公式和最大似然估计，最大后验估计就转化为了： \\operatorname{argmaxp}(\\theta \\mid X)=\\operatorname{argmax} \\frac{p(X \\mid \\theta) p(\\theta)}{p(X)}=\\operatorname{argmaxp}(X \\mid \\theta) p(\\theta)=\\operatorname{argmax}\\left(\\prod_{x 1}^{x n} p(x i \\mid \\theta)\\right) p(\\theta)L2正则就是加入了高斯先验，L1正则就是加入了拉普拉斯先验。 贝叶斯估计在MLE和MAP中，都是假设模型参数$\\theta$未知，但都是固定的值，属于点估计，而在贝叶斯估计中，假设模型参数是未知的随机变量，而不是确定值，最终得到的参数不是具体的值，而是一个分布，然后用这个分布的期望来作为最终的参数值。 总结最后让我们用大佬讲义中的片段总结一下本篇的主要内容： 参考资料贝叶斯估计、最大似然估计、最大后验概率估计 极大似然估计、最大后验估计","link":"/2019/01/07/probability/"},{"title":"第十六篇 python(3)-matplotlib绘图","text":"安装12pip install matplotlib 简单例子12345678910111213141516import matplotlib.pyplot as plt# 数据x = [1, 2, 3, 4, 5]y = [2, 3, 5, 7, 11]# 绘图plt.plot(x, y)# 添加标题和标签plt.title('Simple Line Plot')plt.xlabel('X-axis')plt.ylabel('Y-axis')# 显示图形plt.show() 更多例子多个子图1234567891011121314151617181920212223242526272829303132333435import matplotlib.pyplot as pltimport numpy as np# 数据x = np.linspace(0, 10, 100)y1 = np.sin(x)y2 = np.cos(x)# 创建一个包含两个子图的图形fig, (ax1, ax2) = plt.subplots(2, 1)# 在第一个子图中绘制正弦函数ax1.plot(x, y1, 'r-')ax1.set_title('Sine Function')ax1.set_xlabel('X-axis')ax1.set_ylabel('Y-axis')# 在第二个子图中绘制余弦函数ax2.plot(x, y2, 'g--')ax2.set_title('Cosine Function')ax2.set_xlabel('X-axis')ax2.set_ylabel('Y-axis')# 设置图形的整体标题plt.suptitle('Sine and Cosine Functions')# 自定义图形的风格plt.style.use('ggplot')# 调整子图的间距plt.tight_layout()# 显示图形plt.show()","link":"/2021/08/31/python-2/"},{"title":"第十八篇 python(4)-多进程","text":"协程、线程与进程协程、线程和进程是计算机编程中常用的并发编程概念。总的来说，协程适合于高并发、I/O 密集型的场景，可以减少线程切换的开销；线程适合于 CPU 密集型和需要实时响应的任务；而进程则适合于独立性强、资源隔离要求高的任务。在实际应用中，通常会根据任务的特点和需求选择合适的并发编程模型。 协程协程是一种程序组件，类似于线程，但其执行过程是可中断的。在协程中，执行可以在特定的位置暂停，并在需要时恢复。协程通常在单个线程中运行，因此不涉及线程切换的开销，可以有效地利用系统资源。 线程线程是操作系统能够进行运算调度的最小单位。一个进程中可以包含多个线程，它们共享进程的内存空间和其他资源。多线程编程允许程序同时执行多个任务，提高了程序的并发性和响应性。线程之间的切换开销相对较小，但线程间的共享资源需要进行同步，以避免竞态条件和死锁等问题 进程进程是程序执行时的一个实例，每个进程都有自己独立的内存空间和系统资源。进程间相互独立，各自拥有独立的地址空间和资源，彼此之间通信需要特殊的机制。多进程编程可以充分利用多核处理器，但进程间的切换开销相对较大，因为需要切换地址空间和资源上下文。 python如何使用协程、线程与进程在Python中，可以使用不同的工具来实现协程、线程和进程。在Python中，协程通常使用 asyncio 库来实现。 线程可以使用内置的 threading 模块来实现。进程可以使用 multiprocessing 模块来实现。需要注意的是，在Python中，由于全局解释器锁（GIL）的存在，多线程并不能有效利用多核处理器。因此，如果需要充分利用多核处理器，可以考虑使用多进程。而协程则是在单个线程中实现并发的一种方式，适合于I/O密集型任务。","link":"/2021/09/22/python-3/"},{"title":"python(5) subprocess and logging","text":"subprocessYou can use the Python subprocess module to create new processes, connect to their input and output, and retrieve their return codes and/or output of the process. subprocess runThe subprocess.run() method is a convenient way to run a subprocess and wait for it to complete. Once the subprocess is started, the run() method blocks until the subprocess completes and returns a CompletedProcess object, which contains the return code and output of the subprocess.The check argument is an optional argument of the subprocess.run() function in the Python subprocess module. It is a boolean value that controls whether the function should check the return code of the command being run.When check is set to True, the function will check the return code of the command and raise a CalledProcessError exception if the return code is non-zero. The exception will have the return code, stdout, stderr, and command as attributes. subprocess Popensubprocess.Popen is a lower-level interface to running subprocesses, while subprocess.run is a higher-level wrapper around Popen that is intended to be more convenient to use. Popen allows you to start a new process and interact with its standard input, output, and error streams. It returns a handle to the running process that can be used to wait for the process to complete, check its return code, or terminate it.In general, you should use run if you just need to run a command and capture its output and Popen if you need more control over the process, such as interacting with its input and output streams.The Popen class has several methods that allow you to interact with the process, such as communicate(), poll(), wait(), terminate(), and kill(). subprocess callsubprocess.call() is a function in the Python subprocess module that is used to run a command in a separate process and wait for it to complete. It returns the return code of the command, which is zero if the command was successful, and non-zero if it failed.subprocess.call() is useful when you want to run a command and check the return code, but do not need to capture the output. subprocess check_outputcheck_output is a function in the subprocess module that is similar to run(), but it only returns the standard output of the command, and raises a CalledProcessError exception if the return code is non-zero. Subprocess PipeA pipe is a unidirectional communication channel that connects one process’s standard output to another’s standard input. A pipe can connect the output of one command to the input of another, allowing the output of the first command to be used as input to the second command.Pipes can be created using the subprocess module with the Popen class by specifying the stdout or stdin argument as subprocess.PIPE. loggingLogging provides a set of convenience functions for simple logging usage. These are debug(), info(), warning(), error() and critical().The default level is WARNING, which means that only events of this level and above will be tracked, unless the logging package is configured to do otherwise. logging configlogging.basicConfig(format=’%(levelname)s %(asctime)s %(process)d %(message)s’, level=logging.DEBUG)If not printing after config, note that you should config this before importing other libraries incase the config is overriden. referencehttps://www.datacamp.com/tutorial/python-subprocesshttps://docs.python.org/3/howto/logging.html","link":"/2023/06/28/python-4/"},{"title":"第五篇 算法(1)--排序","text":"排序排序方法选择排序选择排序的思路是找到数组中最小的数和第一个元素交换位置，然后在剩下的元素中找到最小的元素和第二个元素交换位置，直到最后只剩一个元素，这样就是一个从小到大排好序的数组。选择排序的复杂度为O(n^2)。 插入排序插入排序的思路是将元素插入一个已经排好序的子列表中，直到整个列表都排序完成。插入排序的复杂度为O(n^2)。 冒泡排序冒泡排序的思路是每次对连续的邻居元素进行比较，两个元素是逆序排序就交换位置，否则保持不变，直到所有元素都被排序好。每次进行完一轮比较，就能将最大或者最小的元素移到其最终的位置上，当不再发生交换的时候，就说明元素已经被排好序了，冒泡排序的复杂度是O(n^2)。 归并排序归并排序的思路是利用递归的方法，将数组分为两半，各自进行归并排序的过程。其关键是如何将排好序的两个子数组也排好序，鉴于两个子数组是已经排好序了的，只需要将两个子数组依次比较。归并排序的复杂度是O(nlogn）。 快速排序快速排序的思路是挑出一个中心点，把数组分为两半，其中一半所有元素都小于这个中心点，另一半大于这个中心点，再对这两半进行递归处理，所以快速排序的关键在于这个中心点的选择了。快速排序的复杂度是O(nlogn）。 堆排序堆排序用了二叉堆，将一个数组中的所有元素添加到堆中，然后将堆中最大的元素连续移除以获得一个排好序的数组。一个二叉堆具有如下性质：是一个完全二叉树；每个节点都大于或等于它的子节点，二叉堆通常是用数组实现的，父母节点和子节点的位置满足一定的关系，假如一个在位置i的节点，它的左子节点就在位置2i+1上，右子节点在位置2i+2上。所以堆排序的关键在于二叉堆的建立和维护。堆排序的复杂度是O(nlogn)。 桶排序和基数排序桶排序和基数排序用于排序整数非常有效。 桶排序 ​ 桶排序的思路是加入数组中的元素在0到t的范围内，则把这些元素放入对应的标记上0到t的桶当中，每个桶中的元素值都是相同的。 基数排序 在桶排序中，如果元素范围过大的话，就会需要很多桶，此时就可以用基数排序。基数排序基于桶排序，只是基数排序只会用到十个桶，基于基数位置进行桶排序。 外排序当数据量大到无法一次性载入内存时，使用外排序。外排序的思路就是将大量数据拆分成小块数据，小块数据进行内排序之后，再分别合并排序。 相关java基础数组数组一旦创建了，大小就是固定的。java中声明数组变量的语法是’elementType[] arrayRefVar;’，声明数组只是创造了一个数组引用的存储位置，并没有为这个数组分配内存，创建一个数组可以使用new操作符，例如’array RefVar = new elementType[arraySize];’。 数组的拷贝数组变量是一个数组的引用，直接使用赋值语句只是让两个变量去指向同一个数组（同一片存储空间），要想真正地拷贝数组有几种方式：使用循环对数组中的元素一个一个地拷贝；使用System类中的静态方法arraycopy；使用clone方法。arraycopy的语法是’arraycopy(sourceArray,srcPos,targetArray,tarPos,length);’。","link":"/2020/04/04/sort/"},{"title":"第九篇 python(1)-语法进阶","text":"yieldyield可以暂停一个函数的运行，返回值给函数调用者，并使得函数可以从上次离开的地方接着运行。通常我们可以借助yield来实现一个生成器。 生成器生成器是一个可以产生一个序列的函数，调用生成器函数会产生一个生成器对象，并不会开始启动函数，只有当执行__next__()时函数才会执行。生成器时一个一次性操作，和我们常见的列表、字典等可以迭代的对象不同，列表等是可以无限次迭代的。 装饰器python中函数是一等对象，所以函数可以像普通变量一样当作参数传给另一个函数的，装饰器可以在不改变另一个函数的情况下用来封装另一个函数以拓展这个被封装函数的功能，有了装饰器，我们就可以抽离出大量与函数功能本身无关的雷同代码到装饰器中并继续重用。 装饰器不仅可以是函数，也可以是类。使用类装饰器主要依靠类的__call__方法。我们可以直接定义一个装饰器函数或者装饰器类，但是有个缺点是原函数的元信息不见了，比如函数的docstring、__name__都会发生改变，此时我们可以使用functools.wraps，wraps本身也是一个装饰器，它能把原函数的元信息拷贝到装饰器里面的函数中。","link":"/2021/07/30/python/"},{"title":"segmentation","text":"Image segmentationImage segmentation is a sub-domain of computer vision and digital image processing which aims at grouping similar regions or segments of an image under their respective class labels. Semantic segmentationSemantic segmentation refers to the classification of pixels in an image into semantic classes. Instance segmentationInstance segmentation models classify pixels into categories on the basis of “instances” rather than classes. Panoptic segmentationPanoptic segmentation can be expressed as the combination of semantic segmentation and instance segmentation where each instance of an object in the image is segregated and the object’s identity is predicted. Neural networks that perform segmentation typically use an encoder-decoder structure where the encoder is followed by a bottleneck and a decoder or upsampling layers directly from the bottleneck (like in the FCN).","link":"/2023/07/18/segmentation/"},{"title":"speech","text":"signalspectrogram: A spectrogram of a time signal is a special two-dimensional representation that displays time in its horizontal axis and frequency in its vertical axis. short-time Fourier analysisWhy use it?Some regions of speech signals shorter than 100 milliseconds often appear to be periodic, so that we can use the exact definition of Fouriertransform. spectral leakageThis phenomenon is called spectral leakage because the amplitude of one harmonic leaks over the rest and masks its value. feature extractionRepresentation of speech signals in the frequency domain is especially useful because the frequency structure of a phoneme is generally unique. Sinusoids are important because speech signals can be decomposed as sums of sinusoids. For voiced sounds there is typically more energy at low frequenciesthan at high frequencies, also called roll-off. To make the spectrograms easier to read, sometimes the signal is first preemphasized (typically with a first-order difference FIR filter) to boost the high frequenciesto counter the roll-off of natural speech. Digital SystemsLinear Time-Invariant Systems and Linear Time-Varying Systems. The Fourier TransformZ-Transformdigital filterfilterbankA filterbank is a collection of filters that span the whole frequency spectrum. short-time analysis","link":"/2023/03/31/speech/"},{"title":"speech embedding","text":"Contrastive Predictive CodingContrastive Predictive Coding (CPC) learns self-supervised representations by predicting the future in latent space by using powerful autoregressive models. The model uses a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It describes a form of unidirectional modeling in the feature space,where the model learns to predict the near future frames inan acoustic sequence while contrasting with frames from othersequences or frames from a more distant time. Autoregressive Predictive CodingThe APC approach uses an autoregressive model to encodetemporal information of past acoustic sequence; the model thenpredicts future frames like a recurrent-based LM whileconditioning on past frames. TERATERA, which stands for Transformer Encoder Representations from Alteration, is a self-supervised speech pre-trainingmethod. experiment designAmount of labeled data needed to perform well.with pre-trained and without pre-trained.","link":"/2023/05/05/speech-embedding/"},{"title":"thoughts","text":"","link":"/2022/03/13/thoughts/"},{"title":"time complexity","text":"time complexitybig O notationBig O notation measures the asymptotic growth of a function. f (n) = O(g(n)) if for all sufficiently large n, f (n) is at most a constant factor larger than g(n). Ω and Θ notationWe say f (n) = Ω(g(n)) if g(n) = O(f (n)).We say f (n) = Θ(g(n)) if f (n) = O(g(n)) and g(n) = O(f (n)). types of complexityWorst-case complexity: what is the largest possible running time on any input of size n?Average-case complexity: what is the average running time on a random input of size n?Best-case complexity: what is the smallest possible running time on any input of size n? Graph algorithmways of representing graphsadjacency matrixFor graph with n vertices this is an n × n matrix A, where $A{ij}$ = 1 if there is an edge from node i to node j, $A{ij}$ = 0 otherwise.If the graph is undirected, the matrix is symmetric adjacency listsFor each vertex, keep a list of its neighbors. incidence matrixThe incidence matrix of an undirected graph with n vertices and m edges is an n × m matrix B where $B{ij}$ = 1 if the i’th vertex is part of the j’th edge, $B{ij}$ = 0 otherwise. Two fundamental graph exploration algorithmsDepth First Search (DFS)Breadth First Search (BFS)For the BFS tree, this gives the shortest (fewest number of steps) paths from s to all other nodes Greedy AlgorithmsGreedy algorithms are algorithms that build a solution step by step by always choosing the currently best option. Interval SchedulingInput: A list of intervalsOutput: Maximum number of these intervals that can be chosen without getting any overlaps.Solution:Pick the one that ends first.Prove correctness of such an algorithm: Common strategy for analyzing greedy algorithms: prove that the algorithm always “stays ahead” of the optimal solution. Job Scheduling With Minimum LatenessInput: A list of jobs, each job has a deadline di, and a duration ti(how long it takes to finish the job)Output: Smallest possible maximum lateness in a schedule for doingall jobs.Solution: Pick the job with smallest di. Shortest pathIt is helpful to instead consider a more general problem. Let us tryto find the shortest paths from s to all other vertices: Dijkstra’s algorithm: we have some set D of vertices we have foundthe shortest path to, and each step we add a new vertex to D. add the vertex outside D which is closest tos when using only vertices in D as intermediate vertices. Divide &amp; ConquerAlgorithms that split the input into significantly smaller parts, recursively solves each part, and then combines the subresults (somehow). Merge sortO(n log n). Polynomial multiplication$T(n) = O(n^{1.59})$.Using FFT, get time O(n log n) for Polynomial Multiplication Unit cost model and Bit cost modelUnit cost model: assume all numbers fit in machine registers so that basic arithmetic operations take constant time.Bit cost model: account for size of numbers and the time it takes to manipulate them. Integer multiplicationKaratsuba’s algorithm: $T(n) = O(n^{1.59})$. Master TheoremDynamic ProgrammingSplit a problem into smaller subproblems such that results from onesubproblem can be reused when solving others Fibonacci numbersThe Fibonacci numbers are a classic number sequence inmathematics, defined by the linear recurrencef0 = 0; f1 = 1; and fn = fn−1 + fn−2 for n ≥ 2 Weighted Interval SchedulingInput: A list of intervals [s1; t1]; [s2; t2]; : : : ; [sn; tn], each interval[si; ti] has a weight wiOutput: What is the maximum total weight of these intervals thatcan be chosen without getting any overlaps KnapsackInput: A capacity C and a list of objects, each object has a value viand weight wiOutput: Subset S of objects such that$\\sum{i∈S} wi ≤ C$ and $\\sum{i∈S} vi$ is maximized. top-down and bottom-up fashiontop-down fashion: we start at the end result andrecursively compute results for relevant subproblems. bottom-up fashion: we iteratively compute results for larger and larger subproblems. Characteristics of dynamic programmingA problem is amenable to dynamic program if we can define a setof subproblems such that: The number of different subproblems is as small as possible. There is some ordering of subproblems from “small” to “large” The value of a subproblem can be efficiently computed giventhe values of some set of smaller subproblems. Sequence AlignmentInput: Strings x and y of lengths m and n, parameters ‹ and ¸Output: Minimum cost of an alignment of x and y with parameters $\\sigma$ and $\\alpha$.$\\alpha is the cost of aligning two different characters with each other$\\sigma$ is the cost of not aligning a character Matrix Chain MultiplicationNetwork FlowThe Max-Flow problemInput: Flow network G.Output: Flow f maximizing the value v(f ).Solution: The Ford-Fulkerson Algorithm O(C(m + n)) or the scaling algorithm with O(m2log(C)) or Edmonds-Karp algorithm with O(nm(n + m)) . Edge CutsAn edge cut of a graph is a set of edges such that their removal would disconnect the graph. Minimum s-t-CutInput: A flow network G with source s and sink t.Output: An s-t cut A; B of G minimizing the capacity c(A; B). The Max-Flow-Min-Cut TheoremFor every flow network G, the maximum flow from s to t equals theminimum capacity of an s-t cut in G. Vertex CutsA vertex cut in a graph is a set of vertices such that if we removethem, the graph splits into more than one connected component. MatchingsA matching in a graph is a set M of edges such that no vertex appears in more than one edge of M.Of particular interest to us will be bipartite graphs. Maximum Bipartite MatchingInput: A bipartite graph GOutput: A matching M in G of maximum possible size. Edge-Disjoint PathsGiven a directed graph with source and sink, what is maximumnumber of edge-disjoint paths from s to t?(edge-disjoint = no edge used by more than one path) Project Selection?","link":"/2023/05/16/time-complexity/"},{"title":"toolnotes","text":"背景记录平时学习和开发工程中使用工具的一些备忘点。 正文用vim时，鼠标右键不能粘贴而是进入了visual模式，解决方法：：set mouse-=a 远程jupyter配置https://juejin.cn/post/7026371559971520525 For Debian / Ubuntu: .deb packages installed by apt and dpkgFor Rocky / Fedora / RHEL: .rpm packages installed by yumFor FreeBSD: .txz packages installed by pkg ssh-keygen -t rsa -b 4096 -C “your_email@example.com” Kill PyTorch Distributed Training Processes:kill $(ps aux | grep YOUR_TRAINING_SCRIPT.py | grep -v grep | awk ‘{print $2}’) git reset —soft HEAD^ 撤销commitgit reset —hard HEAD^ 撤销add tokenizing in Unix: “tr” command 分词文件排序和统计 tr -sc ’A-Za-z’ ’\\n’ &lt; $file_name| sort | uniq -c conda: conda create -n python=3.7 yourenv pip git find . -name “*.py”|xargs git add — 导出项目依赖：pip install pipreqspipreqs ./ —encoding=utf-8 —force inside docker container, cannot locate package with apt install:echo “deb http://archive.debian.org/debian stretch main” &gt; /etc/apt/sources.list https://unix.stackexchange.com/questions/743839/apt-get-update-failed-to-fetch-debian-amd64-packages-while-building-dockerfile-f PlotNeuralNet https://pub.towardsai.net/creating-stunning-neural-network-visualizations-with-chatgpt-and-plotneuralnet-adab37589e5 remote develophttps://devblogs.microsoft.com/python/remote-python-development-in-visual-studio-code/ references”https://leimao.github.io/blog/Kill-PyTorch-Distributed-Training-Processes/","link":"/2023/01/03/toolnotes/"},{"title":"transformer","text":"IntroductionThe Transformer is a deep learning architecture introduced in the paper “Attention is All You Need” by Vaswani et al., published in 2017.The Transformer is based on the self-attention mechanism, which allows it to capture long-range dependencies in sequences more effectively than traditional recurrent neural networks (RNNs) or convolutional neural networks (CNNs). The key components of the Transformer are:Self-Attention Mechanism,Encoder-Decoder Architecture,Multi-Head Attention,Positional Encoding,Feed-Forward Neural Networks. Self-Attention MechanismThe self-attention mechanism allows the model to weigh the importance of different words in a sentence while encoding the sequence. It computes the attention scores for each word in the input sequence based on its relationships with other words. By attending to relevant words, the model can focus on the most informative parts of the sequence. The first step in calculating self-attention is to create three vectors from each of the encoder’s input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process. The second step in calculating self-attention is to calculate a score. Say we’re calculating the self-attention for the first word in this example, “Thinking”. We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.The score is calculated by taking the dot product of the query vector with the key vector of the respective word we’re scoring. So if we’re processing the self-attention for the word in position #1, the first score would be the dot product of q1 and k1. The second score would be the dot product of q1 and k2. The third and fourth steps are to divide the scores by 8 (the square root of the dimension of the key vectors used in the paper – 64. This leads to having more stable gradients. There could be other possible values here, but this is the default), then pass the result through a softmax operation. Softmax normalizes the scores so they’re all positive and add up to 1. The fifth step is to multiply each value vector by the softmax score. The sixth step is to sum up the weighted value vectors.This produces the output of the self-attention layer at this position (for the first word). Multi-Head AttentionTo capture different types of dependencies and relationships, the Transformer uses multi-head attention. It performs self-attention multiple times with different learned projection matrices, allowing the model to attend to various aspects of the input. With multi-headed attention we have not only one, but multiple sets of Query/Key/Value weight matrices (the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder). Each of these sets is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders/decoders) into a different representation subspace.If we do the same self-attention calculation we outlined above, just eight different times with different weight matrices, we end up with eight different Z matrices.We concat the matrices then multiply them by an additional weights matrix WO to condense these eight down into a single matrix. sequence-to-sequence modelA sequence-to-sequence model is a model that takes a sequence of items (words, letters, features of an images…etc) and outputs another sequence of items.the model is composed of an encoder and a decoder.The encoder processes each item in the input sequence, it compiles the information it captures into a vector (called the context). After processing the entire input sequence, the encoder sends the context over to the decoder, which begins producing the output sequence item by item.The context is a vector (an array of numbers, basically) in the case of machine translation. The encoder and decoder tend to both be recurrent neural networks.By design, a RNN takes two inputs at each time step: an input (in the case of the encoder, one word from the input sentence), and a hidden state. The word, however, needs to be represented by a vector. To transform a word into a vector, we turn to the class of methods called “word embedding” algorithms.The context vector turned out to be a bottleneck for these types of models. It made it challenging for the models to deal with long sentences.A solution was proposed in Bahdanau et al., 2014 and Luong et al., 2015. These papers introduced and refined a technique called “Attention”.Attention allows the model to focus on the relevant parts of the input sequence as needed. attentionAn attention model differs from a classic sequence-to-sequence model in two main ways:First, the encoder passes a lot more data to the decoder. Instead of passing the last hidden state of the encoding stage, the encoder passes all the hidden states to the decoder;Second, an attention decoder does an extra step before producing its output. In order to focus on the parts of the input that are relevant to this decoding time step, the decoder does the following:Look at the set of encoder hidden states it received,Give each hidden state a score,Multiply each hidden state by its softmaxed score, thus amplifying hidden states with high scores, and drowning out hidden states with low scores. Encoder-Decoder ArchitectureThe Transformer architecture consists of two main components: the encoder and the decoder. The encoder takes an input sequence and processes it, while the decoder generates an output sequence based on the encoded representation. One detail in the architecture of the encoder that we need to mention before moving on, is that each sub-layer (self-attention, ffnn) in each encoder has a residual connection around it, and is followed by a layer-normalization step. The encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence.The following steps repeat the process until a special symbol is reached indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did. And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word. In the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to -inf) before the softmax step in the self-attention calculation.The “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack. The decoder stack outputs a vector of floats. How do we turn that into a word? That’s the job of the final Linear layer which is followed by a Softmax Layer.The Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector.The softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step. Layer NormalizationIn traditional normalization techniques like Batch Normalization, the activations of a layer are normalized by computing the mean and variance over a batch of examples. This normalization helps stabilize and accelerate the training process, especially for deeper networks. However, it introduces a dependency on the batch size during training, which can be problematic in scenarios where batch sizes vary or during inference when processing individual samples. Layer Normalization addresses this dependency by computing the mean and variance across all the units within a single layer for each training example. This means that normalization is done independently for each sample and does not rely on batch statistics. Positional EncodingSince Transformers do not inherently have positional information like RNNs, positional encodings are added to the input embeddings. These positional encodings provide the model with information about the order of the elements in the input sequence,or the distance between different words in the sequence. Trainingloss functioncross entropyThe cross-entropy loss calculates the negative log-likelihood of the true class’s predicted probability. Kullback–Leibler divergenceKullback-Leibler (KL) divergence, also known as relative entropy, is a measure of how one probability distribution diverges from another.KL divergence measures the average amount of information lost when using Q to approximate P. It is not symmetric. decodinggreedy decodingIn greedy decoding, at each step of sequence generation, the model selects the most likely output token based on its predicted probability distribution. It chooses the token with the highest probability without considering the impact on future decisions. This means that the model makes locally optimal choices at each step without considering the global context of the entire sequence.For example, in machine translation, a model using greedy decoding will predict each target word one at a time, selecting the word with the highest probability given the source sentence and previously generated words. The process continues iteratively until an end-of-sentence token is generated. Beam searchIn beam search, instead of selecting only the most likely token at each step, the algorithm maintains a fixed-size list, known as the “beam,” containing the most promising candidate sequences. The beam size determines how many candidate sequences are considered at each decoding step.At the beginning of the decoding process, the beam is initialized with a single token representing the start of the sequence. At each step, the model generates the probabilities for the next possible tokens and expands the beam with the top-k most likely candidate sequences based on their cumulative probabilities. The k represents the beam size, and higher values of k result in a more diverse exploration of possibilities. referenceshttp://fancyerii.github.io/2019/03/09/transformer-illustrated/#%E6%A6%82%E8%BF%B0https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/http://jalammar.github.io/illustrated-transformer/https://colah.github.io/posts/2015-09-Visual-Information/https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained","link":"/2023/07/19/transformer/"},{"title":"object detection","text":"Object detectionObject detection is the field of computer vision that deals with the localization and classification of objects contained in an image or video.Deep learning-based approaches use neural network architectures like RetinaNet, YOLO (You Only Look Once), CenterNet, SSD (Single Shot Multibox detector), Region proposals (R-CNN, Fast-RCNN, Faster RCNN, Cascade R-CNN) for feature detection of the object, and then identification into labels.The YOLO series current provide the SOTA of object detection in real-time. Object detection usually consists of the following parts:Input: Refers to the input of the pictureBackbone: A skeleton pre-trained on ImageNetNeck: Usually used to extract feature maps of different levelsHead: Predict the object category and the detector of bndBox, usually divided into two types: Dense Prediction (one stage), Sparse Prediction (two stage). metricmAPMean average precision (mAP) is the average value of AP of each category.The AP metric is the area under curve (AUC) of PR curve (Precision-Recall curve).This metric provides a balanced assessment of precision and recall by considering the area under the precision-recall curve. PR curve is a curve drawn with Recall as the X axis and Precision as the Y axis. The higher the Precision and Recall, the better the performance of the model, so the closer to the upper right corner, the better. The AP metric incorporates the Intersection over Union (IoU) measure to assess the quality of the predicted bounding boxes. If the IOU is greater than the threshold (Threshold, usually set to 0.5), and the same Ground Truth can only be calculated once, it will be considered as a TP. Intersection over Union(IoU)IoU is the ratio of the intersection area to the union area of the predicted bounding box and the ground truth bounding box. It measures the overlap between the ground truth and predicted bounding boxes. Flops and FPSFLOPS (Floating-Point Operations Per Second) is a measure of a computer’s or a processor’s performance in terms of the number of floating-point operations it can perform per second.Higher FLOPS values generally indicate faster computational capabilities.FPS (Frames Per Second) is a measure of how many individual frames (images) a video system can display or process per second. Non-Maximum Suppression (NMS)Non-Maximum Suppression (NMS) is a post-processing technique used in object detection algorithms to reduce the number of overlapping bounding boxes and improve the overall detection quality. Model HistoryTraditionally, object detection is done by Viola Jones Detector \\cite{viola2001rapid}, Histogram of Oriented Gradients (HOG) detector, or Deformable Part-based Model (DPM) before deep learning took off. With deep learning, object detection generally is categorized into 2 categories: one-stage detector and two-stage detector. Two-stage detector is started by Regions with CNN features (RCNN). Spatial Pyramid Pooling Networks (SPPNet), Fast RCNN, Faster RCNN, and Feature Pyramid Networks (FPN) were proposed after it. Limited by the poor speed of the two-stage detector, the one-stage detector came with the first representative You Only Look Once (YOLO). Subsequent versions of YOLO, Single Shot MultiBox Detector (SSD), RetinaNet, CornerNet, CenterNet,DETR were proposed latter. YOLOv7 performs best compared to most detectors. RCNNThe object detection system consists of three modules. The first generates category-independent region proposals. These proposals define the set of candidate detections available to our detector. The second module is a large convolutional neural network that extracts a fixed-length feature vector from each region. The third module is a set of classspecific linear SVMs. YOLO seriesThe history of YOLO (You Only Look Once) dates back to 2015 when the original YOLO algorithm was introduced in “You Only Look Once: Unified, Real-Time Object Detection,” .The original YOLO architecture used a convolutional neural network (CNN) to process the entire image and output a fixed number of bounding boxes along with their associated class probabilities. It divided the image into a grid and applied convolutional operations to predict bounding boxes within each grid cell, considering multiple scales and aspect ratios.In subsequent years, YOLO underwent several iterations and improvements to enhance its accuracy and speed. YOLOv2 was introduced in 2016, featuring an updated architecture that incorporated anchor boxes and multi-scale predictions. YOLOv3 followed in 2018, introducing further advancements, including feature pyramid networks (FPN) and Darknet-53 as the backbone architecture. YOLO (You Only Look Once)Network architecture is inspired by the GoogLeNet model for image classification.The network has 24 convolutional layers followed by 2 fully connected layers. They pretrain our convolutional layers on the ImageNet 1000-class competition dataset.For pretraining they use the first 20 convolutional layers followed by a average-pooling layer and a fully connected layer. Then they add four convolutional layers and two fully connected layers with randomly initialized weights. The final layer predicts both class probabilities and bounding box coordinates. They optimize for sum-squared error in the output of the model by increasing the loss from bounding box coordinate predictions and decreasing the loss from confidence predictions for boxes that don’t contain objects and predicting the square root of the bounding box width and height instead of the width and height directly. They design the loss to handle the problem that the sum-squared error weights localization error equally with classification error and also equally weights errors in large boxes and small boxes. YOLOv2The improvements of YOLOv2 in YOLOv1:The author adds a batch normalization layer after each convolutional layer, no longer uses dropout.YOLOv1 uses a 224x224 image classifier. YOLO2 increases the resolution to 448x448.Because YOLOv1 has difficulty learning to adapt to the shape of different objects during training, resulting in poor performance in precise positioning. YOLOv2 also tries to use rectangles of different shapes as anchor boxes (Anchor Box).Unlike YOLOv1, Anchor Box does not directly predict the coordinate value of bndBox, but predicts the offset (offset value of coordinates) and confidence scores (confidence) of Anchor Box.In Faster R-CNN and SSD, the size of the Anchor Box is manually selected.YOLOv2 uses the k-means clustering method to perform cluster analysis on the bndBox of the objects in the training set.YOLOv2 uses a new basic model (feature extractor) Darknet-19, including 19 convolutional layers, 5 maxpooling layers. YOLO9000YOLO9000 is a model that can detect more than 9,000 categories proposed on the basis of YOLOv2. Its main contribution is to propose a joint training strategy for classification and detection.For the detection data set, it is used to learn the bounding box (bndBox), confidence (confidence) and object classification of the predicted object, while for the classification data set, it is only used to learn classification, but it can greatly expand the capabilities of the model the type of object detected. The author proposes a hierarchical classification method (Hierarchical classification),which establishes a tree structure WordTree according to the affiliation between categories.When softmax is performed, it is not performed on all categories, but on the categories of the same level.When making predictions, it traverses down from the root node, selects the child node with the highest probability at each level, and calculates the product of all conditional probabilities from the node to the root node. Stop when the product of the conditional probability is less than a certain threshold, and use the current node to represent the predicted category. YOLOv3On the basis of YOLOv2, YOLOv3 improves the network backbone, uses multi-scale feature maps (feature maps) for detection, and uses multiple independent Logistic regression classifiers instead of softmax to predict category classification.YOLOv3 proposes a new backbone: Darknet-53, from layer 0 to layer 74, a total of 53 convolutional layers, and the rest are Resnet layers.Darknet-53 joins Resnet Network (Residual Network) to solve the gradient problem.YOLOv3 draws on the Feature Pyramid Network (FPN) method, uses multi-scale feature maps to detect objects of different sizes, and improves the prediction ability of small objects.The feature map of each scale will predict 3 Anchor priors, and the size of the Anchor priors is clustered using K-means. Feature Pyramid Networks (FPN)The main idea behind FPNs is to leverage the nature of convolutional layers — which reduce the size of the feature space and increase the coverage of each feature in the initial image — to output predictions at different scales.FPNs provide semantically strong features at multiple scales which make them extremely well suited for object detection. YOLOv4Bag-of-Freebies refers to the techniques used in network training, which does not affect the time of reasoning and prediction, mainly including:Data augmentation: Random erase, CutOut, Hide-and-seek, Grid mask, GAN, MixUp, CutMix;Regularization methods: DropOut, DropConnect;Dealing with data imbalance: focal loss, Online hard example mining, Hard negative example mining;Handle bndBox regression problems: MSE, IOU, GIOU, DIOU/CIOU. Bag-of-specials refers to the techniques used in network design or post-processing, which slightly increases the time of reasoning and prediction, but can improve the accuracy, mainly including:Receptive field: SPP, ASPP, RFB;Feature Fusion: FPN, PAN;Attention mechanism: attention module;Activation functions: Swish, Mish;NMS: Soft-NMS、DIoU NMS. The architecture of the YOLOv4 model consists of three partsBackBone: CSPDarknet53; Neck: SPP+PAN; HEAD: YOLO HEAD. Cross Stage Partial Network (CSPNet)The main purpose of CSPNet is to enable the network architecture to obtain richer gradient fusion information and reduce the amount of calculation.The method is to first divide the feature map of the Base layer into two parts, and then pass through transition -&gt; concatenation -&gt; transition. parts merged.This approach allows CSPNet to solve three problems:Increase the learning ability of CNN, even if the model is lightweight, it can maintain accuracy;Remove the computing bottleneck structure with high computing power (reduce computing);Reduce memory usage. SPP+PANSPP (Spatial Pyramid Pooling): Concate all feature maps in the last layer of the network, and then continue to connect CNN module.PANet (Path Aggregation Network): Improve on the basis of FPN. CutMixCutMix is ​​a data enhancement method proposed in 2019. The method is to cut off a part of the area but not fill it with 0 pixels, but randomly fill the area pixel values ​​​​of other data in the training set.Mixup: Mix two random samples proportionally, and the classification results are distributed proportionally.utout: Randomly cut out some areas in the sample and fill them with 0 pixel values, and the classification result remains unchanged. Mosaic data augmentationWhilst common transforms in object detection tend to be augmentations such as flips and rotations, the YOLO authors take a slightly different approach by applying Mosaic augmentation; which was previously used by YOLOv4, YOLOv5 and YOLOX models.The objective of mosaic augmentation is to overcome the observation that object detection models tend to focus on detecting items towards the centre of the image. The key idea is that, if we stitch multiple images together, the objects are likely to be in positions and contexts that are not normally observed in images seen in the dataset; which should force the features learned by the model to be more position invariant. It uses random scaling and cropping to mix and stitch 4 kinds of pictures for training. When using Mosaic training, the data of 4 pictures can be directly calculated, so that the size of the Mini-batch does not need to be large. Post-mosaic affine transformsAs we noted earlier, the mosaics that we are creating are significantly bigger than the image sizes we will use to train our model, so we will need to do some sort of resizing here. Whilst this would work, this is likely to result in some very small objects, as we are essentially resizing four images to the size of one - which is likely to become a problem where the domain already contains very small bounding boxes. Additionally, each of our mosaics are structurally quite similar, with an image in each quadrant. Recalling that our aim was to make the model more robust to position changes, this may not actually help that much; as the model is likely just to start looking in the middle of each quadrant.To overcome this, one approach that we can take is to simply take a random crop from our mosaic. This will still provide the variability in positioning whilst preserving the size and aspect ratio of the target objects. At this point, it may also be a good opportunity to add in some other transforms such as scaling and rotation to add even more variability. DropBlock regularizationDropout, which randomly deletes the number of neurons, but the network can still learn the same information from adjacent activation units.DropBlock randomly deletes the entire local area, and the network will focus on learning certain features to achieve correct classification and get better generalization effects. Class label smoothingIn multi-classification tasks, the output is usually normalized with softmax, and then one-hot label is used to calculate the cross-entropy loss function to train the model. However, the use of one-hot vector representation can easily lead to the problem of network overfitting, so Label Smoothing is to make the one-hot label softer, so that the phenomenon of overfitting can be effectively suppressed when calculating the loss, and the generalization ability of the model can be improved. Mish activationMish is a continuously differentiable non-monotonic activation function. Compared with ReLU, Mish’s gradient is smoother, and it allows a smaller negative gradient when it is negative, which can stabilize the network gradient flow and has better generalization ability.$f(x) = xtanh(ln(1+e^x))$. Multiinput weighted residual connections (MiWRC)YOLOv4 refers to the architecture and method of EfficientDet , and uses the multi-input weighted residual connection (MiWRC).The backbone of EfficientDet uses EfficientNet, Neck is BiFPN.EfficientNet-B0 is constructed by multiple MBConv Blocks. MBConv Block refers to the Inverted Residual Block of MobileNet V2.The design of MBConv is to first increase the dimension and then reduce the dimension, which is different from the operation of the residual block to first reduce the dimension and then increase the dimension. This design allows MobileNetV2 to better use the residual connection to improve Accuracy.The idea of ​​MiWRC is derived from BiFPN. In FPN, the features obtained by each layer are regarded as equal, while MiWRC believes that the features of different layers should have different importance, and different weight ratios should be given to the features of different scales. loss2 problems with using IOU loss:When the predict box (predict bndBox) and the target box (ground truth) do not intersect, the IOU is 0, which cannot reflect the distance between the two boxes. At this time, the loss function is not derivable, that is to say, the gradient cannot be calculated, so it cannot Optimizing the case where two boxes do not intersect;The IOU cannot reflect the coincidence size of the prediction frame and the target frame.Subsequent GIoU, DIoU, CIoU are based on IOU loss to add a penalty item: GIOU loss (Generalized IOU loss):C is the minimum bounding box of the target box Ground Truth and the prediction box Predict. $L_{GIOU}=1-IOU+\\frac{|C-B\\cupB^{gt}|}{|C|}$. DIOU loss (Distance IOU loss) considers the overlapping area and the center point distance, and adds a penalty term to minimize the center point distance between the two boxes.CIOU loss (Complete IOU loss) adds a penalty item based on DIOU, taking into account the factor of aspect ratio. CmBN (Cross mini-Batch Normalization) BN is to normalize the current mini-batch, but often the batch size is very small, and uneven sampling may occur, which may cause problems in normalization. Therefore, there are many Batch Normalization methods for small batch sizes.The idea of ​​CBN is to calculate the previous mini-batch together, but not keep too many mini-batches. The method is to normalize the results of the current and the current 3 mini-batches.The CmBN newly created by YOLOv4 is based on CBN for modification, and does not update calculations between mini-batches, but updates network parameters after a batch is completed. Self-Adversarial Training (SAT) SAT is a data enhancement method innovated by the author, which is completed in two stages:First, forward-propagate the training samples, and then modify the image pixels (without modifying the network weights) during back-propagation to reduce the performance of model detection. In this way, the neural network can perform adversarial attacks on itself. Creates the illusion that there is no detected object in the picture. This first stage is actually increasing the difficulty of training samples.The second stage is to use the modified pictures to train the model. Eliminate grid sensitivity The author observed a video of object detection and found that because the center point of the detected object is mostly located close to the center of the Grid, it is difficult to detect when it is on the edge of the Grid. The author believes that the problem that the center point of the detected object is mostly located close to the center point of the Grid is because of the gradient of the Sigmoid function. Therefore, the author made some changes in the Sigmoid function, multiplying Sigmoid by a value greater than 1, and taking into account the sensitivity of different Grid sizes to boundary effects, using (1+x)Sigmoid — (0.5x), where When the Grid resolution is higher, the x will be higher. Cosine annealing scheduler Cosine annealing is to use the cosine function to adjust the learning rate. At the beginning, the learning rate will be slowly reduced, then accelerated halfway, and finally slowed down again. Optimal hyperparameters Use Genetic Algorithms (Evolutionary Algorithms) to select hyperparameters. The method is to randomly combine hyperparameters for training, then select the best 10% hyperparameters and then randomly combine and train them, and finally select the best model. SAM-block (Spatial Attention Module) SAM is derived from the CBAM (Convolutional Block Attention Module) paper, which provides two attention mechanism techniques. DIoU-NMS In the classic NMS, the detection frame with the highest confidence and other detection frames will calculate the corresponding IOU value one by one, and the detection frame whose value exceeds the threshold is filtered out. But in the actual situation, when two different objects are very close, due to the relatively large IOU value, after the NMS algorithm, there is often only one detection frame left, which may cause missed detection.DIoU-NMS considers not only the IOU value, but also the distance between the center points of two boxes. If the IOU between the two frames is relatively large, but the distance between them is relatively far, it will be considered as the detection frame of different objects and will not be filtered out. YOLOv7Anchor boxesYOLOv7 family is an anchor-based model.In these models, the general philosophy is to first create lots of potential bounding boxes, then select the most promising options to match to our target objects; slightly moving and resizing them as necessary to obtain the best possible fit.The basic idea is that we draw a grid on top of each image and, at each grid intersection (anchor point), generate candidate boxes (anchor boxes) based on a number of anchor sizes. That is, the same set of boxes is repeated at each anchor point. However, one issue with this approach is that our target, ground truth, boxes can range in size — from tiny to huge! Therefore, it is usually not possible to define a single set of anchor sizes that can be matched to all targets. For this reason, anchor-based model architectures usually employ a Feature-Pyramid-Network (FPN) to assist with this. Center PriorsIf we put 3 anchor boxes in each anchor point of each of the grids, we end up with a lot of boxes.The issue is that most of these predictions are not going to contain an object, which we classify as ‘background’.To make the problem cheaper computationally, the YOLOv7 loss finds first the anchor boxes that are likely to match each target box and treats them differently — these are known as the center prior anchor boxes. This process is applied at each FPN head, for each target box, across all images in batch at once. model reparameterizationModel re-parametrization techniques merge multiple computational modules into one at inference stage. The model re-parameterization technique can be regarded as an ensemble technique, and we can divide it into two categories, i.e., module-level ensemble and model-level ensemble. Model scalingModel scaling is a way to scale up or down an already designed model and make it fit in different computing devices.Network architecture search (NAS) is one of the commonly used model scaling methods. efficient layer aggregation networks(ELAN)VovNet/OSANetVovNet, short for “Variance-based Overparameterized Convolutional Networks,” is a convolutional neural network (CNN) architecture proposed by Lee et al. in their paper “Variance-based Overparameterization for Robustness” in 2019. VovNet is designed to improve the robustness of deep neural networks, particularly in the context of image classification tasks.The key idea behind VovNet is to introduce variance-based overparameterization to enhance the representation power of CNNs. Overparameterization involves increasing the number of parameters in a neural network, which can improve the model’s ability to learn complex patterns and features.VovNet achieves variance-based overparameterization by introducing multiple “VovNet blocks.” Each VovNet block is designed to capture different levels of granularity within the input data. Instead of using a single set of convolutional filters for all spatial dimensions, VovNet employs different filters for each spatial dimension. This allows the network to capture variations in features at different scales, leading to more robust representations. One-shot aggregation (OSA) module is designed which is more efficient than Dense Block in DenseNet.By cascading OSA module, an efficient object detection network VoVNet is formed.One-shot aggregation (OSA) module is designed to aggregate its feature in the last layer at once.It has much less Memory access cost (MAC) than that with dense block.Also, OSA improves GPU computation efficiency. The input sizes of intermediate layers of OSA module are constant. Hence, it is unnecessary to adopt additional 1×1 conv bottleneck to reduce dimension. The means it consists of fewer layers. CSPVOVNetIt combines CSPNet and VoVNet and considers the gradient path for improvement, so that the weights of different layers can learn more diverse features to improve accuracy. Deep supervisionWhen training deep networks, auxiliary head and auxiliary classifiers are often added to the middle layer of the neural network to improve stability, convergence speed, and avoid gradient disappearance problems, that is, to use auxiliary loss for shallow layers. Network weights for training, this technique is called Deep Supervision. dynamic label assignmentLabel Assigner is a mechanism that considers the network prediction results together with the ground truth and then assigns soft labels. In the past, the definition of the target label was usually to use a hard label that follows the ground truth. In recent years, it has also been used to perform some optimization operations on the prediction results of the model and the ground truth to obtain a soft label. This mechanism is called label assigner in this paper.The author discusses three methods of assigning soft labels on the auxiliary head and lead head: Independent, Lead head guided label assigner,Coarse-to-fine lead head guided label assigner.Independent:Auxiliary head and lead head perform label assignment with ground truth respectively, which is the most used method at present.Lead head guided label assigner:Since the lead head has a stronger learning ability than the auxiliary head, the soft label obtained by optimizing the prediction result of the lead head and the ground truth can better express the distribution and correlation between the data and the ground truth.Then use the soft label as the target of the auxiliary head and lead head for training, so that the shallower auxiliary head can directly learn the information that the lead head has learned, while the lead head pays more attention to the unlearned residual information. Coarse-to-fine lead head guided label assigner:This part is also the soft label obtained by optimizing the prediction result of the lead head and the ground truth. The difference is that two different soft labels will be generated: coarse label and fine label, where the fine label is the same as the soft label of the lead head , coarse label is used for auxiliary head. Optimal Transport AssignmentThe simplest approach is to define an Intersection over Union (IoU) threshold and decide based on that. While this generally works, it becomes problematic when there are occlusions, ambiguity or when multiple objects are very close together. Optimal Transport Assignment (OTA) aims to solve some of these problems by considering label assignment as a global optimization problem for each image.YOLOv7 implements simOTA (introduced in the YOLOX paper), a simplified version of the OTA problem. Model EMAWhen training a model, it can be beneficial to set the values for the model weights by taking a moving average of the parameters that were observed across the entire training run, as opposed to using the parameters obtained after the last incremental update. This is often done by maintaining an exponentially weighted average (EMA) of the model parameters, in practice, this usually means maintaining another copy of the model to store these averaged weights. This technique has been employed in several training schemes for popular models such as training MNASNet, MobileNet-V3 and EfficientNet. The approach to EMA taken by the YOLOv7 authors is slightly different to other implementations as, instead of using a fixed decay, the amount of decay changes based on the number of updates that have been made. Loss algorithmwe can break down the algorithm used in the YOLOv7 loss calculation into the following steps: For each FPN head (or each FPN head and Aux FPN head pair if Aux heads used):Find the Center Prior anchor boxes.Refine the candidate selection through the simOTA algorithm. Always use lead FPN heads for this.Obtain the objectness loss score using Binary Cross Entropy Loss between the predicted objectness probability and the Complete Intersection over Union (CIoU) with the matched target as ground truth. If there are no matches, this is 0.If there are any selected anchor box candidates, also calculate (otherwise they are just 0): The box (or regression) loss, defined as the mean(1 - CIoU) between all candidate anchor boxes and their matched target. The classification loss, using Binary Cross Entropy Loss between the predicted class probabilities for each anchor box and a one-hot encoded vector of the true class of the matched target.If model uses auxiliary heads, add each component obtained from the aux head to the corresponding main loss component (i.e., x = x + aux_wt*aux_x). The contribution weight (aux_wt) is defined by a predefined hyperparameter.Multiply the objectness loss by the corresponding FPN head weight (predefined hyperparameter). Multiply each loss component (objectness, classification, regression) by their contribution weight (predefined hyperparameter). Sum the already weighted loss components. Multiply the final loss value by the batch size. using yolov7github address: https://github.com/WongKinYiu/yolov7;Format converter:https://github.com/wy17646051/UA-DETRAC-Format-Converter potential ideasefficiencyIn order to enhance the real-time detection of the network, researchers generally analyze the number of parameters, calculation amount and calculation density from the aspects of model parameters, calculation amount, memory access times, input-output channel ratio, element-wise operation, etc. In fact, these research methods are similar to ShuffleNetV2 at that time. NAS(Neural Architecture Search)NAS was an inspiring work out of Google that lead to several follow up works such as ENAS, PNAS, and DARTS. It involves training a recurrent neural network (RNN) controller using reinforcement learning (RL) to automatically generate architectures. Vision TransformerThe core conclusion in the original ViT paper is that when there is enough data for pre-training, ViT’s performance will exceed CNN, breaking through the limitation of transformer lack of inductive bias, you can use it in Better transfer results in downstream tasks. However, when the training data set is not large enough, the performance of ViT is usually worse than that of ResNets of the same size, because Transformer lacks inductive bias compared with CNN, that is, a priori knowledge, a good assumption in advance. improve choosing anchor boxdatasetsPASCAL VOC 2007, VOC 2012, Microsoft COCO (Common Objects in Context).UA-DETRAC: https://detrac-db.rit.albany.edu/ https://www.kaggle.com/datasets/patrikskalos/ua-detrac-fix-masks-two-wheelers?resource=download https://colab.research.google.com/github/hardik0/Multi-Object-Tracking-Google-Colab/blob/main/Towards-Realtime-MOT-Vehicle-https://github.com/hardik0/Towards-Realtime-MOT/tree/masterTracking.ipynb#scrollTo=y6KZeLt9ViDehttps://github.com/wy17646051/UA-DETRAC-Format-Converter/tree/mainMIO-TCD:https://tcd.miovision.com/KITTI:https://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmarkTRANCOS: https://gram.web.uah.es/data/datasets/trancos/index.htmlSTREETS:https://www.kaggle.com/datasets/ryankraus/traffic-camera-object-detection: single classVERI-Wild: https://github.com/PKU-IMRE/VERI-Wild https://universe.roboflow.com/7-class/11-11-2021-09.41https://universe.roboflow.com/szabo/densitytrafficcontroller-1axlmhttps://universe.roboflow.com/future-institute-of-technology-1wuwl/indian-vehicle-set-1https://universe.roboflow.com/cv-2022-kyjj6/tesihttps://universe.roboflow.com/vehicleclassification-kxtkb/vehicle_classification-fvssnhttps://universe.roboflow.com/urban-data/urban-datahttps://www.kaggle.com/datasets/ashfakyeafi/road-vehicle-images-datasethttps://github.com/MaryamBoneh/Vehicle-Detection Referenceshttps://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173https://medium.com/ching-i/yolo%E6%BC%94%E9%80%B2-1-33220ebc1d09https://medium.com/ching-i/yolo%E6%BC%94%E9%80%B2-2-85ee99d114a1https://medium.com/@chingi071/yolo%E6%BC%94%E9%80%B2-3-yolov4%E8%A9%B3%E7%B4%B0%E4%BB%8B%E7%B4%B9-5ab2490754efhttps://zhuanlan.zhihu.com/p/183261974https://sh-tsang.medium.com/review-vovnet-osanet-an-energy-and-gpu-computation-efficient-backbone-network-for-real-time-3b26cd035887https://medium.com/ching-i/yolo%E6%BC%94%E9%80%B2-yolov7-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80-97b0e914bdbehttps://towardsdatascience.com/yolov7-a-deep-dive-into-the-current-state-of-the-art-for-object-detection-ce3ffedeeaebhttps://towardsdatascience.com/neural-architecture-search-limitations-and-extensions-8141bec7681fhttps://learnopencv.com/fine-tuning-yolov7-on-custom-dataset/#The-Training-Experiments-that-We-Will-Carry-Outhttps://learnopencv.com/yolov7-object-detection-paper-explanation-and-inference/","link":"/2023/05/05/object-detection/"},{"title":"bigdata - wide column stores","text":"wide column storesWide column stores were invented to provide more control over performance and in particular, in order to achieve high-throughput and low latency for objects ranging from a few bytes to about 10 MB, which are too big and numerous to be efficiently stored as so-called clobs (character large objects) or blobs (binary large objects) in a relational database system, but also too small and numerous to be efficiently accessed in a distributed file system. Why wide column storesA wide column store will be more tightly integrated with the parallel data processing systems.wide column stores have a richer logical model than the simple key-value model behind object storage. wide column stores also handle very small values (bytes and kBs) well thanks to batch processing. different from RDBMSIt does not have any data model for values, which are just arrays of bytes; since it efficiently handles values up to 10 MB, the values can be nested data in various formats, which breaks the first normal form; tables do not have a schema; there is no language like SQL, instead the API is on a lower level and more akin to that of a key-value store; tables can be very sparse, allowing for billions of rows and millions of columns at the same time; this is another reason why data stored in HBase is denormalized. BigTable and HBaseRationaleHBase is an open-source equivalent to the BigTable as part of the Hadoop ecosystem.The data model of HBase is based on the realization that joins are expensive, and that they should be avoided or minimized on a cluster architecture. The second design principle underlying HBase is that it is efficient to store together what is accessed together. In the big picture, this is a flavor of batch processing, one of the overarching principles in Big Data. Batch processing reduces the impact of latency. Tables and row IDsFrom an abstract perspective, HBase can be seen as an enhanced keyvalue store, in the sense that:a key is compound and involves a row, a column and a version;values can be larger (clobs, blobs), up to around 10 MB; keys are sortable. A row ID is logically an array of bytes, although there is a library to easily create row ID bytes from specific primitive values. In HBase, the key identifying every cell consists of: the row ID, the column family, the column qualifier, the version. Column familiesThe other attributes, called columns, are split into so-called column families. This is a concept that does not exist in relational databases and that allows scaling the number of columns. Column qualifiersColumns in HBase have a name (in addition to the column family) called column qualifier, however unlike traditional RDBMS, they do not have a particular type. Column qualifiers are arrays of bytes (rather than strings), and as for row IDs, there is a library to easily create column qualifiers from primitive values. Unlike the values which can be large arrays of bytes (blobs), it is important to keep column families and column qualifiers short, because as we will see, they are repeated a gigantic number of times on the physical layer. VersioningHBase generally supports versioning, in the sense that it keeps track of the past versions of the data. As we will see, this is implemented by associating any value with a timestamp, also called version, at which it was created (or deleted).Users can also override timestamps with a value of their choice to have more control about versions. Logical queriesHBase supports four kinds of low-level queries: get, put, scan and delete. Unlike a traditional key-value store, HBase also supports querying ranges of row IDs and ranges of timestamps. HBase offers a locking mechanism at the row level, meaning that different rows can be modified concurrently, but the cells in the same row cannot: only one user at a time can modify any given row. Physical architecturePartitioningA table in HBase is physically partitioned in two ways: on the rows and on the columns. The rows are split in consecutive regions. Each region is identified by a lower and an upper row key, the lower row key being included and the upper row key excluded. A partition is called a store and corresponds to the intersection of a region and of a column family. Network topologyHBase has exactly the same centralized architecture as HDFS. The HMaster and the RegionServers should be understood as processes running on the nodes, rather than the nodes themselves. The HMaster assigns responsibility of each region to one of the RegionServers. There is no need to attribute the responsibility of a region to more than one RegionServer at a time because, as we will see soon, fault tolerance is already handled on the storage level by HDFS. If a region grows too big, for example because of many writes in the same row ID interval, then the region will be automatically split by the responsible RegionServer. If a RegionServer has too many regions compared to other RegionServers, then the HMaster can reassign regions to other RegionServers. Physical storageThe store is, physically, nothing less than a set of cells. Each cell is thus handled physically as a key-value pair where the key is a (row ID, column family, column qualifier, version) tuple. All the cells within a store are eventually persisted on HDFS, in files that we will call HFiles. An HFile is, in fact, nothing else than a (boring) flat list of KeyValues, one per cell. What is important is that, in an HFile, all these KeyValues are sorted by key in increasing order, meaning, first sorted by row ID, then by column family (trivially unique for a given store), then by column qualifier, then by version (in decreasing order, recent to old). This means that all versions of a given cell that are in the same HFile are located together, and one of the cells (within this HFile) is the latest. If we zoom in at the bit level, a KeyValue consists in four parts: The length of the keys in bits (this length is encoded on a constant, known number of bits) • The length of the value in bits (this length is encoded on a constant, known number of bits) • The actual key (of variable length) • The actual value (of variable length). Why do we not just store the key and the value? This is because their length can vary. If we do not know their length, then it is impossible to know when they stop just looking at the bits. KeyValues, within an HFile, are organized in blocks. But to not confuse them with HDFS blocks, we will call them HBlocks. HBlocks have a size of 64 kB, but this size is variable: if the last KeyValue goes beyond this boundary, then the HBlock is simply longer and stops whenever the last KeyValue stops. The HFile then additionally contains an index of all blocks with their key boundaries. This separate index is loaded in memory prior to reading anything from the HFile. Log-structured merge treesWhen accessing data, HBase needs to generally look everywhere for cells to potentially return: in every HFile, and in memory. As long as there is room in memory, freshly created cells are added in memory. At some point, the memory becomes full (or some other limits are reached). When this happens, all the cells need to be flushed to a brand new HFile. Upon flushing, all cells are written sequentially to a new HFile in ascending key order, HBlock by HBlock, concurrently building the index structure. When cells are added to memory, they are added inside a data structure that maintains them in sorted order (such as tree maps) and then flushing is a linear traversal of the tree. What happens if the machine crashes and we lose everything in memory? We have a so-called write-ahead-log for this. Before any fresh cells are written to memory, they are written in sequential order (append) to an HDFS file called the HLog. There is one HLog per RegionServer. A full write-ahead-log also triggers a flush of all cells in memory to a new HFile. If there is a problem and the memory is lost, the HLog can be retrieved from HDFS and “played back” in order to repopulate the memory and recreate the sorting tree structure. After many flushes, the number of HFiles to read from grows and becomes impracticable. For this reason, there is an additional process called compaction that takes several HFiles and outputs a single,merged HFile. Since the cells within each HFile are already sorted, this can be done in linear time, as this is essentially the merge part of the merge-sort algorithm. Compaction is not done arbitrarily but follows a regular, logarithmic pattern. When the memory is flushed again, an standard-size HFile is written and the two standard-size HFiles are immediately compacted to a double-size HFile. Additional design aspectsBootstrapping lookupsIn order to know which RegionServer a client should communicate with to receive cells corresponding to a specific region, there is a main, big lookup table that lists all regions of all tables together with the coordinates of the RegionServer in charge of this region as well as additional metadata. CachingIn order to improve latency, cells that are normally persisted to HFiles (and thus no longer in memory) can be cached in a separate memory region, with the idea of keeping in the cache those cells that are frequently accessed. Bloom filtersHBase has a mechanism to avoid looking for cells in every HFile. This mechanism is called a Bloom filter. It is basically a black box that can tell with absolute certainty that a certain key does not belong to an HFile, while it only predicts with good probability (albeit not certain) that it does belong to it. Data locality and short-circuitingWhen a RegionServer flushes cells to a new HFile, a replica of each (HDFS) block of the HFile is written, by the DataNode process living on the same machine as the RegionServer process, to the local disk. This makes accessing the cells in future reads by the RegionServer extremely efficient, because the RegionServer can read the data locally without communicating with the NameNode: this is known as short-circuiting in HDFS. using HabseAfter installing Hbase, we can use Hbase shell. There are some commands in hbase shell: get, scan.We can use filters with scan to query and filter data. exercisesbloom filterperfect hash function should have uniform probability.all hash functions set a bit to 1 = collide at the same place: probability of a FP casedeleting only happens when compacting. referenceshttps://ghislainfourny.github.io/big-data-textbook/https://datakaresolutions.com/hbase-quick-guide-to-key-commands/https://www.datapotion.io/blog/hbase-shell-column-filters/","link":"/2023/10/17/bigdata4/"},{"title":"Probabilistic Artificial Intelligence - Variational Inference","text":"Variational InferenceThe fundamental idea behind variational inference is to approximate the true posterior distribution using a “simpler” posterior that is as close as possible to the true posterior. p(\\theta\\mid x_{1:n},y_{1:n})=\\frac1Zp(\\theta,y_{1:n}\\mid x_{1:n})\\approx q(\\theta\\mid\\lambda)\\doteq q_{\\lambda}(\\theta)where λ represents the parameters of the variational posterior q, also called variational parameters. Laplace ApproximationA natural idea for approximating the posterior distribution is to use a Gaussian approximation (that is, a second-order Taylor approximation) around the mode of the posterior. Such an approximation is known as a Laplace approximation. As an example, we will look at Laplace approximation in the context of Bayesian logistic regression. Bayesian logistic regression corresponds to Bayesian linear regression with a Bernoulli likelihood. Intuitively, the Laplace approximation matches the shape of the true posterior around its mode but may not represent it accurately elsewhere — often leading to extremely overconfident predictions. Inference with a Variational \\begin{aligned} p(y^{\\star}\\mid x^{\\star},x_{1:n},y_{1:n})& =\\int p(y^\\star\\mid x^\\star,\\boldsymbol{\\theta})p(\\boldsymbol{\\theta}\\mid x_{1:n},y_{1:n})d\\boldsymbol{\\theta} \\\\ &\\approx\\int p(y^\\star\\mid x^\\star,\\boldsymbol{\\theta})q_\\lambda(\\boldsymbol{\\theta})d\\boldsymbol{\\theta} &=\\int\\int p(y^\\star\\mid f^\\star)p(f^\\star\\mid x^\\star,\\theta)q_\\lambda(\\theta)d\\theta df^\\star\\\\&=\\int p(y^\\star\\mid f^\\star)\\int p(f^\\star\\mid x^\\star,\\theta)q_\\lambda(\\theta)d\\theta df^\\star\\\\&=\\int p(y^\\star\\mid f^\\star)q_\\lambda(f^\\star\\mid x^\\star)df^\\star.\\quad\\quad \\end{aligned}Information TheorySurpriseThe surprise about an event with probability u is defined as $S[u] = -log u$. EntropyThe entropy of a distribution p is the average surprise about samples from p.if the entropy of p is large, we are more uncertain about x ∼ p than if the entropy of p were low. \\mathbb{H}[p]\\doteq\\mathbb{E}_{x\\sim p}[\\mathbb{S}[p(x)]]=\\mathbb{E}_{x\\sim p}[-\\log p(x)].Cross-EntropyCross-entropy can also be expressed in terms of the KL-divergence. Quite intuitively, the average surprise in samples from p with respect to the distribution q is given by the inherent uncertainty in p and the additional surprise that is due to us assuming the wrong data distribution q. The “closer” q is to the true data distribution p, the smaller is the additional average surprise. \\mathrm{H}[p\\|q]=\\mathrm{H}[p]+\\mathrm{KL}(p\\|q)\\geq\\mathrm{H}[p].Variational FamiliesWe can view variational inference more generally as a family of approaches aiming to approximate the true posterior distribution by one that is closest (according to some criterion) among a “simpler” class of distributions. Kullback-Leibler Divergence In words, KL(p∥q) measures the additional expected surprise when observing samples from p that is due to assuming the (wrong) distribution q and which not inherent in the distribution p already. Intuitively, the KL-divergence measures the information loss when approximating p with q. Forward and reverse KL-divergenceKL(p∥q) is also called the forward (or inclusive) KL-divergence. In contrast, KL(q∥p) is called the reverse (or exclusive) KL-divergence. It can be seen that the reverse KL-divergence tends to greedily select the mode and underestimating the variance which, in this case, leads to an overconfident prediction. The forward KL-divergence, in contrast, is more conservative and yields what one could consider the “desired” approximation. The reverse KL-divergence is typically used in practice. This is for computational reasons. In principle, if the variational family Q is “rich enough”, minimizing the forward and reverse KL-divergences will yield the same result. There is a nice interpretation of minimizing the forward KullbackLeibler divergence of the approximation $q_\\lambda$ with respect to the true distribution p as being equivalent to maximizing the marginal model likelihood on a sample of infinite size. This interpretation is not useful in the setting where p is a posterior distribution over model parameters θ as a maximum likelihood estimate requires samples from p which we cannot obtain in this case. KL-divergence of Gaussians Evidence Lower BoundThe Evidence Lower Bound is a quantity maximization of which is equivalent to minimizing the KL-divergence. As its name suggests, the evidence lower bound is a (uniform) lower bound to the log-evidence $log p(y{1:n}|x{1:n})$. Note that this inequality lower bounds the logarithm of an integral by an expectation of a logarithm over some variational distribution q. Hence, the ELBO is a family of lower bounds — one for each variational distribution. Such inequalities are called variational inequalities. Gradient of Evidence Lower BoundObtaining the gradient of the evidence lower bound is more difficult. This is because the expectation integrates over the measure $q_\\lambda$, which depends on the variational parameters $\\lambda$. Thus, we cannot move the gradient operator inside the expectation. There are two main techniques which are used to rewrite the gradient in such a way that Monte Carlo sampling becomes possible. The first is to use score gradients. The second is the so-called reparameterization trick. reparameterization trick The procedure of approximating the true posterior using a variational posterior by maximizing the evidence lower bound using stochastic optimization is also called black box stochastic variational inference. The only requirement is that we can obtain unbiased gradient estimates from the evidence lower bound (and the likelihood).","link":"/2023/10/20/pai2/"},{"title":"bigdata - Map Reduce","text":"Patterns of large-scale query processingTextual inputWe saw that the data we want to query can take many forms. First, it can be billions of lines of text. It can also be plenty of CSV lines. Some other formats (e.g., Parquet, …) can be binary. We also encountered HFiles in Chapter 6, which are lists of keyvalue pairs. In fact, Hadoop has another such kind of key-value-based format called Sequence File, which is simply a list of key-values, but not necessarily sorted by key (although ordered) and with keys not necessarily unique. ShardsThere are several practical motivations for the many-files pattern even in HDFS. Querying patternThis is the motivation behind the standard MapReduce pattern: a map-like phase on the entire input, then a shuffle phase on the intermediate data, then another map-like phase (called reduce) producing the entire output MapReduce ModelIn MapReduce, the input data, intermediate data, and output data are all made of a large collection of key-value pairs (with the keys not necessarily unique, and not necessarily sorted by key). The types of the keys and values are known at compile-time (statically), and they do not need to be the same across all three collections. MapReduce architectureOn a cluster, the architecture is centralized, just like for HDFS and HBase. In the original version of MapReduce, the main node is called JobTracker, and the worker nodes are called TaskTrackers. In fact, the JobTracker typically runs on the same machine as the NameNode (and HMaster) and the TaskTrackers on the same machines as the DataNodes (and RegionServers). This is called “bring the query to the data.” As the map phase progresses, there is a risk that the memory becomes full. But we have seen this before with HBase: the intermediate pairs on that machine are then sorted by key and flushed to the disk to a Sequence File. And as more flushes happen, these Sequence Files can be compacted to less of them, very similarly to HBase’s Log-Structured Merge Trees. When the map phase is over, each TaskTracker runs an HTTP server listening for connections, so that they can connect to each other and ship the intermediate data over to create the intermediate partitions ensuring that the same keys are on the same machines.This is the phase called shuffling. Then, the reduce phase can start. When the reduce phase is completed, each output partition will be output to a shard (as we saw, a file named incrementally) in the output destination (HDFS, S3, etc) and in the desired format. Note that shuffling can start before the map phase is over, but the reduce phase can only start after the map phase is over. MapReduce input and output formatsImpedance mismatchMapReduce can read its input from files lying in a data lake as well as directly from a database system such as HBase or a relational database management system. MapReduce only reads and writes lists of key-value pairs, where keys may be duplicates and need not appear in order. However, the inputs we considered are not key-value pairs. So we need an additional mechanism that allows MapReduce to interpret this input as key-value pairs. For tables, whereas relational or in a wide column stores, this is relatively easy: indeed, tables have primary keys, consisting of either a single column or multiple columns. Thus, each tuple can be interpreted as a key-value pair. Mapping files to pairsHow do we read a (possibly huge) text file as a list of key-value pairs? The most natural way to do so is to turn each line of text in a key value pair1: the value is the string corresponding to the entire line, while the key is an integer that expresses the position (as a number of characters), or offset, at which the line starts in the current file being read. A few examplesCounting words SelectingThis is something easily done by having a map function that outputs a subset of its input, based on some predicate provided by the user.Here we notice that the output of the map phase already gives us the desired result; we still need to provide a reduce function, which is taken trivially as the identity function. This is not unusual (and there are also examples where the map function is trivial, and the reduce function is doing the actual processing). ProjectingThe map function can project this object to an object with less attributes: MapReduce and the relational algebraAs an exercise, try to figure out how to implement a GROUP BY clause and an ORDER BY clause. What about the HAVING clause? Naturally, executing these queries directly in MapReduce is very cumbersome because of the low-level code we need to write. Combine functions and optimizationIn addition to the map function and the reduce function, the user can supply a combine function. This combine function can then be called by the system during the map phase as many times as it sees fit to “compress” the intermediate key-value pairs. Strategically, the combine function is likely to be called at every flush of key-value pairs to a Sequence File on disk, and at every compaction of several Sequence Files into one. However, there is no guarantee that the combine function will be called at all, and there is also no guarantee on how many times it will be called. Thus, if the user provides a combine function, it is important that they think carefully about a combine function that does not affect the correctness of the output data. In fact, in most of the cases, the combine function will be identical to the reduce function, which is generally possible if the intermediate key-value pairs have the same type as the output key-value pairs, and the reduce function is both associative and commutative. MapReduce programming APIMapper classesIn Java, the user needs to define a so-called Mapper class that contains the map function, and a Reducer class that contains the reduce function. A map function takes in particular a key and a value. Note that it outputs key-value pairs via the call of the write method on the context, rather than with a return statement. Reducer classesA reduce function takes in particular a key and a list of values. Note that it outputs key-value pairs via the call of the write method on the context, rather than with a return statement. Running the jobFinally, a MapReduce job can be created and invoked by supplying a Mapper and Reducer instance to the job. A combine function can also be supplied with the setCombinerClass method. It is also possible to use Python rather than Java, via the socalled Streaming API. The Streaming API is the general way to invoke MapReduce jobs in other languages than Java. Using correct terminologyFunctionsA map function is a mathematical, or programmed, function that takes one input key-value pair and returns zero, one or more intermediate key-value pairs. A reduce function is a mathematical, or programmed, function that takes one or more intermediate key-value pairs and returns zero, one or more output key-value pairs. A combine function is a mathematical, or programmed, function that takes one or more intermediate key-value pairs and returns zero, one or more intermediate key-value pairs. Note that the combine function is an optional local aggregation step that occurs before shuffling and sorting, and its purpose is to reduce the amount of data that needs to be transferred to the reducers. The reduce function, on the other hand, performs the final aggregation and processing based on keys. TasksA map task is an assignment (or “homework”, or “TODO”) that consists in a (sequential) series of calls of the map function on a subset of the input. A reduce task is an assignment that consists in a (sequential) series of calls of the reduce function on a subset of the intermediate input. We insist that the calls within a task are sequential, meaning that there is no parallelism at all within a task. There is no such thing as a combine task. Calls of the combine function are not planned as a task, but is called ad-hoc during flushing and compaction. SlotsThe map tasks are processed thanks to compute and memory resources (CPU and RAM). These resources are called map slots. One map slot corresponds to one CPU core and some allocated memory. The number of map slots is limited by the number of available cores. Each map slot then processes one map task at a time, sequentially. The resources used to process reduce tasks are called reduce slots. So, there is no parallelism either within one map slot, or one reduce slot. In fact, parallelism happens across several slots. PhasesThe map phase thus consists of several map slots processing map tasks in parallel. blocks vs. splitsHDFS blocks have a size of (at most) 128 MB. In every file, all blocks but the last one have a size of exactly 128 MB. Splits, however, only contain full records: a key-value pair will only belong to one split (and thus be processed by one map task).","link":"/2023/10/31/bigdata6/"},{"title":"bigdata - data model","text":"data modelA data model is an abstract view over the data that hides the way it is stored physically. The JSON Information SetThe appropriate abstraction for any JSON document is a tree. The nodes of that tree, which are JSON logical values, are naturally of six possible kinds: the six syntactic building blocks of JSON. These are the four leaves corresponding to atomic values:Strings, Numbers,Booleans, Nulls. As well as two intermediate nodes: Objects, Arrays. These nodes are generally called information items and form the logical building blocks of the model, called information set. The XML Information SetA fundamental difference between JSON trees and XML trees is that for JSON, the labels (object keys) are on the edges connecting an object information item to each one of its children information items. In XML, the labels (these would be element and attribute names) are on the nodes (information items) directly. In XML, there are many more information items: Document information items, Element, Attribute, Character, Comment, Processing instruction, Name space, Unexpanded entity reference, DTD, Unparsed entity, Notation. Here we only focus on documents, elements, attributes, and characters. ValidationOnce documents, JSON or XML, have been parsed and logically abstracted as a tree in memory, the natural next step is to check for further structural constraints. In a relational database, the schema of a table is defined before any data is populated into the table. Thus, the data in the table is guaranteed, at all times, to fulfil all the constraints of the schema. The schema was enforced at write time (schema on write). A collection of JSON and XML documents out there can exist without any schema and contain arbitrary structures. Validation happens “ex post,” that is, only after reading the data (schema on read). JSON and XML documents undergo two steps: a well-formedness check: attempt to parse the document and construct a tree representation in memory;(if the first step succeeded) a validation check given a specific schema. Note that, unlike well-formedness, validation is schema dependent: a given well-formed document can be valid against schema A and invalid against schema B. Item typesA fundamental aspect of validation is the type system. A well-designed type system, in turn, allows for storing the data in much more efficient, binary formats tailored to the model. Atomic typesAtomic types correspond to the leaf of a tree data model: these are types that do not contain any further nestedness. The kinds of atomic types available are also relatively standard and common to most technologies. Also, all atomic types have in common that they have a logical value space and a lexical value space. An atomic type also has a (not necessarily injective) mapping from its lexical value space to its logical value space (e.g., mapping the hexadecimal literal x10 to the mathematical integer sixteen).Atomic types can be in a subtype relationship: a type is a subtype of another type if its logical value space is a subset of the latter. StringsIn “pure computer science” textbooks, strings are often presented as structured values rather than as atomic values because of their complexity on the physical layer. However, for us data scientists, strings are atomic values. Numbers: integersIntegers correspond to finite cardinalities (counting) as well as their negative counterparts. In older programming languages, support for integers used to be bounded. However, in modern databases, it is customary to support unbounded integers. Engines can optimize computations for small integers, but might become less efficient with very large integers. Numbers: decimalsDecimals correspond to real numbers that can be written as a finite sequence of digits in base 10, with an optional decimal period. Numbers: floating-pointSupport for the entire decimal value space can be costly in performance. In order to address this issue, a floating-point standard (IEEE 754) was invented and is still very popular today. Floating-point numbers are limited both in precision and magnitude (both upper and lower) in order to fit on 32 bits (float) or 64 bits (double). Floats have about 7 digits of precision and their absolute value can be between roughly 10^−37 and 10^37, while doubles have 15 digits of precision and their absolute value can be between roughly 10^−307 and 10^308. BooleansThe logical value space for the Boolean type is made of two values: true and false as in NoSQL queries, two-valued logic is typically assumed. Dates and timesDates are commonly using the Gregorian calendar (with some technologies possibly supporting more) with a year (BC or AD), a month and a day of the month. Times are expressed in the hexagesimal (60) basis with hours, minutes, seconds, where the seconds commonly go all the way to microseconds (six digits after the decimal period). Datetimes are expressed with a year, a month, a day of the month, hours, minutes and (decimal) seconds. Timestamp values are typically stored as longs (64-bit integers) expressing the number of milliseconds elapsed since January 1, 1970 by convention. XML Schema, JSound and JSONiq follow the ISO 8601 standard, where lexical values look like so (with many parts optional): 2022-08-07T14:18:00.123456+02:00. DurationsThe lexical representation can vary, but there is a standard defined by ISO 8601 as well, starting with a P and prefixing sub-day parts with a T.4 days, 3 hours, 2 minutes and 1.123456 seconds: P4DT3H2M1.123456S. Binary dataBinary data is, logically, simply a sequence of bytes. There are two main lexical representations used in data: hexadecimal and base64. Hexadecimal expresses the data with two hexadecimal digits per byte. Base 64, formally, does the same but in the base 64, which “wastes” less lexical space in the text. It does so by encoding the bits six by six, encoding each sequence of six bits with one base-64 digit. NullA schema can either allow, or disallow the null value.XML also supports null values, but calls them “nil” and does so with a special attribute and no content rather than with a lexical representation Structured typesListsLists correspond to JSON arrays and are ordered sequences of (atomic or structured) values. RecordsRecords, or structs, correspond to JSON objects and are maps from strings to values. MapsMaps (not be confused with records, which are similar) are maps from any atomic value to any value, i.e., generalize objects to keys that are not necessarily strings (e.g., numbers, dates, etc).With a schema, it is possible to restrict the type of the keys, as well as the type of the values. However, unlike records, the type of the values must be the same for all keys. SetsSets are like lists, but without any specific ordering, and without duplicate values. XML elements and attributesXML Schema stands apart from most other technologies and formats, in that it does not offer specific support for records and maps; it offers some limited support for lists, but considers them to be simple types, which are “inbetween” atomic types and structured types. n XML Schema, structure is obtained, instead, with elements and attributes, and the machinery for elements and attributes is highly specific to XML. Type names Sequence typesCardinalityMany type system give options regarding the number of occurrences of items in a sequence. Collections vs. nested listsA collection of items is on the outer level, and can be massively large (billions, trillions of items). A list (or array) of items, however, usually refers to a nested structure, for example an array nested inside a document or object. Such lists of items are usually restricted in size for reasons of performance and scalability. It is thus important to keep this subtle difference in mind, in particular, do not confuse a collection of integers with a collection that contains a single array of integers. JSON validationValidating flat objectsJSound is a schema language that was designed to be simple for 80% of the cases, making it particularly suitable in a teaching environment.It is independent of any programming language.JSON Schema is another technology for validating JSON documents. The available JSON Schema types are string, number, integer, boolean, null, array and object. An example for a json document is like:{ “name” : “Einstein”, “first” : “Albert”, “age” : 142 }The JSound and the JSON Schema are as follows:{ “name” : “string”, “first” : “string”, “age” : “integer” } { “type” : “object”, “properties” : { “name” : “string”, “first” : “string”, “age” : “number” } }. The type system of JSON Schema is thus less rich than that of JSound, but extra checks can be done with so-called formats, which include date, time, duration, email, and so on including generic regular expressions. Requiring the presence of a keyIt is possible to require the presence of a key by adding an exclamation mark in JSound. The equivalent JSON Schema uses a “required” property associated with the list of required keys to express the same. Open and closed object typesIn the JSound compact syntax, extra keys are forbidden. The schema is said to be closed. There are ways to define JSound schemas to allow arbitrary additional keys (open schemas), with a more verbose syntax. Unlike JSound, in JSON Schema, extra properties are allowed by default. JSON Schema then allows to forbid extra properties with the “additionalProperties” property. Nested structures{ “numbers” : [ “integer” ] }Every schema can be given a name, turning into a type.JSound allows for the definition not only of arbitrary array and object types, but also user-defined types. Primary key constraints, allowing for null, default valuesThere are a few more features available in the compact JSound syntax (not in JSON Schema) with the special characters @, ? and =. The question mark (?) allows for null values. The arobase (@) indicates that one or more fields are primary keys for a list of objects that are members of the same array. The equal sign (=) is used to indicate a default value that is automatically populated if the value is absent. Note that validation only checks whether lexical values are part of the type’s lexical space. Accepting any valuesAccepting any values in JSound can be done with the type “item”, which contains all possible values. In JSON Schema, in order to declare a field to accept any values, you can use either true or an empty object in lieu of the type. JSON Schema additionally allows to use false to forbid a field. Type unionsIn JSON Schema, it is also possible to combine validation checks with Boolean combinations. JSound schema allows defining unions of types with the vertical bar inside type strings, like so: “string|array”. Type conjunction, exclusive or, negationIn JSON Schema only (not in JSound), it is also possible to do a conjunction (logical and), as well as exclusive or (xor), as well as negation. XML validationSimple typesAll elements in an XML Schema are in a namespace, the XML Schema namespace. It is recommended to stick to the prefix xs, or xsd, which is also quite popular. We do not recommend declaring the XML Schema namespace as a default namespace, because it can create confusion in several respects. The list of predefined atomic types is the same as in JSound, except that in XML Schema, all these predefined types live in the XML Schema namespace and thus bear the prefix xs as well. Builtin typesXML Schema allows you to define user-defined atomic types, for example restricting the length of a string to 3 for airport codes, and then use it with an element. Complex typesIt is also possible to constrain structures and the element/attribute/text hierarchy with complex types applying to element nodes.There are four main kinds of complex types:• complex content: there can be nested elements, but there can be no text nodes as direct children. • simple content: there are no nested elements: just text, but attributes are also possible. • empty content: there are neither nested elements nor text, but attributes are also possible. • mixed content: there can be nested elements and it can be intermixed with text as well. Attribute declarationsFinally, all types of content can additionally contain attributes. Attributes always have a simple type. Anonymous typesFinally, it is not mandatory to give a name to all types. Be careful: if there is neither a type attribute nor a nested type declaration, then anything is allowed! MiscellaneousFinally, XML Schema documents are themselves XML documents, and can thus be validated against a “schema or schemas”, itself written as an XML Schema.This schema has the wonderful property of being valid against itself. Data framesHeterogeneous, nested datasetsThe beauty of the JSON data model is that, unlike the relational model and the CSV syntax, it supports nested, heterogeneous datasets, while also supporting as a particular case flat, homogeneous datasets. Dataframe visualsThere is a particular subclass of semi-structured datasets that are very interesting: valid datasets, which are collections of JSON objects valid against a common schema, with some requirements on the considered schemas. The datasets belonging to this particular subclass are called data frames, or dataframes. Specifically, for the dataset to qualify as a data frame, firstly, we forbid schemas that allow for open object types. secondly, we forbid schemas that allow for object or array values to be too permissive and allow any values. We, however, include schemas that allow for null values and/or absent values. Relational tables are data frames, while data frames are not necessarily relational tables. Data frames are a generalization of (normalized) relational tables allowing for (organized and structured) nestedness. exerciescomplextType cannot contain character by default but with mixed=”true” it can. protobufconvert json-like data to columnar representation(why we want this: make it more efficient to get relevant data rather than get the whole table). convert the columnar representation back to the original format. Replace the missing field with NULL. It’s a “lossless” conversion. Dremeloptional: 0 or 1. repeated: 1 or more.","link":"/2023/10/25/bigdata5/"},{"title":"Probabilistic Artificial Intelligence - Markov Chain Monte Carlo Methods","text":"Markov Chain Monte Carlo MethodsThe key idea of Markov chain Monte Carlo methods is to construct a Markov chain, which is efficient to simulate and has the stationary distribution p. Markov Chains Intuitively, the Markov property states that future behavior is independent of past states given the present state. Markov chains can be classified into different types based on their behavior. These classifications include time-homogeneous and time-inhomogeneous Markov chains, irreducible Markov chains, and periodic and aperiodic Markov chains. We restrict our attention to time-homogeneous Markov chains. That is, the transition probabilities do not change over time, which can be characterized by a transition function. Irreducible Markov chains are those in which every state can be reached from any other state, while periodic Markov chains exhibit a repeating pattern in their states. On the other hand, aperiodic Markov chains do not exhibit any regular pattern in their states. If there is no fixed period at which the probabilities return to the starting values, the chain is classified as aperiodic. Aperiodic Markov chains often display a more complex and unpredictable behavior compared to periodic ones. Stationarity Convergence A Markov Chain is called ergodic, if there exists a finite t such that every state can be reached from every state in exactly t steps. Fundamental theorem of ergodic Markov chains Detailed Balance Equation Ergodic Theorem This result is the fundamental reason for why Markov chain Monte Carlo methods are possible. In practice, one observes that Markov chain Monte Carlo methods have a so-called “burn-in” time during which the distribution of the Markov chain does not yet approximate the posterior distribution well. Typically, the first t0 samples are therefore discarded. It is not clear in general how T and t0 should be chosen such that the estimator is unbiased, rather they have to be tuned. Elementary Sampling MethodsMetropolis-Hastings Algorithm Gibbs SamplingA popular example of a Metropolis-Hastings algorithm is Gibbs sampling. Intuitively, by re-sampling single coordinates according to the posterior distribution given the other coordinates, Gibbs sampling finds states that are successively “more” likely. Langevin DynamicsGibbs Distributionsreferenceshttps://www.collimator.ai/reference-guides/what-is-a-aperiodic-markov-chain","link":"/2023/10/27/pai3/"},{"title":"bigdata - YARN","text":"Resource managementThe first version of MapReduce is inefficient in several respects. For this reason, the architecture was fundamentally changed by adding a resource management layer to the stack, adding one more level of decoupling between scheduling and monitoring. A resource management system, here YARN, is a very important building block not only for a better MapReduce, but also for many other technologies running on a cluster. Limitations of MapReduce in its first versionThe JobTracker has a lot on its shoulders! It has to deal with resource management, scheduling, monitoring, the job lifecycle, and fault tolerance.The first consequence of this is scalability. The second consequence is the bottleneck that this introduces at the JobTracker level, which slows down the entire system. The third issue is that it is difficult to design a system that do many things well. The fourth issue is that resources are statically allocated to the Map or the Reduce phase, meaning that parts of the cluster remain idle during both phases. The fifth issue is the lack of fungibility between the Map phase and the Reduce phase. YARNGeneral architectureYARN means Yet Another Resource manager. It was introduced as an additional layer that specifically handles the management of CPU and memory resources in the cluster. YARN, unsurprisingly, is based on a centralized architecture in which the coordinator node is called the ResourceManager, and the worker nodes are called NodeManagers. NodeManagers furthermore provide slots (equipped with exclusively allocated CPU and memory) known as containers. When a new application is launched, the ResourceManager assigns one of the container to act as the ApplicationMaster which will take care of running the application. The ApplicationMaster can then communicate with the ResourceManager in order to book and use more containers in order to run jobs. This is a fundamental change from the initial MapReduce architecture, in which the JobTracker was also taking care of running the MapReduce job. Thus, YARN cleanly separates between the general management of resources and bootstrapping new applications, which remains centralized on the coordinator node, and monitoring the job lifecycle, which is now delegated to one or more ApplicationMasters running concurrently. This means, in particular, that several applications can run concurrently in the same cluster. This ability, known as multi-tenancy, is very important for large companies or universities in order to optimize the use of their resources. Resource managementIn resource management, one abstracts away from hardware by distinguish between four specific resources used in a distributed database system. These four resources are: • Memory • CPU • Disk I/O • Network I/O.ApplicationMasters can request and release containers at any time, dynamically. A container request is typically made by the ApplicationMasters with a specific demand. If the request is granted by the ResourceManager fully or partially, this is done indirectly by signing and issuing a container token to the ApplicationMaster that acts as proof that the resource was granted. The ApplicationMaster can then connect to the allocated NodeManager and send the token. The NodeManager will then check the validity of the token and provide the memory and CPU granted by the ResourceManager. The ApplicationMaster ships the code (e.g., as a jar file) as well as parameters, which then runs as a process with exclusive use of this memory and CPU. Job lifecycle management and fault toleranceVersion 2 of MapReduce works on top of YARN by leaving the job lifecycle management to an ApplicationMaster. The ApplicationMaster requests containers for the Map phase, and sets these containers up to execute Map tasks. As soon as a container is done executing a Map task, the ApplicationMaster will assign a new Map task to this container from the remaining queue, until no Map tasks are left. a container in the Map phase can contain several Map slots. Sharing memory and containers across slots in this way improves the overall efficiency, because setting up a container adds latency. It is thus more efficient to allocate 10 containers of each 4 cores, compared to 40 containers of each 1 core. In the event of a container crashing during the Map phase, the ApplicationMaster will handle this by re-requesting containers and restarting the failed tasks. In the case that some data is lost in the Reduce phase, it is possible that the entire job must be restarted, because this the only way to recreate the intermediate data is to re-execute the Map tasks. SchedulingThe ResourceManager keeps track of the list of available NodeManagers (who can dynamically come and go) and their status. Just like in HDFS, NodeManagers send periodic heartbeats to the ResourceManager to give a sign of life. The ResourceManager also offers an interface so that ApplicationMasters can register and send container requests. ApplicationMasters also send periodic heartbeats to the ResourceManager. It is important to understand that, unlike the JobTracker, the ResourceManager does not monitor tasks, and will not restart slots upon failure. This job is left to the ApplicationMasters. Scheduling strategiesFIFO schedulingIn FIFO scheduling, there is one application at a time running on the entire cluster. When it is done, the next application runs again on the entire cluster, and so on. Capacity schedulingIn capacity scheduling, the resources of the cluster are partitioned into several sub-clusters of various sizes. Each one of these sub-clusters has its own queue of applications running in a FIFO fashion within this queue. Capacity scheduling also exists in a more “dynamic flavour” in which, when a sub-cluster is not currently used, its resources can be temporarily lent to the other sub-clusters. This is also in the spirit of usage maximization. Fair schedulingFair scheduling involves more complex algorithms that attempt to allocate resources in a way fair to all users of the cluster and based on the share they are normally entitled to. fair scheduling consists on making dynamic decisions regarding which requests get granted and which requests have to wait. Fair scheduling combines several ways to compute cluster shares: Steady fair share: this is the share of the cluster officially allocated to each department. Instantaneous fair share: this is the fair share that a department should ideally be allocated (according to economic and game theory considerations) at any point in time. This is a dynamic number that changes constantly, based on departments being idle: if a department is idle, then the instantaneous fair share of others department becomes higher than their steady fair shares. Current share: this is the actual share of the cluster that a department effectively uses at any point in time. This is highly dynamic. The current share does not necessarily match the instantaneous fair share because there is some inertia in the process: a department might be using more resources while another is idle. When the other department later stops being idle, these resources are not immediately withdrawn from the first department; rather, the first department will stop getting more resources, and the second department will gradually recover these resources as they get released by the first department. The easiest case of fair scheduling is when only one resource is considered: for example, only CPU cores, or only memory. Things become more complicated when several resources are considered, in practice both CPU cores and memory. This problem was solved game-theoretically with the Dominant Resource Fairness algorithm. The two (or more) dimensions are projected again to a single dimension by looking at the dominant resource for each user. exercisemotivationimprove hadoop 1.x by adding a layer YARN to separate resource management from data processing. architectureNodeMananger is generalized taskTracker.On ResourceManager, there is an ApplicationManager rsponsible for admiting new jobs and collecting finished jobs and logs.A scheduler has a global view to assign an ApplicationMaster. The applicationMaster will compute how many resources needed and send a request to the scheduler. fault tolerance is provided by applicationMaster+NodeManager+HDFS.","link":"/2023/11/09/bigdata7/"},{"title":"Probabilistic Artificial Intelligence - Bayesian Deep Learning","text":"SWAG(Stochastic Weight Averaging Gaussian)This paper proposes a different approach to Bayesian deep learning: they use the information contained in the SGD trajectory to efficiently approximate the posterior distribution over the weights of the neural network [1]. SWAThis paper shows that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training [2]. cyclical learning rate scheduleCalibration of Modern Neural NetworksConfidence calibration – the problem of predicting probability estimates representative of the true correctness likelihood – is important for classification models in many applications. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. references[1] Maddox W J, Izmailov P, Garipov T, et al. A simple baseline for bayesian uncertainty in deep learning[J]. Advances in neural information processing systems, 2019, 32.[2] Izmailov P, Podoprikhin D, Garipov T, et al. Averaging weights leads to wider optima and better generalization[J]. arXiv preprint arXiv:1803.05407, 2018.[3] Guo C, Pleiss G, Sun Y, et al. On calibration of modern neural networks[C]//International conference on machine learning. PMLR, 2017: 1321-1330.","link":"/2023/11/09/pai4/"},{"title":"bigdata - JSONiq","text":"Querying denormalized dataimperative vs declarativeImperative programming is a programming paradigm that expresses computation as a series of statements that change a program’s state. In imperative programming, the focus is on describing how a program operates step by step. Common imperative languages include C, C++, Java, and Python. In contrast to imperative programming, declarative programming focuses on describing what the program should accomplish rather than how to achieve it. In a declarative host language, the emphasis is on specifying the desired outcome or properties, and the language itself takes care of the underlying implementation details. Such as SQL, HTML. motivationDenormalized datait is characterized with two features: nestedness, and heterogeneity. In fact, denormalized datasets should not be seen as “broken tables pushed to their limits”, but rather as collections of trees. Features of a query languageA query language for datasets has three main features: Declarative, Functional, and Set-based. First, it is declarative. This means that the users do not focus on how the query is computed, but on what it should return. Being functional means that the query language is made of composable expressions that nest with each other, like a Lego game. Finally, it is set-based, in the sense that the values taken and returned by expressions are not only single values (scalars), but are large sequences of items (in the case of SQL, an item is a row). Query languages for denormalized dataFor denormalized data though, sadly, the number of languages keeps increasing: the oldest ones being XQuery, JSONiq, but then now also JMESPath, SpahQL, JSON Query, PartiQL, UnQL, N1QL, ObjectPath, JSONPath, ArangoDB Query Language (AQL), SQL++, GraphQL, MRQL, Asterix Query Language (AQL), RQL. JSONiq as a data calculatorit can perform arithmetics, but also comparison and logic. It is, however, more powerful than a common calculator and supports more complex constructs, for example variable binding.It also supports all JSON values. Any copy-pasted JSON value literally returns itself.And, unlike a calculator, it can access storage. The JSONiq Data ModelEvery expression of the JSONiq “data calculator” returns a sequence of items.An item can be either an object, an array, an atomic item, or a function item. A sequence can also be empty. Caution, the empty sequence is not the same logically as a null item. NavigationThe general idea of navigation is that it is possible to “dive” into the nested data with dots and square brackets (originally, these were slashes with XPath) – all in parallel: starting with an original collection of objects (or, possibly, a single document), each step (i.e., for each dot and each pair of square brackets) goes down the nested structure and returns another sequence of nested items. Object lookups (dot syntax)json-doc(“file.json”).o Array unboxing (empty square bracket syntax)We can unbox the array, meaning, extract its members as a sequence of seven object items, with empty square brackets.json-doc(“file.json”).o[]. Parallel navigationThe dot syntax, in fact, works on sequences, too and will extract the value associated with a key in every object of the sequence (anything else than an object is ignored and thrown away). Filtering with predicates (simple square bracket syntax)It is possible to filter any sequence with a predicate, where in the predicate refers to the current item being tested. json-doc(\"file.json\").o[].a.b[][.c = 3].It is also possible to access the item at position n in a sequence with this same notation: json-doc(“file.json”).o[].a.b[][5]. Array lookup (double square bracket syntax)To access the n-th member of an array, you can use double-squarebrackets, like so:json-doc(“file.json”).o[[2]].a. A common pitfall: Array lookup vs. Sequence predicatesDo not confuse sequence positions (single square brackets) with array positions (double square brackets)!([1, 2], [3, 4])[2] -&gt; [ 3, 4 ]([1, 2], [3, 4])[[2]] -&gt; 2 4 Schema discoveryCollectionsmany datasets are in fact found in the form of large collections of smaller objects (as in document stores). Such collections are access with a function call together with a name or (if reading from a data lake) a path. Getting all top-level keysThe keys function retrieves all keys. It can be called on the entire sequence of objects and will return all unique keys found (at the top level) in that collection. Getting unique values associated with a keyWith distinct-values, it is then possible to eliminate duplicates and look at unique values:distinct-values(collection( “https://www.rumbledb.org/samples/git-archive.jsonl“ ).type) AggregationsAggregations can be made on entire sequences with a single function call:The five basic functions are count, sum, avg, min, max.count(distinct-values(collection( “https://www.rumbledb.org/samples/git-archive.jsonl“ ).type)) ConstructionConstruction of atomic valuesAtomic values that are core to JSON can be constructed with exactly the same syntax as JSON. Construction of objects and arraysIn fact, one can copy-paste any JSON value, and it will always be recognized as a valid JSONiq query returning that value. Construction of sequencesSequences can be constructed (and concatenated) using commas.Increasing sequences of integers can also be built with the to keyword. Scalar expressionsSequences of items can have any number of items. A few JSONiq expression (arithmetic, logic, value comparison…) work on the particular case that a sequence has zero or one items. ArithmeticJSONiq supports basic arithmetic: addition (+), subtraction (-), division (div), integer division (idiv) and modulo (mod).If the data types are different, then conversions are made automatically. The empty sequence enjoys special treatment: if one of the sides (or both) is the empty sequence, then the arithmetic expression returns an empty sequence (no error). If one of the two sides is null (and the other side is not the empty sequence), then the arithmetic expression returns null. If one of the sides (or both) is not a number, null, or the empty sequence, then a type error is thrown. String manipulationString concatenation is done with a double vertical bar: “foo” || “bar”.Most other string manipulation primitives are available from the rich JSONiq builtin function library:concat(“foo”, “bar”),string-join((“foo”, “bar”, “foobar”), “-“),substr(“foobar”, 4, 3),string-length(“foobar”). Value comparisonSequences of one atomic item can be compared with eq (equal), ne (not equal), le (lower or equal), gt (greater or equal), lt (lower than) and gt (greater than). LogicJSONiq supports the three basic logic expressions and, or, and not. not has the highest precedence, then and, then or.JSONiq also supports universal and existential quantification:every $i in 1 to 10 satisfies $i gt 0, some $i in 1 to 10 satisfies $i gt 5. If one of the two sides is not a sequence of a single Boolean item, then implicit conversions are made. This mechanism is called the Effective Boolean Value (EBV). For example, an empty sequence, or a sequence of one empty string, or a sequence of one zero integer, is considered false. A sequence of one non-empty string, or a sequence or one non-zero integer, or a sequence starting with one object (or array) is considered true. General comparisonJSONiq has a shortcut for existential quantification on value comparisons. This is called general comparison. some $i in (1, 2, 3, 4, 5) satisfies $i eq 1 ==(1, 2, 3, 4, 5) = 1. ComposabilityData flowA few expressions give some control over the data flow by picking the output or this or that expression based on the value of another expression. This includes conditional expressions. This includes conditional expressions. This also includes try-catch expressions. Binding variables with cascades of let clausesVariables in JSONiq start with a dollar sign. It is important to understand that this is not a variable assignment that would change the value of a variable. This is only a declarative binding. FLWOR expressionsOne of the most important and powerful features of JSONiq is the FLWOR expression. It corresponds to SELECT-FROM-WHERE queries in SQL, however, it is considerably more expressive and generic than them in several aspects. In JSONiq the clauses can appear in any order with the exception of the first and last clause. JSONiq supports a let clause, which does not exist in SQL.In SQL, when iterating over multiple tables in the FROM clause, they “do not see each other”. In JSONiq, for clauses (which correspond to FROM clauses in SQL), do see each other, meaning that it is possible to iterate in higher and higher levels of nesting by referring to a previous for variable. For clausesIt can thus be seen that the for clause is akin to the FROM clause in SQL, and the return is akin to the SELECT clause. Projection in JSONiq can be made with a project() function call, with the keys to keep. It is possible to implement a join with a sequence of two for clauses and a predicate. Note that allowing empty can be used to perform a left outer join, i.e., to account for the case when there are no matching records in the second collection. Let clausesA let clause outputs exactly one outgoing tuple for each incoming tuple (think of a map transformation in Spark). Let clauses also allow for joining the two datasets. Where clausesWhere clauses are used to filter variable bindings (tuples) based on a predicate on these variables. They are the equivalent to a WHERE clause in SQL. Order by clausesOrder by clauses are used to reorganize the order of the tuples, but without altering them. They are the same as ORDER BY clauses in SQL.It is also possible, like in SQL, to specify an ascending or a descending order. In case of ties between tuples, the order is arbitrary. But it is possible to sort on another variable in case there is a tie with the first one (compound sorting keys). It is possible to control what to do with empty sequences: they can be considered smallest or greatest. Group by clausesGroup by clauses organize tuples in groups based on matching keys, and then output only one tuple for each group, aggregating other variables (count, sum, max, min…). It is also possible to opt out of aggregating other (non-grouping-key) variables. TypesVariable typesSince every value in JSONiq is a sequence of item, a sequence type consists of two parts: an item type, and a cardinality.Item types can be any of the builtin atomic types. as well as “object”, “array” and the most generic item type, “item”.Cardinality can be one of the following four:Any number of items (suffix ); for example object, One or more items (suffix +); for example array+,Zero or one item (suffix ?); for example integer,Exactly one item (no suffix); for example boolean? Type expressionsAn “instance of” expression checks whether a sequences matches a sequence type, and returns true or false. A “cast as” expression casts single items to an expected item type.A “castable as” expression tests whether a cast would succeed (in which case it returns true) or not (false).A “treat as” expression checks whether its input sequence matches an expected type (like a type on a variable); if it does, the input sequence is returned unchanged. If not, an error is raised. Types in user-defined functionsJSONiq supports user-defined functions. Parameter types can be optionally specified, and a return type can also be optionally specified. Validating against a schemaIt is possible to declare a schema, associating it with a user-defined type, and to validate a sequence of items against this user-defined type. Architecture of a query engineStatic phaseWhen a query is received by an engine, it is text that needs to be parsed. The output of this is a tree structure called an Abstract Syntax Tree. An Abstract Syntax Tree, even though it already has the structure of a tree, is tightly tied to the original syntax. Thus, it needs to be converted into a more abstract Intermediate Representation called an expression tree. Every node in this tree corresponds to either an expression or a clause in the JSONiq language, making the design modular. At this point, static typing takes place, meaning that the engine infers the static type of each expression, that is, the most specific type possible expected at runtime (but without actually running the program). Engines like RumbleDB perform their optimization round on this Intermediate Representation. Once optimizations have been done, RumbleDB decides the mode with which each expression and clause will be evaluated (locally, sequentially, in parallel, in DataFrames, etc). The resulting expression tree is then converted to a runtime iterator tree; this is the query plan that will actually be evaluated by the engine. Dynamic phaseDuring the dynamic phase, the root of the tree is asked to produce a sequence of items, which is to be the final output of the query as a whole. Then, recursively, each node in the tree will ask its children to produce sequences of items (or tuple streams). Each node then combines the sequences of items (or tuple streams) it receives from its children in order to produce its own sequence of items according to its semantics, and pass it to its parent. MaterializationWhen a sequence of items is materialized, it means that an actual List (or Array, or Vector), native to the language of implementation (in this case Java) is stored in local memory, filled with the items. StreamingWhen a sequence of items (or tuple stream) is produced and consumed in a streaming fashion, it means that the items (or tuples) are produced and consumed one by one, iteratively. But the whole sequence of items (or tuple stream) is never stored anywhere. The classical pattern for doing so is known as the Volcano iterator architecture. However, there are two problems with this: first, it can take a lot of time to go through the entire sequence (imagine doing so with billions or trillions of items). Second, there are expressions or clauses that are not compatible with streaming (consider, for example, the group by or order by clause, which cannot be implemented without materializing their full input). Parallel execution (with RDDs)When a sequence becomes unreasonably large, RumbleDB switches to a parallel execution, leveraging Spark capabilities: the sequences of items are passed and processed as RDDs of Item objects. Parallel execution (with DataFrames)The RDD implementation supports heterogeneous sequences by leveraging the polymorphism of Item objects. However, this is not efficient in the case that Items in the same sequence happen to have a regular structure. Thus, if the Items in a sequence are valid against a specific schema, or even against an array type or an atomic type, the underlying physical storage in memory relies on Spark DataFrames instead of RDDs. To summarize, homogeneous sequences of the most common types are stored in DataFrames, and RDDs are used in all other cases. Parallel execution (with Native SQL)In some cases (more in every release), RumbleDB is able to evaluate the query using only Spark SQL, compiling JSONiq to SQL directly instead of packing Java runtime iterators in UDFs. This leads to faster execution, because UDFs are slower than a native execution in SQL. This is because, to a SQL optimizer, UDFs are opaque and prevent automatic optimizations.","link":"/2023/11/28/bigdata10/"},{"title":"bigdata - MongoDB","text":"Document storesCan we rebuild a similar system for collections of trees, in the sense that we drop all three constraints: relational integrity, domain integrity, and atomic integrity? Document stores bring us one step in this direction. ChallengesSchema on readIn a relational database management system, it is not possible to populate a table without having defined its schema first. However, when encountering such denormalized data, in the real world, there is often no schema. In fact, one of the important features of a system that deals with denormalized data is the ability to discover a schema. Making trees fit in tablesSeveral XML elements (or, likewise, several JSON objects) can be naturally mapped to a relational table with several rows if the collection is flat and homogeneous, but semi-structured data can generally be nested and heterogeneous. if we map nested and heterogeneous into a table,such mapping will at best have to be done for every single dataset, and requires in most cases a schema, whereas we are looking for a generic solution for semistructured data with no a-priori schema information. Document storesDocument stores provide a native database management system for semi-structured data. Document stores work on collections of records, generalizing the way that relational tables can be seen as collections of rows. It is important to understand that document stores are optimized for the typical use cases of many records of small to medium sizes. Typically, a collection can have millions or billions of documents, while each single document weighs no more than 16 MB (or a size in a similar magnitude). Finally, a collection of documents need not have a schema: it is possible to insert random documents that have dissimilar structures with no problem at all. Most document stores, however, do provide the ability to add a schema. Document stores can generally do selection, projection, aggregation and sorting quite well, but many of them are typically not (yet) optimized for joining collections. In fact, often, their language or API does not offer any joining functionality at all, which pushes the burden to reimplement joins in a host language to the users. This is a serious breach of data independence. ImplementationsThere is a very large number of products in the document store space for both JSON and XML, let us mention for example MongoDB, CouchDB, ElasticSearch, Cloudant, existDB, ArangoDB, BaseX, MarkLogic, DocumentDB, CosmosDB, and so on. We will focus, as an example, on MongoDB. Physical storageJust like the storage format is optimized for tabular data in a relational database management system, it is optimized for tree-like data in a document store. In MongoDB, the format is a binary version of JSON called BSON. BSON is basically based on a sequence of tokens that efficiently encode the JSON constructs found in a document. The immediate benefit of BSON is that it takes less space in storage than JSON stored as a text file: for example, null, true and false literals need four or five bytes in text format at best, while they can be efficiently encoded as single bytes in BSON. Furthermore, BSON supports additional types that JSON does not have, such as dates. Querying paradigm (CRUD)The API of MongoDB, like many document stores, is based on the CRUD paradigm. CRUD means Create, Read, Update, Delete and corresponds to low-level primitives similar to those for HBase. MongoDB supports several host languages to query collections via an API. This includes in particular JavaScript and Python, but many other languages are supported via drivers. We will use JavaScript here because this is the native host language of MongoDB. It is important to note that these APIs are not query languages. MongoDB also provides access to the data via a shall called mongo or, newly, mongosh. This is a simple JavaScript interface wrapped around the MongoDB’s node.js driver. Populating a collectionTo create a collection, one can simply insert a document in it, and it will be automatically created if it does not exist. MongoDB automatically adds to every inserted document a special field called “ id” and associated with a value called an Object ID and with a type of its own.Object IDs are convenient for deleting or updating a specific document with no ambiguity. Querying a collectionScan a collectionAsking for the contents of an entire collection is done with a simple find() call on the previous object:db.collection.find().This function does not in fact return the entire collection; rather, it returns some pointer, called a cursor, to the collection; the user can then iterate on the cursor in an imperative fashion in the host language. SelectionIt is possible to perform a selection on a collection by passing a parameter to find() that is a JSON object:db.collection.find({ “Theory” : “Relativity” }). A disjunction (OR) uses a special MongoDB keyword, prefixed with a dollar sign:db.collection.find( { “$or” : [ { “Theory”:”Relativity” }, { “Last”:”Einstein” } ] } ). MongoDB offers many other keywords, for example for comparison other than equality:db.collection.find( { “Publications” : { “$gte” : 100 } } ) ProjectionProjections are made with the second parameter of this same find() method. This is done in form of a JSON object associating all the desired keys in the projection with the value 1. db.scientists.find( { “Theory” : “Relativity” }, { “First” : 1, “Last”: 1 } ). It is also possible to project fields away in the same way with 0s, however 1s and 0s cannot be mixed in the projection parameter, except in the specific above case of projecting away the object ID CountingCounting can be done by chaining a count() method call:db.scientists.find( { “Theory” : “Relativity” } ).count(). SortingSorting can be done by chaining a sort() method call.db.scientists.find( { “Theory” : “Relativity” }, { “First” : 1, “Last” : 1 } ).sort( { “First” : 1, “Name” : -1 } ) 1 is for ascending order and -1 for descending order.It is also possible to add limits and offsets to paginate results also by chaining more method calls:db.scientists.find( { “Theory” : “Relativity” }, { “First” : 1, “Last” : 1 } ).sort( { “First” : 1, “Name” : -1 } ).skip(30).limit(10). Note that, contrary to intuition, the order of the calls does not matter, as this is really just the creation of a query plan by providing parameters (in any order). Duplicate eliminationIt is possible to obtain all the distinct values for one field with a distinct() call: db.scientists.distinct(“Theory”) Querying for heterogeneityAbsent fieldsAbsent fields can be filtered with: db.scientists.find( { “Theory” : null } ). Filtering for values across typesdb.collection.find( { “$or” : [ { “Theory”: “Relativity” }, { “Theory”: 42 }, { “Theory”: null } ] } ) db.scientists.find( { “Theory” : { “$in” : [“Relativity”, 42, null ] } } ). MongoDB is also able to sort on fields that have heterogeneous data types. It does so by first order by type in some (arbitrary, but documented) order, and then within each type. Querying for nestednessNestedness in MongoDB is handled in several ad-hoc ways for specific use cases. Values in nested objectsWe saw how to select documents based on values associated with a top-level keys. What about values that are not at the top-level, but in nested objects?The first solution that might come to mind is something like this: db.scientists.find({ “Name” : { “First” : “Albert” } }) However, this query will not have the behavior many would have expected: instead of finding documents that have a value “Albert” associated with the key “First” in an object itself associated with the top-level key ”Name”, this query looks for an exact match of the entire object.In order to include documents such as above, MongoDB uses a dot syntax: db.scientists.find({ “Name.First” : “Albert” }). Values in nested arraysMongoDB allows to filter documents based on whether a nested array contains a specific value, like so: db.scientists.find({ “Theories” : “Special relativity” }). Deleting objects from a collectionObjects can be deleted from a collection either one at a time with deleteOne(), or several at a time with deleteMany(): db.scientists.deleteMany( { “century” : “15” } ). Updating objects in a collectionDocuments can be updated with updateOne() and updateMany() by providing both a filtering criterion (with the same syntax as the first parameter of find()) and an update to apply. The command looks like so: db.scientists.updateMany( { “Last” : “Einstein” }, { $set : { “Century” : “20” } } ). The granularity of updates is per document, that is, a single document can be updated by at most one query at the same time.However, within the same collection, several different documents can be modified concurrently by different queries in parallel. Complex pipelinesFor grouping and such more complex queries, MongoDB provides an API in the form of aggregation pipelines.db.scientists.aggregate( { $match : { “Century” : 20 }}, { $group : { “Year” : “$year”, “Count” : { “$sum” : 1 } } }, { $sort : { “Count” : -1 } }, { $limit : 5 } ). Limitations of a document store querying APISimple use cases are straightforward to handle, however more complex use cases require a lot of additional code in the host language, be it JavaScript or Python. An example is that joins must be taken care of by the end user in the host language: the burden of implementing more complex use cases is pushed to the end user. ArchitectureThe architecture of MongoDB follows similar principles to what we covered before: scaling out the hardware to multiple machine, and sharding as well as replicating the data. Sharding collectionsCollections in MongoDB can be sharded. Shards are determined by selecting one or several fields. (Lexicographically-ordered) intervals over these fields then determine the shards. The fields used to shard must be organized in a tree index structure. Replica setsA replica set is a set of several nodes running the MongoDB server process. The nodes within the same replica set all have a copy of the same data. Each shard of each collection is assigned to exactly one replica set. Note that this architecture is not the same as that of HDFS, in which the replicas are spread over the entire cluster with no notion of “walls” between replica sets and no two DataNodes having the exact same block replicas. Write concernsWhen writing (be it delete, update or insert) to a collection, more exactly, to a specific shard of a collection, MongoDB checks that a specific minimum number of nodes (within the replica set that is responsible for the shard) have successfully performed the update. MotivationA document store, unlike a data lake, manages the physical data layout. This has a cost: the need to import (ETL) data before it is possible to query it, but this cost comes with a nice benefit: index support, just like relational database management systems. Hash indicesHash indices are used to optimize point queries and more generally query that select on a specific value of a field. The general idea is that all the values that a field takes in a specific collection can be hashed to an integer. The value, together with pointers to the corresponding documents, is then placed in a physical array in memory, at the position corresponding to this integer. Tree indicesHash indices are great and fast, but have limitations: first, they consume space. The more one wants to avoid hash collisions, the larger the array needs to be. But more importantly, hash indices cannot support range queries. This is because hashes do not preserve the order of the values and distribute them “randomly” in the index array structure. Range queries are supported with tree indices. Instead of an array, tree indices use some sort of tree structure in which they arrange the possible values of the indexed field, such that the values are ordered when traversing the tree in a depth-first-search manner. More precisely, the structure is called a B+-tree. Unlike a simple binary tree, nodes have a large number of children. Secondary indicesBy default, MongoDB always builds a tree index for the id field. Users can request to build hash and tree indices for more fields. These indices are called secondary indices. The command for building a hash index looks like so: db.scientists.createIndex({ “Name.Last” : “hash” }) And for a tree index (1 means in ascending order, -1 would be descending): db.scientists.createIndex({ “Name.Last” : 1 }). When are indices usefulWhen building indices, it is important to get a feeling for whether a query will be faster or not with this index. index typesSingle Field IndexesBy default, all collections have an index on the _id field. You can add additional indexes to speed up important queries and operations. You can create a single-field index on any field in a document, including:Top-level document fields, Embedded documents ,Fields within embedded documents. When you create an index on an embedded document, only queries that specify the entire embedded document use the index. Queries on a specific field within the document do not use the index. In order for a dot notation query to use an index, you must create an index on the specific embedded field you are querying, not the entire embedded object. Compound IndexesCompound indexes collect and sort data from two or more fields in each document in a collection. Data is grouped by the first field in the index and then by each subsequent field. The order of the indexed fields impacts the effectiveness of a compound index. Compound indexes contain references to documents according to the order of the fields in the index. To create efficient compound indexes, follow the ESR (Equality, Sort, Range) rule. The ESR (Equality, Sort, Range) Rule is to place fields that require exact matches first in your index. Sort follows equality matches because the equality matches reduce the number of documents that need to be sorted. Sorting after the equality matches also allows MongoDB to do a non-blocking sort. “Range” filters scan fields. The scan doesn’t require an exact match, which means range filters are loosely bound to index keys. To improve query efficiency, make the range bounds as tight as possible and use equality matches to limit the number of documents that must be scanned. MongoDB cannot do an index sort on the results of a range filter. Place the range filter after the sort predicate so MongoDB can use a non-blocking index sort. Compound indexes cannot support queries where the sort order does not match the index or the reverse of the index. Index prefixes are the beginning subsets of indexed fields. Compound indexes support queries on all fields included in the index prefix. Index fields are parsed in order; if a query omits an index prefix, it is unable to use any index fields that follow that prefix. Multikey IndexesMultikey indexes collect and sort data from fields containing array values. Multikey indexes improve performance for queries on array fields. In a compound multikey index, each indexed document can have at most one indexed field whose value is an array. exercisedata modelEmbedded Data ModelsIn general, embedding provides better performance for read operations, as well as the ability to request and retrieve related data in a single database operation. Embedded data models make it possible to update related data in a single atomic write operation. Normalized Data ModelsNormalized data models describe relationships using references between documents. referenceshttps://www.mongodb.com/docs/manual/core/data-model-design/","link":"/2023/11/28/bigdata9/"},{"title":"bigdata - spark","text":"Generic Dataflow ManagementMapReduce is very simple and generic, but many more complex uses involve not just one, but a sequence of several MapReduce jobs. Furthermore, the MapReduce API is low-level, and most users need higherlevel interfaces, either in the form of APIs or query languages. This is why, after MapReduce, another generation of distributed processing technologies were invented. The most popular one is the open source Apache Spark. A more general dataflow modelMapReduce consists of a map phase, followed by shuffling, followed by a reduce phase. In fact, the map phase and the reduce phase are not so different: both involve the computation of tasks in parallel on slots. Resilient distributed datasetsThe first idea behind generic dataflow processing is to allow the dataflow to be arranged in any distributed acyclic graph (DAG). All the rectangular nodes in the above graph correspond to intermediate data. They are called resilient distributed datasets, or in short, RDDs. A major difference with MapReduce, though, is that RDDs need not be collections of pairs. In fact, RDDs can be (ordered) collections of just about anything: strings, dates, integers, objects, arrays, arbitrary JSON values, XML nodes, etc.The only constraint is that the values within the same RDD share the same static type, which does not exclude the use of polymorphism. The RDD lifecycleCreationRDDs undergo a lifecycle. First, they get created. RDDs can be created by reading a dataset from the local disk, or from cloud storage, or from a distributed file system, or from a database source, or directly on the fly from a list of values residing in the memory of the client using Apache Spark TransformationThen, RDDs can be transformed into other RDDs. Mapping or reducing, in this model, become two very specific cases of transformations. However, Spark allows for many, many more kinds of transformations. This also includes transformations with several RDDs as input. ActionRDDs can also undergo a final action leading to making an output persistent. This can be by outputting the contents of an RDD to the local disk, to cloud storage, to a distributed file system, to a database system, or directly to the screen of the user. Lazy evaluationAnother important aspect of the RDD lifecycle is that the evaluation is lazy: in fact, creations and transformations on their own do nothing. It is only with an action that the entire computation pipeline is put into motion, leading to the computation of all the necessary intermediate RDDs all the way down to the final output corresponding to the action. TransformationsUnary transformationsLet us start with unary transformations, i.e., those that take a single RDD as their input. Binary transformationsThere are also transformations that take two RDDs as input. Pair transformationsSpark has transformations specifically tailored for RDDs of key-value pairs. ActionsGathering output locallyThe collect action downloads all values of an RDD on the client machine and outputs them as a (local) list. It is important to only call this action on an RDD that is known to be small enough (e.g., after filtering) to avoid a memory overflow. The count action computes (in parallel) the total number of values in the input RDD. This one is safe even for RDDs with billions of values, as it returns a simple integer to the client. Writing to sharded datasetsThere is also an action called saveAsTextFile that can save the entire RDD to a sharded dataset on Cloud storage (S3, Azure blob storage) or a distributed file system (HDFS).Binary outputs can be saved with saveAsObjectFile. Actions on Pair RDDsPhysical architectureNarrow-dependency transformationsIn a narrow-dependency transformation, the computation of each output value involves a single input value. In a narrow-dependency transformation, the computation of each output value involves a single input value. By default, if the transformation is applied to an RDD freshly created from reading a dataset from HDFS, each partition will correspond to an HDFS block. Thus, the computation of the narrow-dependency transformation mostly involves local reads by short-circuiting HDFS. In fact, the way this works is quite similar to MapReduce: the sequential calls of the transformation function on each input value within a single partition is called a task. And just like MapReduce, the tasks are assigned to slots. These slots correspond to cores within YARN containers. YARN containers used by Spark are called executors. The processing of the tasks is sequential within each executor, and tasks are executed in parallel across executors. And like in MapReduce, a queue of unprocessed tasks is maintained, and everytime a slot is done, it gets a new task. When all tasks have been assigned, the slots who are done become idle and wait for all others to complete. Chains of narrow-dependency transformationsIn fact, on the physical level, the physical calls of the underlying map/filter/etc functions are directly chained on each input value to directly produce the corresponding final, output value, meaning that the intermediate RDDs are not even materialized anywhere and exist purely logically. Such a chain of narrow-dependency transformations executed efficiently as a single set of tasks is called a stage, which would correspond to what is called a phase in MapReduce. Physical parametersUsers can parameterize how many executors there are, how many cores there are per executor and how much memory per executor (remember that these then correspond to YARN container requests). ShufflingWhat about wide-dependency transformations? They involve a shuffling of the data over the network, so that the data necessary to compute each partition of the output RDD is physically at the same location.Thus, on the high-level of a job, the physical execution consists of a sequence of stages, with shuffling happening everytime a new stage begins. OptimizationsPinning RDDsEverytime an action is triggered, all the computations of the ”reverse transitive closure” (i.e., all the way up the DAG through the reverted edges) are set into motion. In some cases, several actions might share subgraphs of the DAG. It makes sense, then, to “pin” the intermediate RDD by persisting it. Pre-partitioningShuffle is needed to bring together the data that is needed to jointly contribute to individual output values. If, however, Spark knows that the data is already located where it should be, then shuffling is not needed. DataFrames in SparkData independenceUnlike a relational database that has everything right off-theshelf, with RDDs, the user has to re-implement all the primitives they need. This is a breach of the data independence principle. The developers behind Spark addressed this issue in a subsequent version of Spark by extending the model with support for DataFrames and Spark SQL, bringing back a widely established and popular declarative, high-level language into the ecosystem. A specific kind of RDDA DataFrame can be seen as a specific kind of RDD: it is an RDD of rows (equivalently: tuples, records) that has relational integrity, domain integrity, but not necessarily atomic integrity. Performance impactDataFrames are stored column-wise in memory, meaning that the values that belong to the same column are stored together. Furthermore, since there is a known schema, the names of the attributes need not be repeated in every single row, as would be the case with raw RDDs. DataFrames are thus considerably more compact in memory than raw RDDs. Generally, Spark converts Spark SQL to internal DataFrame transformation and eventually to a physical query plan. An optimizer known as Catalyst is then able to find many ways of making the execution faster. Input formatsNote that Spark automatically infers the schema from discovering the JSON Lines file, which adds a static performance overhead that does not exist for raw RDDs. DataFrame transformationsIt is also possible, instead of Spark SQL, to use a transformation API similar to (but distinct from) the RDD transformation API. Unlike the RDD transformation API, there is no guarantee that the execution will happen as written, as the optimizer is free to reorganize the actual computations. DataFrame column typesDataFrames also support the three structured types: arrays, structs, and maps. As a reminder, structs have string keys and arbitrary value types and correspond to generic JSON objects, while in maps, all values have the same type. Thus, structs are more common than maps. The Spark SQL dialectNote that both GROUP BY and ORDER BY will trigger a shuffle in the system, even though this can be optimized as the grouping key and the sorting key are the same. The SORT BY clause can sort rows within each partition, but not across partitions, i.e., does not induce any shuffling. The DISTRIBUTE BY clause forces a repartition by putting all rows with the same value (for the specified field(s)) into the same new partition.Note that the SORT BY clause is used to return the result rows sorted within each partition in the user specified order. When there is more than one partition SORT BY may return result that is partially ordered. This is different than ORDER BY clause which guarantees a total order of the output. A word of warning must be given on SORT, DISTRIBUTE and CLUSTER clauses: they are, in fact, a breach of data independence, because they expose partitions. Spark SQL also comes with language features to deal with nested arrays and objects. First, nested arrays can be navigated with the explode() function. Lateral views are more powerful and generic than just an explode() because they give more control, and they can also be used to go down several levels of nesting. A lateral view can be intuitively described this way: the array mentioned in the LATERAL VIEW clause is turned into a second, virtual table with the rest of the original table is joined. The other clauses can then refer to columns in both the original and second, virtual table. exerciseRDDWhy RDD should be immutable and lazy: immutable is for lineage.Why need RDD partitioning: parallel computing and reduce shuffling. DataFrame APIFor nested array,use array_contains. spark SQLIn jupyter notebook, we can use “%load_ext sparksql_magic” directly. referencehttps://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/","link":"/2023/11/14/bigdata8/"},{"title":"pai - active learning","text":"Active learningActive learning is a machine learning paradigm in which a model is trained on a dataset that is not entirely labeled. Instead of relying solely on a fully labeled dataset for training, active learning involves an iterative process where the model actively selects the most informative instances from an unlabeled pool, queries an oracle (typically a human annotator), and adds the newly labeled instances to the training set. The model is then retrained on the expanded labeled dataset. The key idea behind active learning is to strategically choose the most valuable instances for labeling, with the goal of improving the model’s performance while minimizing the number of labeled instances needed. This process is especially beneficial when obtaining labeled data is expensive or time-consuming. Active learning strategies vary in how they select instances for labeling. Common strategies include uncertainty sampling (select instances where the model is uncertain), query-by-committee (select instances where different model hypotheses disagree), and diversity sampling (select instances to ensure diverse coverage of the input space). Conditional Entropy Intuitively, the conditional entropy of X given Y describes our average surprise about realizations of X given a particular realization of Y, averaged over all such possible realizations of Y. In other words, conditional entropy corresponds to the expected remaining uncertainty in X after we observe Y. That is, the joint entropy of X and Y is given by the uncertainty about X and the additional uncertainty about Y given X. Mutual Information In words, we subtract the uncertainty left about X after observing Y from our initial uncertainty about X. This measures the reduction in our uncertainty in X (as measured by entropy) upon observing Y. Thus, the mutual information between X and Y can be understood as the approximation error (or information loss) when assuming that X and Y are independent. Thus, the conditional mutual information corresponds to the reduction of uncertainty in X when observing Y, given we already observed Z. Following our introduction of mutual information, it is natural to answer the question “where should I collect data?” by saying “wherever mutual information is maximized”. Submodularity of Mutual Information That is, “adding” x to the smaller set A yields more marginal gain than adding x to the larger set B. In other words, the function F has “diminishing returns”. In this way, submodularity can be interpreted as a notion of “concavity” for discrete functions. Maximizing Mutual InformationUncertainty Sampling Therefore, if f is modeled by a Gaussian and we assume homoscedastic noise, greedily maximizing mutual information corresponds to simply picking the point x with the largest variance. This strategy is also called uncertainty sampling. Heteroscedastic NoiseUncertainty sampling is clearly problematic if the noise is heteroscedastic. If there are a particular set of inputs with a large aleatoric uncertainty dominating the epistemic uncertainty, uncertainty sampling will continuously choose those points even though the epistemic uncertainty will not be reduced substantially. Thus, we choose locations that trade large epistemic uncertainty with large aleatoric uncertainty. Ideally, we find a location where the epistemic uncertainty is large, and the aleatoric uncertainty is low, which promises a significant reduction of uncertainty around this location. ClassificationHere, uncertainty sampling corresponds to selecting samples that maximize the entropy of the predicted label yx.","link":"/2023/11/16/pai5/"},{"title":"pai - Bayesian Optimization","text":"Bayesian Optimization Exploration-Exploitation DilemmaIn Bayesian optimization, we want to learn a model of f ⋆ and use this model to optimize f ⋆ simultaneously. These goals are somewhat contrary. Learning a model of f ⋆ requires us to explore the input space while using the model to optimize f ⋆ requires us to focus on the most promising well-explored areas. This trade-off is commonly known as the exploration-exploitation dilemma. It is common to use a so-called acquisition function to greedily pick the next point to sample based on the current model. Online Learning and Bandits Multi-Armed Bandits(MAB)The “multi-armed bandits” (MAB) problem is a classical, canonical formalization of the exploration-exploitation dilemma. In the MAB problem, we are provided with k possible actions (arms) and want to maximize our reward online within the time horizon T. We do not know the reward distributions of the actions in advance, however, so we need to trade learning the reward distribution with following the most promising action. Bayesian optimization can be interpreted as a variant of the MAB problem where there can be a potentially infinite number of actions (arms), but their rewards are correlated (because of the smoothness of the Gaussian process prior). Smoothness means that nearby points in the input space are likely to have similar function values. Because of the smoothness property inherent in the Gaussian process prior, Bayesian optimization can make informed decisions about where to explore next in the input space. The model can leverage information from previously evaluated points to predict the behavior of the objective function at unexplored points more effectively. RegretThe key performance metric in online learning is the regret. Achieving sublinear regret requires balancing exploration with exploitation. Typically, online learning (and Bayesian optimization) consider stationary environments, hence the comparison to the static optimum. Acquisition FunctionsThroughout our description of acquisition functions, we will focus on a setting where we model $f^⋆$ using a Gaussian process which we denote by f. The methods generalize to other means of learning $f^⋆$ such as Bayesian neural networks. One possible acquisition function is uncertainty sampling. However, this acquisition function does not at all take into account the objective of maximizing $f^⋆$ and focuses solely on exploration. Suppose that our model f of $f^⋆$ is well-calibrated, in the sense that the true function lies within its confidence bounds. Consider the best lower bound, that is, the maximum of the lower confidence bound. Now, if the true function is really contained in the confidence bounds, it must hold that the optimum is somewhere above this best lower bound. Therefore, we only really care how the function looks like in the regions where the upper confidence bound is larger than the best lower bound. The key idea behind the methods that we will explore is to focus exploration on these plausible maximizers. Upper Confidence BoundThis acquisition function naturally trades exploitation by preferring a large posterior mean with exploration by preferring a large posterior variance.This optimization problem is non-convex in general. However, we can use approximate global optimization techniques like Lipschitz optimization (in low dimensions) and gradient ascent with random initialization (in high dimensions). Another widely used option is to sample some random points from the domain, score them according to this criterion, and simply take the best one. Observe that if the information gain is sublinear in T then we achieve sublinear regret and, in particular, converge to the true optimum. The term “sublinear” refers to a growth rate that is slower than linear. Intuitively, to work even if the unknown function $f^⋆$ is not contained in the confidence bounds, we use βt to re-scale the confidence bounds to enclose $f^⋆$. Probability of Improvement Probability of improvement tends to be biased in favor of exploitation, as it prefers points with large posterior mean and small posterior variance. Expected Improvement Intuitively, expected improvement seeks a large expected improvement (exploitation) while also preferring states with a large variance (exploration). Thompson Sampling Probability matching is exploratory as it prefers points with larger variance (as they automatically have a larger chance of being optimal), but at the same time exploitative as it effectively discards points with low posterior mean and low posterior variance. Unfortunately, it is generally difficult to compute π analytically given a posterior. Instead, it is common to use a sampling-based approximation of π. In many cases, the randomness in the realizations of ̃ ft+1 is already sufficient to effectively trade exploration and exploitation. Model SelectionSelecting a model of f ⋆ is much harder than in the i.i.d. data setting of supervised learning. There are mainly the two following dangers, • the data sets collected in active learning and Bayesian optimization are small; and • the data points are selected dependently on prior observations. This leads to a specific danger of overfitting. In particular, due to feedback loops between the model and the acquisition function, one may end up sampling the same point repeatedly. Another approach that often works fairly well is to occasionally (according to some schedule) select points uniformly at random instead of using the acquisition function. This tends to prevent getting stuck in suboptimal parts of the state space. difference betwwen active learning and Bayesian OptimizationProblem Setting:Active Learning: Active learning typically deals with supervised learning tasks where there is a large pool of unlabeled instances, and the algorithm decides which instances to query for labels to improve the model.Bayesian Optimization: Bayesian optimization deals with optimization problems where the objective function is unknown, expensive to evaluate, and possibly noisy. It aims to find the global optimum with as few evaluations as possible. Nature of Queries:Active Learning: In active learning, the queries are often in the form of “Which instance should be labeled next?” The goal is to select instances that will most benefit the model’s learning process.Bayesian Optimization: In Bayesian optimization, the queries are in the form of “What point should be evaluated next in the input space to maximize/minimize the objective function?” The goal is to efficiently explore the input space and find the optimal configuration. Algorithmic Approaches:Active Learning: Active learning involves various strategies such as uncertainty sampling, query-by-committee, and diversity sampling to select informative instances for labeling.Bayesian Optimization: Bayesian optimization employs probabilistic surrogate models (often Gaussian processes) to model the unknown objective function. Acquisition functions guide the search to balance exploration and exploitation efficiently.","link":"/2023/11/17/pai6/"},{"title":"pai - Markov Decision Processes","text":"Markov Decision ProcessesPlanning deals with the problem of deciding which action an agent should play in a (stochastic) environment(An environment is stochastic as opposed to deterministic, when the outcome of actions is random.). A key formalism for probabilistic plan ning in known environments are so-called Markov decision processes. Our fundamental objective is to learn how the agent should behave to optimize its reward. In other words, given its current state, the agent should decide (optimally) on the action to play. Such a decision map — whether optimal or not — is called a policy. For the purpose of our discussion of Markov decision processes and reinforcement learning, we will focus on a very common reward called discounted payoff. Because we assumed stationary dynamics, rewards, and policies, the discounted payoff starting from a given state x will be independent of the start time t. Bellman Expectation EquationLet us now see how we can compute the value function： This equation is known as the Bellman expectation equation, and it shows a recursive dependence of the value function on itself. The intuition is clear: the value of the current state corresponds to the reward from the next action plus the discounted sum of all future rewards obtained from the subsequent states. Policy EvaluationBellman’s expectation equation tells us how we can find the value function vπ of a fixed policy π using a system of linear equations. Fixed-point Iteration Policy OptimizationGreedy Policies Bellman Optimality Equation These equations are also called the Bellman optimality equations. Intuitively, the Bellman optimality equations express that the value of a state under an optimal policy must equal the expected return for the best action from that state. Bellman’s theorem is also known as Bellman’s optimality principle, which is a more general concept. Policy Iteration It can be shown that policy iteration converges to an exact solution in a polynomial number of iterations.Each iteration of policy iteration requires computing the value function, which we have seen to be of cubic complexity in the number of states. Value IterationValue iteration converges to an ε-optimal solution in a polynomial number of iterations. Unlike policy iteration, value iteration does not converge to an exact solution in general.an iteration of 7 Sparsity refers to the interconnectivity of the state space. When only few states are reachable from any state, we call an MDP sparse. value iteration can be performed in (virtually) constant time in sparse Markov decision processes. Partial ObservabilityIn this section, we consider how Markov decision processes can be extended to a partially observable setting where the agent can only access noisy observations Yt of its state Xt. Observe that a Kalman filter can be viewed as a hidden Markov model with conditional linear Gaussian motion and sensor models and a Gaussian prior on the initial state. POMDPs can be reduced to a Markov decision process with an enlarged state space.","link":"/2023/11/17/pai7/"},{"title":"pai - Tabular Reinforcement Learning","text":"Tabular Reinforcement LearningThe Reinforcement Learning ProblemReinforcement learning is concerned with probabilistic planning in unknown environments. In this chapter, we will begin by considering reinforcement learning with small state and action spaces. This setting is often called the tabular setting, as the value functions can be computed exhaustively for all states and stored in a table. Clearly, the agent needs to trade exploring and learning about the environment with exploiting its knowledge to maximize rewards. In fact, Bayesian optimization can be viewed as reinforcement learning with a fixed state and a continuous action space: In each round, the agent plays an action, aiming to find the action that maximizes the reward.Another key challenge of reinforcement learning is that the observed data is dependent on the played actions. Trajectories Crucially, the newly observed states xt+1 and the rewards rt (across multiple transitions) are conditionally independent given the previous states xt and actions at. This independence property is crucial for being able to learn about the underlying Markov decision process. Notably, this implies that we can apply the law of large numbers (1.83) and Hoeffding’s inequality (1.87) to our estimators of both quantities. The collection of data is commonly classified into two settings. In the episodic setting, the agent performs a sequence of “training” rounds (called episodes). In the beginning of each round, the agent is reset to some initial state. In contrast, in the continuous setting (or non-episodic,or online setting), the agent learns online. controlAnother important distinction in how data is collected, is the distinction between on-policy and off-policy control. As the names suggest, on-policy methods are used when the agent has control over its own actions, in other words, the agent can freely choose to follow any policy. In contrast, off-policy methods can be used even when the agent cannot freely choose its actions. Off-policy methods are therefore able to make use of observational data.Off-policy methods are therefore more sample-efficient than on-policy methods. This is crucial, especially in settings where conducting experiments (i.e., collecting new data) is expensive. On-Policy learning algorithms are the algorithms that evaluate and improve the same policy which is being used to select actions. Off-Policy learning algorithms evaluate and improve a policy that is different from Policy that is used for action selection. To understand the difference between on-policy learning and off-policy learning one must first understand the difference between the behavior policy (i.e., sampling policy) and the update policy. The behavior policy is the policy an agent follows when choosing which action to take in the environment at each time step. The update policy is how the agent updates the Q-function. On-policy algorithms attempt to improve upon the current behavior policy that is used to make decisions and therefore these algorithms learn the value of the policy carried out by the agent, Off-policy algorithms learn the value of the optimal policy and can improve upon a policy that is different from the behavior policy. Determining if the update and behavior policy are the same or different can give us insight into whether or not the algorithm is on-policy or off-policy. Model-based ApproachesApproaches to reinforcement learning are largely categorized into two classes. Model-based approaches aim to learn the underlying Markov decision process. In contrast, model-free approaches learn the value function directly. Learning the Underlying Markov Decision ProcessA natural first idea is to use maximum likelihood estimation to approximate transition and reward function. ε-greedy Algorithm The key problem of ε-greedy is that it explores the state space in an uninformed manner. In other words, it explores ignoring all past experience. It thus does not eliminate clearly suboptimal actions. Rmax AlgorithmA key principle in effectively trading exploration and exploitation is “optimism in the face of uncertainty”. Let us apply this principle to the reinforcement learning setting. The key idea is to assume that the dynamics and rewards model “work in our favor” until we have learned “good estimates” of the true dynamics and rewards. How many transitions are “enough”? We can use Hoeffding’s inequality to get a rough idea! challengesModel-free ApproachesA significant benefit to model-based reinforcement learning is that it is inherently off-policy. That is, any trajectory regardless of the policy used to obtain it can be used to improve the model of the underlying Markov decision process. In the model-free setting, this not necessarily true. On-policy Value Estimation Note that to estimate this expectation we use a single(!) sample.However, there is one significant problem in this approximation. Our approximation of vπ does in turn depend on the (unknown) true value of vπ. The key idea is to use a bootstrapping estimate of the value function instead. That is, in place of the true value function vπ, we will use a “running estimate” Vπ. In other words, whenever observing a new transition, we use our previous best estimate of vπ to obtain a new estimate Vπ. Crucially, using a bootstrapping estimate generally results in biased estimates of the value function. Moreover, due to relying on a single sample, the estimates tend to have very large variance. TD-learningThe variance of the estimate is typically reduced by mixing new estimates of the value function with previous estimates using a learning rate αt. This yields the temporal-difference learning algorithm. TD-learning is a fundamentally on-policy method. That is, for the estimates Vπ to converge to the true value function vπ, the transitions that are used for the estimation must follow policy π. SARSA Off-policy Value EstimationThis adapted update rule explicitly chooses the subsequent action a′ according to policy π whereas SARSA absorbs this choice into the Monte Carlo approximation. The algorithm has analogous convergence guarantees to those of SARSA. Crucially, this algorithm is off-policy. As noted, the key difference to the on-policy TD-learning and SARSA is that our estimate of the Qfunction explicitly keeps track of the next-performed action. It does so for any action in any state. Q-learningCrucially, the Monte Carlo approximation of eq. (11.21) does not depend on the policy. Thus, Q-learning is an off-policy method. Optimistic Q-learning ChallengesWe have seen that both the model-based Rmax algorithm and the modelfree Q-learning take time polynomial in the number of states |X| and the number of actions |A| to converge. While this is acceptable in small grid worlds, this is completely unacceptable for large state and action spaces. referenceshttps://core-robotics.gatech.edu/2022/02/28/bootcamp-summer-2020-week-4-on-policy-vs-off-policy-reinforcement-learning/","link":"/2023/11/17/pai8/"},{"title":"bigdata - Graph Database","text":"Why graphsNow, we do know a way to avoid joins and studied it at length: denormalizing the data to (homogeneous) collections of trees is a way of “pre-computing” the joins statically, so that the data is already joined (via nesting) at runtime. Now, why is it efficient? Because doing down a tree only necessitates following pointers in memory. But trees cannot have cycles. Graph databases provide a way of generalizing the use in-memory pointers to traverse data to the general case in which cycles are present: this is called “index-free adjacency.” Kinds of graph databasesThere are many different graph database system products on the market, and they can be classified along several dimensions: Labeled property graph model vs. triple stores;Read-intensive vs. write-intensive;Local vs. distributed;Native vs. non-native. Graph data modelsLabeled property graphsComputer scientists need to go one step further and also design how to store graphs physically. One way of doing so is to create an associative array mapping each node to the list of nodes that it connects to via an edge (adjacency lists). Another storage form is with an adjacency matrix: each row and each column represent a node, and a 0 or a 1 indicate the absence or presence of an edge between the row node and the column node. Now, this does not quite work for us, because labeled property graphs enhance mathematical graphs with extra ingredients: properties, and labels.how to “convert” a relational table to a labeled property graph: labels can be seen as table names, nodes as records, and properties as the attribute values for the records. This shows that relational tables can be physically stored as labeled property graphs. Of course, this does not work the other way round: given a graph, it will often not be possible to convert it “back” to a table in this specific way. Triple storesTriple stores are a different and simpler model. It views the graph as nodes and edges that all have labels, but without any properties. The graph is then represented as a list of edges, where each edge is a triple with the label of the origin node (called the subject), the label of the edge (called the property), and the label of the destination node (called the object). Triple stores typically provide SPARQL capabilities to reason about and stored RDF data. Querying graph dataWe will now have a look at query languages for querying graphs, with a focus on Cypher, which is neo4j’s query language. Cypher PhilosophyCypher enables a user (or an application acting on behalf of a user) to ask the database to find data that matches a specific pattern. Colloquially, we ask the database to “find things like this.” And the way we describe what “things like this” look like is to draw them, using ASCII art. Like most query languages, Cypher is composed of clauses. The simplest queries consist of a MATCH clause followed by a RETURN clause. There are other clauses we can use in a Cypher query: WHERE,WITH…AS…,CREATE,MERGE,DELETE,SET,UNION,FORWACH and so on. A Comparison of Relational and Graph ModelingRelational databases—with their rigid schemas and complex modeling characteristics—are not an especially good tool for supporting rapid change. What we need is a model that is closely aligned with the domain, but that doesn’t sacrifice performance, and that supports evolution while maintaining the integrity of the data as it undergoes rapid change and growth. That model is the graph model. creating a graphIn practice, we tend to use CREATE when we’re adding to the graph and don’t mind duplication, and MERGE when duplication is not permitted by the domain. Beginning a QueryIn Cypher we always begin our queries from one or more well-known starting points in the graph—what are called bound nodes. Cypher uses any labels and property predicates supplied in the MATCH and WHERE clauses, together with metadata supplied by indexes and constraints, to find the starting points that anchor our graph patterns. INDEXES AND CONSTRAINTSTo support efficient node lookup, Cypher allows us to create indexes per label and property combinations. For unique property values we can also specify constraints that assure uniqueness. Neo4j Neo4j is a graph database with native processing capabilities as well as native graph storage. Native Graph ProcessingOf the many different engine architectures, we say that a graph database has native processing capabilities if it exhibits a property called index-free adjacency. A database engine that utilizes index-free adjacency is one in which each node maintains direct references to its adjacent nodes. Each node, therefore, acts as a micro-index of other nearby nodes, which is much cheaper than using global indexes. It means that query times are independent of the total size of the graph, and are instead simply proportional to the amount of the graph searched. A nonnative graph database engine, in contrast, uses (global) indexes to link nodes together. Proponents for native graph processing argue that index-free adjacency is crucial for fast, efficient graph traversals. To understand why native graph processing is so much more efficient than graphs based on heavy indexing, consider the following. Depending on the implementation, index lookups could be O(log n) in algorithmic complexity versus O(1) for looking up immediate relationships. To traverse a network of m steps, the cost of the indexed approach, at O(m log n), dwarfs the cost of O(m) for an implementation that uses index-free adjacency. Native Graph StorageNeo4j stores graph data in a number of different store files. Each store file contains the data for a specific part of the graph (e.g., there are separate stores for nodes, relationships, labels, and properties).The division of storage responsibilities—particularly the separation of graph structure from property data—facilitates performant graph traversals, even though it means the user’s view of their graph and the actual records on disk are structurally dissimilar. Like most of the Neo4j store files, the node store is a fixed-size record store, where each record is nine bytes in length. Fixed-size records enable fast lookups for nodes in the store file. If we have a node with id 100, then we know its record begins 900 bytes into the file. Based on this format, the database can directly compute a record’s location, at cost O(1), rather than performing a search, which would be cost O(log n). TransactionsTransactions in Neo4j are semantically identical to traditional database transactions. Writes occur within a transaction context, with write locks being taken for consistency purposes on any nodes and relationships involved in the transaction. On successful completion of the transaction, changes are flushed to disk for durability, and the write locks released. These actions maintain the atomicity guarantees of the transaction. CORE API, TRAVERSAL FRAMEWORK, OR CYPHER?The Core API allows developers to fine-tune their queries so that they exhibit high affinity with the underlying graph. A well-written Core API query is often faster than any other approach. The downside is that such queries can be verbose, requiring considerable developer effort. Moreover, their high affinity with the underlying graph makes them tightly coupled to its structure. When the graph structure changes, they can often break. Cypher can be more tolerant of structural changes—things such as variable-length paths help mitigate variation and change. The Traversal Framework is both more loosely coupled than the Core API (because it allows the developer to declare informational goals), and less verbose, and as a result a query written using the Traversal Framework typically requires less developer effort than the equivalent written using the Core API. Because it is a general-purpose framework, however, the Traversal Framework tends to perform marginally less well than a well-written Core API query. exerciseNeo4j system design is different from mongodb on the consistency. MongoDB is eventually consistent, but neo4j is strong consistency and obey the ACID rules. Cypher: note that label and property are case sensitive but the clause is not case sensitive. RDF: Turtle Syntax Mannual grouping and grouping sets: need to know how to translate between them.grouping with rollup: rollup(c1,c2,c3) is equal to grouping sets((c1,c2,c3),(c1,c2),(c1),()). The ordering is important. difference between neo4j and RDF:Data Model:Neo4j: Neo4j is a graph database that uses the property graph data model. In this model, nodes represent entities, relationships represent connections between entities, and both nodes and relationships can have key-value pairs as properties.RDF: RDF is a data model for representing knowledge in the form of subject-predicate-object triples. Each triple represents a statement, and these triples can be used to build a graph of linked data. RDF is more abstract and can be implemented using various storage formats. Query Language:Neo4j: Neo4j uses the Cypher query language, which is specifically designed for querying graph databases. Cypher allows users to express graph patterns and relationships in a concise and readable manner.RDF: RDF data is typically queried using SPARQL (SPARQL Protocol and RDF Query Language). SPARQL is a query language for querying and manipulating RDF data, and it provides powerful capabilities for navigating the graph structure. Graph Structure:Neo4j: In Neo4j, the graph is explicit, with nodes, relationships, and properties forming a connected graph structure. The focus is on relationships between entities and their properties.RDF: RDF represents a graph, but the graph structure is more implicit. Resources are identified by URIs, and relationships are expressed through triples, allowing for the creation of a distributed and linked data web. Schema:Neo4j: Neo4j supports a flexible schema where nodes and relationships can have dynamic properties. While it provides some level of schema flexibility, users can define constraints and indexes to enforce certain structures.RDF: RDF is schema-agnostic, allowing for more dynamic and extensible data representation. Schemas can be defined using RDF vocabularies and ontologies, such as RDFS and OWL. Use Cases:Neo4j: Neo4j is often used for applications where relationships and graph structures are central, such as social networks, recommendation engines, and network analysis.RDF: RDF is commonly used in the context of the Semantic Web for representing and linking diverse data sources, allowing for interoperability and knowledge representation. referencesRobinson, I. et al. (2015). Graph Databases (2nd ed.)","link":"/2023/12/12/bigdata11/"},{"title":"bigdata - Cube Data","text":"On-Line Analytic Processing(OLAP)On-Line Analytic Processing generally involves highly complex queries that use one or more aggregations. OLAP and Data WarehousesData from many separate databases may be integrated into the warehouse. The warehouse is usually only updated overnight. Data warehouses play an important role in OLAP applications. First, the warehouse is necessary to organize and centralize data in a way that supports OLAP queries. Second, OLAP queries are usually complex and touching much of the data and take too much time to be executed in a transaction-processing system with high throughput requirements. A Multidimensional View of OLAP DataIn typical OLAP applications there is a central relation or collection of data called the fact table. Often, it helps to think of the objects in the fact table as arranged in a multidimensional space. Two broad directions that have been taken by specialized systems that support cube-structured data for OLAP: ROLAP and MOLAP. ROLAP, which is Relational OLAP. In this approach, data may be stored in relations with a specialized structure called a “star schema”. MOLAP, which is Multidimensional OLAP. A specialized structure “data cude” is used to hold the data, including its aggregates. Star SchemasA star schema consists of the schema for the fact table, which links to several other relations, called “dimension tables”. Slicing and DicingA choice of partition for each dimension “dices” the cude. The result is that the cude is divided into smaller cubes that represent groups of points whose statistics are aggregated by a query that performs this partitioning in its “group by” clause. Through the “where” clause, a query has the option of focusing on particular partitions alone one or more dimensions.(on a particular “slice” of the cube). Data CubesThe formal data cube precomputes all possible aggregates in a systematic way. The Cube OperatorGiven a fact table F, we can define an augmented table CUBE(F) that adds an additional value, denoted , to each dimension. The has the intuitive meaning “any,” and it represents aggregation along the dimension in which it appears. The Cube Operator in SQLSQL gives us a way to apply the cube operator within queries. If we add the term “WITH CUBE” to a group-by clause, then we get not only the tuple for each group, but also the tuples that represent aggregation along one or more of the dimensions along which we have grouped. These tuples appear in the result with NULL where we have used *. However, SalesRollup would not contain tuples such as","link":"/2023/12/12/bigdata12/"},{"title":"pai - Model-based Approximate Reinforcement Learning","text":"model-based reinforcement learningWe face three main challenges in model-based reinforcement learning. First, given a fixed model, we need to perform planning to decide on which actions to play. Second, we need to learn models f and r accurately and efficiently. Third, we need to effectively trade exploration and exploitation. PlanningDeterministic DynamicsTo begin with, let us assume that our dynamics model is deterministic and known. That is, given a state-action pair, we know the subsequent state.","link":"/2023/12/07/pai10/"},{"title":"pai - Model-free Approximate Reinforcement Learning","text":"Model-free Approximate Reinforcement LearningTabular Reinforcement Learning as OptimizationIn particular, in the tabular setting (i.e., over a discrete domain), we can parameterize the value function exactly by learning a separate parameter for each state. Now, we cannot compute this derivative because we cannot compute the expectation. Firstly, the expectation is over the true value function which is unknown to us. Secondly, the expectation is over the transition model which we are trying to avoid in model-free methods. To resolve the first issue, analogously to TD-learning, instead of learning the true value function vπ which is unknown, we learn the bootstrapping estimate Vπ. To resolve the first issue, analogously to TD-learning, instead of learning the true value function vπ which is unknown, we learn the bootstrapping estimate Vπ. we will use a Monte Carlo estimate using a single sample. Recall that this is only possible because the transitions are conditionally independent given the state-action pair. Therefore, TD-learning is essentially performing stochastic gradient descent using the TD-error as an unbiased gradient estimate.Stochastic gradient descent with a bootstrapping estimate is also called stochastic semi-gradient descent. Value Function ApproximationOur goal for large state-action spaces is to exploit the smoothness properties5 of the value function to condense the representation. HeuristicsThe vanilla stochastic semi-gradient descent is very slow.There are mainly two problems.As we are trying to learn an approximate value function that depends on the bootstrapping estimate, this means that the optimization target is “moving” between iterations. In practice, moving targets lead to stability issues. One such technique aiming to “stabilize” the optimization targets is called neural fitted Q-iteration or deep Q-networks (DQN). DQN updates the neural network used for the approximate bootstrapping estimate infrequently to maintain a constant optimization target across multiple episodes. One approach is to clone the neural network and maintain one changing neural network (“online network”) for the most recent estimate of the Q-function which is parameterized by θ, and one fixed neural network (“target network”) used as the target which is parameterized by θold and which is updated infrequently. This technique is known as experience replay. Another approach is Polyak averaging where the target network is gradually “nudged” by the neural network used to estimate the Q function. Now, observe that the estimates Q⋆ are noisy estimates of q⋆. The fact that the update rules can be affected by inaccuracies (i.e., noise in the estimates) of the learned Q-function is known as the “maximization bias”. Double DQN (DDQN) is an algorithm that addresses this maximization bias. Instead of picking the optimal action with respect to the old network, it picks the optimal action with respect to the new network. Intuitively, this change ensures that the evaluation of the target network is consistent with the updated Q-function, which makes it more difficult for the algorithm to be affected by noise. Policy ApproximationMethods that find an approximate policy are also called policy search methods or policy gradient methods. Policy gradient methods use randomized policies for encouraging exploration. Estimating Policy ValuesThe policy value function measures the expected discounted payoff of policy π. Reinforce Gradient In this context, however, we cannot apply the reparameterization trick. Fortunately, there is another way of estimating this gradient. When using a neural network for the parameterization of the policy π, we can use automatic differentiation to obtain unbiased gradient estimates. However, it turns out that the variance of these estimates is very large. Using so-called baselines can reduce the variance dramatically. The baseline essentially captures the expected or average value, providing a reference point. By subtracting this reference point, the updates become more focused on the deviations from the expected values, which can reduce the variance in these deviations. Typically, policy gradient methods are slow due to the large variance in the score gradient estimates. Because of this, they need to take small steps and require many rollouts of a Markov chain. Moreover, we cannot reuse data from previous rollouts, as policy gradient methods are fundamentally on-policy. Actor-Critic MethodsActor-Critic methods reduce the variance of policy gradient estimates by using ideas from value function approximation. They use function approximation both to approximate value functions and to approximate policies. Advantage Function Intuitively, the advantage function is a shifted version of the state-action function q that is relative to 0. It turns out that using this quantity instead, has numerical advantages. Policy Gradient Theorem Intuitively, ρθ(x) measures how often we visit state x when following policy πθ. It can be thought of as a “discounted frequency”. Importantly, ρθ is not a probability distribution, as it is not normalized to integrate to 1. Instead, ρθ is what is often called a finite measure. Therefore, eq. (12.57) is not a real expectation! On-policy Actor-Critics OACDue to the use of TD-learning for learning the critic, this algorithm is fundamentally on-policy. A2C that the Q-function is an absolute quantity, whereas the advantage function is a relative quantity, where the sign is informative for the gradient direction. Intuitively, an absolute value is harder to estimate than the sign. Actor-Critic methods are therefore often implemented with respect to the advantage function rather than the Q-function. GAE/GAAETaking a step back, observe that the policy gradient methods such as REINFORCE generally have high variance in their gradient estimates. However, due to using Monte Carlo estimates of Gt, the gradient estimates are unbiased. In contrast, using a bootstrapped Q-function to obtain gradient estimates yields estimates with a smaller variance, but those estimates are biased. We are therefore faced with a bias-variance tradeoff. A natural approach is therefore to blend both gradient estimates to allow for effectively trading bias and variance. This leads to algorithms such as generalized advantage estimation (GAE/GAAE). Improving sample efficiency(TRPO/PPO)Actor-Critic methods generally suffer from low sample efficiency. One well-known variant that slightly improves the sample efficiency is trust-region policy optimization (TRPO). Intuitively, taking the expectation with respect to the previous policy πθk , means that we can reuse data from rollouts within the same iteration. TRPO allows reusing past data as long as it can still be “trusted”. This makes TRPO “somewhat” off-policy. Fundamentally, though, TRPO is still an on-policy method. Proximal policy optimization (PPO) is a heuristic variant of TRPO that often works well in practice. Off-policy Actor-CriticsThese algorithms use the reparameterization gradient estimates, instead of score gradient estimators. DDPGAs our method is off-policy, a simple idea in continuous action spaces is to add Gaussian noise to the action selected by πθ — also known as Gaussian noise “dithering”. This corresponds to an algorithm called deep deterministic policy gradients.This algorithm is essentially equivalent to Q-learning with function approximation (e.g., DQN), with the only exception that we replace the maximization over actions with the learned policy πθ. Twin delayed DDPG (TD3) is an extension of DDPG that uses two separate critic networks for predicting the maximum action and evaluating the policy. This addresses the maximization bias akin to Double-DQN. TD3 also applies delayed updates to the actor network, which increases stability. Off-Policy Actor Critics with Randomized Policies The algorithm that uses eq. (12.81) to obtain gradients for the critic and reparameterization gradients for the actor is called stochastic value gradients (SVG). Off-policy Actor-Critics with Entropy RegularizationIn practice, algorithms like SVG often do not explore enough. A key issue with relying on randomized policies for exploration is that they might collapse to deterministic policies. A simple trick that encourages a little bit of extra exploration is to regularize the randomized policies “away” from putting all mass on a single action. This approach is known as entropy regularization and it leads to an analogue of Markov decision processes called entropy-regularized Markov decision process, where suitably defined regularized state-action value functions (so-called soft value functions) are used. soft actor critic(SAC) referenceshttps://towardsdatascience.com/soft-actor-critic-demystified-b8427df61665https://github.com/tsmatz/reinforcement-learning-tutorials/blob/master/06-sac.ipynbhttps://spinningup.openai.com/en/latest/algorithms/sac.htmlhttps://towardsdatascience.com/policy-gradients-in-a-nutshell-8b72f9743c5dhttps://lilianweng.github.io/posts/2018-04-08-policy-gradient/","link":"/2023/12/07/pai9/"},{"title":"bigdata - review notes","text":"data cubesslidesOLTP VS OLAP:record keeping vs decision supportread-intensive vs write-intensivedetailed individual records vs summarized dataLots of transactions on small portions of data vs Large portionsof the data.fully interactive vs slow interactive.ETL: Extract Transform Load slicing and dicing,PivotingSlicing involves selecting a specific “slice” or subset of the data cube by fixing one or more dimensions at a particular value. Dicing involves creating a subcube by selecting specific values for two or more dimensions. It’s like slicing, but you are selecting a rectangular subset of the cube, cutting through more than one dimension. Pivoting is another operation often used alongside slicing and dicing. It involves rotating the data cube to view it from a different perspective by swapping the positions of dimensions. Aggregation and roll-upThese operations allow users to summarize and view data at different levels of granularity within a multidimensional dataset. Roll-up is a specific form of aggregation that involves moving from a lower level of detail to a higher level by collapsing one or more dimensions(moving up the hierarchy from a finer level of granularity (monthly) to a coarser level (quarterly)). Two flavors of OLAP: ROLAP and MOLAPROLAP (Relational Online Analytical Processing) and MOLAP (Multidimensional Online Analytical Processing) are two different approaches to organizing and processing data in the context of Online Analytical Processing (OLAP) systems. In summary, the main difference between ROLAP and MOLAP lies in how they store and organize data. ROLAP uses relational databases to store multidimensional data in tables, while MOLAP uses specialized databases with a cube structure optimized for efficient multidimensional analysis. fact table and Satellite tableFact tables are surrounded by dimension tables in a star schema. They usually have foreign key relationships with dimension tables, linking to various dimensions that provide context to the measures. Satellite tables are used to store additional details about dimension members that are not part of the core structure of the dimension table. Satellite tables typically have a foreign key relationship with the main dimension table, linking to the primary key of the dimension. They can be joined to the main dimension table to enrich the analysis with additional context. star schema and snow-flake schema In a star schema, there is a central fact table surrounded by dimension tables. The fact table contains quantitative data (measures), and the dimension tables store descriptive data related to the measures. A snowflake schema is an extension of the star schema, where dimension tables are normalized into multiple related tables. This normalization leads to a structure resembling a snowflake when visualized, as opposed to the star-like structure of a star schema. SQL querying tables and MDX querying Cubes MDX stands for Multi-DimensionaleXpressions. “Roll-up” and “drill down” Roll-up involves summarizing data at a higher level of aggregation or moving from a lower level of detail to a higher level. It’s a way of viewing data at a coarser level in the hierarchy. Drill down is the opposite of roll-up. It involves accessing more detailed information or moving from a higher level of aggregation to a lower, more detailed level. GROUPING SETS, CUBE, ROLLUP GROUPING SETS is a SQL feature that allows you to specify multiple grouping sets within a single query. The CUBE operation is an extension of GROUP BY that generates all possible combinations of grouped elements. ROLLUP is another extension of GROUP BY that generates subtotals and grand totals for a specified set of columns. It creates a hierarchy of grouping levels, calculating subtotals as it rolls up the hierarchy. Note that the column order in rollup is important. bookexerciseconcepts of fact tables;pivot table;SQL query with grouping set, cube… Graph DatabasessildesIndex-free adjacencyWith index-free adjacency, graph databases are designed in such a way that relationships between nodes are directly accessible without the need for an index structure. Instead of traversing indexes to find related nodes, the relationships are stored alongside the nodes, allowing for faster and more efficient traversal.Each node maintains direct pointers or references to its neighboring nodes, making it possible to navigate from one node to another without the need for an explicit index lookup. This design principle is particularly beneficial for scenarios where traversing relationships in a graph is a common and performance-critical operation. Property Graph and Triple stores (RDF)Labeled property graphs: ingredients: Nodes, Edges,Properties,Labels.A property graph is a graph data model that consists of nodes, edges, and properties.RDF is a standard for representing data about resources in the form of subject-predicate-object triples. CypherCypher is a query language commonly used with property graph databases like Neo4j. SPARQLSPARQL is a query language commonly used with RDF triple stores. Neo4jData replication;sharding:neo4j fabric;Caching and pages; RDFIRI (=URI for all practical purposes);RDF formats:RDF/XML;Turtle;JSON-LD;RDFa;N-Triples. bookLabeled property graph model vs. triple storeslabeled property graphs enhance mathematical graphs with extra ingredients: properties, and labels.Labels are some sort of “tags”, in the form of a string, that can be attached to a node or an edge.Each node and each edge can be associated with a map from strings to values, which represents its properties. Triple stores are a different and simpler model. It views the graph as nodes and edges that all have labels, but without any properties.The graph is then represented as a list of edges, where each edge is a triple with the label of the origin node (called the subject), the label of the edge (called the property), and the label of the destination node (called the object). Labels can be:URIs,Literals, that is, atomic values,Literals are only allowed as objects. Absent, in which case the node is called a blank node. Blank nodes are only allowed as subjects or objects, but not as properties. cypherOther graph databases have other means of querying data. Many, including Neo4j, support the RDF query language SPARQL and the imperative, path-based query language Gremlin. Cypher enables a user (or an application acting on behalf of a user) to ask the database to find data that matches a specific pattern. Colloquially, we ask the database to “find things like this.” And the way we describe what “things like this” look like is to draw them, using ASCII art. The MATCH clause is at the heart of most Cypher queries. native graph processingTo understand why native graph processing is so much more efficient than graphs based on heavy indexing, consider the following. Depending on the implementation, index lookups could be O(log n) in algorithmic complexity versus O(1) for looking up immediate relationships. To traverse a network of m steps, the cost of the indexed approach, at O(m log n), dwarfs the cost of O(m) for an implementation that uses index-free adjacency. With index-free adjacency, bidirectional joins are effectively precomputed and stored in the database as relationships storesNeo4j stores graph data in a number of different store files. Each store file contains the data for a specific part of the graph (e.g., there are separate stores for nodes, relationships, labels, and properties). Like most of the Neo4j store files, the node store is a fixed-size record store, where each record is nine bytes in length. Fixed-size records enable fast lookups for nodes in the store file. CYPHER,Traverser API and Core APINeo4j’s Core API is an imperative Java API that exposes the graph primitives of nodes, relationships, properties, and labels to the user. When used for reads, the API is lazily evaluated, meaning that relationships are only traversed as and when the calling code demands the next node. The Traversal Framework is a declarative Java API. It enables the user to specify a set of constraints that limit the parts of the graph the traversal is allowed to visit. Cypher can be more tolerant of structural changes—things such as variable-length paths help mitigate variation and change. however, the Traversal Framework tends to perform marginally less well than a well-written Core API query. Choosing between the Core API and the Traversal Framework is a matter of deciding whether the higher abstraction/lower coupling of the Traversal Framework is sufficient, or whether the close-to-the-metal/higher coupling of the Core API is in fact necessary for implementing an algorithm correctly and in accordance with our performance requirements. exerciseQuerying treesslidesJSONiqData independence withheterogeneous, denormalized data.JSONiq Data Model (JDM): Sequences of Items; Declarative languages,Functional languages and Set-based languagesDeclarative languages focus on describing what the program should accomplish, rather than specifying how to achieve it. Functional languages treat computation as the evaluation of mathematical functions and avoid changing state or mutable data.Haskell, Lisp, and Erlang are functional programming languages.Parts of JavaScript and Python support functional programming paradigms. Set-based languages are a subset of declarative languages that focus on manipulating sets of data.It’s worth noting that languages can often belong to more than one category. For example, SQL is both declarative and set-based, and functional programming concepts can be integrated into languages that are not purely functional. FLWOR clausesquery,Abstract Syntax Tree, Expression Tree, Iterator TreeMaterialized execution,Streamed execution,Parallel executionMaterialized execution takes lots of space. Streamed execution takes lots of time.Parallel execution can take lots of machines.Execution modes determined statically for every expression and clause. RumbleTraditional RDBMS/warehouses vs. data lakesbookexerciseDocument StoresslidesJSON and XMLSQL and NoSQL:NoSQL: validation after the data was populated. CRUDCreate,Read,Update,Delete MongoDB projecting away: not selecting a field. hash indices,Tree indices (B+-tree),compund index Limitations of hash indices: No support for range queries,Hash function not perfect in real life, Space requirements for collision avoidance. B+-tree: All leaves at same depth, All non-leaf nodes have between 3 and 5 children,But it’s fine if the root has less. bookexercisePerformance at Large Scalesslidessources of bottleneckMemory, CPU, Disk I/O, Network I/O.Sweet Spot for MapReduce and Spark: Disk I/O. Latency, Throughput,Response timeLatency: When do I start receiving data.Throughput:”How fast can we transmit data.Response time=Latency + Transfer. speedup,Amdahl’s law,Gustafson’s lawspeedup = latency(old)/latency(new).Gustafson’s law:Constant computing power.Amdahl’s law: Constant problem size. Scaling out,Scaling uptail latency, SLAbookexerciseMassive Parallel Processing I(MapReduce)slidescombine and reducemap task and reduce task, map slot and reduce slot, map phase amd reduce phaseno combine task and combine slot.map slot = sequential map task; map task = sequential split map; split(mapreduce) and block(HDFS)1 split = 1 map tasksplit(mapreduce)!= block(HDFS) bookmapreduce modelIn MapReduce, the input data, intermediate data, and output data are all made of a large collection of key-value pairs (with the keys not necessarily unique, and not necessarily sorted by key). MapReduce architectureIn the original version of MapReduce, the main node is called JobTracker, and the worker nodes are called TaskTrackers.In fact, the JobTracker typically runs on the same machine as the NameNode (and HMaster) and the TaskTrackers on the same machines as the DataNodes (and RegionServers). This is called “bring the query to the data.” Note that shuffling can start before the map phase is over, but the reduce phase can only start after the map phase is over. combineCombining happens during the map phase.In fact, in most of the cases, the combine function will be identical to the reduce function, which is generally possible if the intermediate key-value pairs have the same type as the output key-value pairs, and the reduce function is both associative and commutative. MapReduce programming APIIn Java, the user needs to define a so-called Mapper class that contains the map function, and a Reducer class that contains the reduce function.A map function takes in particular a key and a value. Note that it outputs key-value pairs via the call of the write method on the context, rather than with a return statement.A reduce function takes in particular a key and a list of values. function,task,slot,phaseA map function is a mathematical, or programmed, function that takes one input key-value pair and returns zero, one or more intermediate key-value pairs.Then, a map task is an assignment (or “homework”, or “TODO”) that consists in a (sequential) series of calls of the map function on a subset of the input. There is no such thing as a combine task. Calls of the combine function are not planned as a task, but is called ad-hoc during flushing and compaction. The map tasks are processed thanks to compute and memory resources (CPU and RAM). These resources are called map slots. One map slot corresponds to one CPU core and some allocated memory. Each map slot then processes one map task at a time, sequentially. The map phase thus consists of several map slots processing map tasks in parallel. short-circuiting(split and block)This is because the DataNode process of HDFS and the TaskTracker process of MapReduce are on the same machine. Thus, getting a replica of the block containing the data necessary to the processing of the task is as simple as a local read. This is called short-circuiting.split(logical level)!=HDFS block(physical level). exerciseResource managementbookYARNYARN means Yet Another Resource manager. It was introduced as an additional layer that specifically handles the management of CPU and memory resources in the cluster.YARN, unsurprisingly, is based on a centralized architecture in which the coordinator node is called the ResourceManager, and the worker nodes are called NodeManagers. NodeManagers furthermore provide slots (equipped with exclusively allocated CPU and memory) known as containers. YARN provides generic support for allocating resources to any application and is application-agnostic. When the user launches a new application, the ResourceManager assigns one of the container to act as the ApplicationMaster which will take care of running the application. Version 2 of MapReduce works on top of YARN by leaving the job lifecycle management to an ApplicationMaster.It is important to understand that, unlike the JobTracker, the ResourceManager does not monitor tasks, and will not restart slots upon failure. This job is left to the ApplicationMasters. Scheduling strategiesFIFO scheduling,Capacity scheduling,Fair scheduling Dominant Resource Fairness algorithm.The two (or more) dimensions are projected again to a single dimension by looking at the dominant resource for each user. Massive Parallel Processing II (Spark)slidesYARNResourceManager + NodeManager; ResourceManager allocates one NodeManager as application master. Application Master communicates with containers. ResourceManagerDoes not monitor tasks and Does not restart upon failure. Fault tolerance is on the application master. sparkFull-DAG query processing.distributed acyclic graph (DAG)RDD: Resilient Distributed Dataset. RDDlazy evaluation:Lazy evaluation means that the execution of transformations on RDDs is deferred until an action is triggered. Instead of immediately executing the transformations, Spark keeps track of the sequence of transformations in the form of a logical execution plan. The actual computation is only performed when an action is called. A narrow dependency (also known as a narrow transformation) occurs when each partition of the parent RDD contributes to at most one partition of the child RDD. In other words, the number of partitions remains the same before and after the transformation, and each partition of the child RDD depends on a one-to-one relationship with partitions of the parent RDD. A wide dependency (also known as a wide transformation) occurs when each partition of the parent RDD contributes to multiple partitions of the child RDD. This typically involves operations that require data shuffling or redistribution, such as groupByKey or reduceByKey. Data FramesbookResilient distributed datasets(RDDs)Resilient means that they remain in memory or on disk on a “best effort” basis, and can be recomputed if need be. Distributed means that, just like the collections of key-value pairs in MapReduce, they are partitioned and spread over multiple machines.A major difference with MapReduce, though, is that RDDs need not be collections of pairs. Since a key-value pair is a particular example of possible value, RDDs are a generalization of the MapReduce model for input, intermediate input and output. The RDD lifecycleCreation,Transformation(Mapping or reducing, in this model, become two very specific cases of transformations.),Action.transformations:unary transformations: The filter transformation,The map transformation,The flatMap transformation,The distinct transformation,The sample transformationBinary transformations: taking the union of two RDDs, take the intersection, take the subtraction.Pair transformations: Spark has transformations specifically tailored for RDDs of key-value pairs: The key transformation,The values transformation,The reduceByKey transformation, The groupByKey transformation,The sortByKey transformation,The mapValues transformation,The join transformation,The subtractByKey transformation.Actions: The collect action,The count action,The countByValue action,The take action,The top action,The takeSample action,The reduce action,saveAsTextFile action,saveAsObjectFile action. Note that Spark, at least in its RDD API, is not aware of any particular format or syntax, i.e., it is up to the user to parse and serialize values to the appropriate text or bytes. Actions on Pair RDDs: There are actions specifically working on RDDs of key-value pairs:The countByKey action, The lookup action, Lazy evaluationIt is only with an action that the entire computation pipeline is put into motion, leading to the computation of all the necessary intermediate RDDs all the way down to the final output corresponding to the action. Physical architectureThere are two kinds of transformations: narrow-dependency transformations and wide-dependency transformations. Such a chain of narrow-dependency transformations executed efficiently as a single set of tasks is called a stage, which would correspond to what is called a phase in MapReduce. OptimizationsPinning RDDs, Pre-partitioning. DataFrames in SparkA DataFrame can be seen as a specific kind of RDD: it is an RDD of rows (equivalently: tuples, records) that has relational integrity, domain integrity, but not necessarily (as the name “Row” would otherwise fallaciously suggest) atomic integrity. Note that Spark automatically infers the schema from discovering the JSON Lines file, which adds a static performance overhead that does not exist for raw RDDs: there is no free lunch. Unlike the RDD transformation API, there is no guarantee that the execution will happen as written, as the optimizer is free to reorganize the actual computations. Spark SQLboth GROUP BY and ORDER BY will trigger a shuffle in the system. The SORT BY clause can sort rows within each partition, but not across partitions, i.e., does not induce any shuffling. The DISTRIBUTE BY clause forces a repartition by putting all rows with the same value (for the specified field(s)) into the same new partition. use both SORT and DISTRIBUTE = the use of another clause, CLUSTER BY. A word of warning must be given on SORT, DISTRIBUTE and CLUSTER clauses: they are, in fact, a breach of data independence, because they expose partitions. explode() and lateral view: Lateral views are more powerful and generic than just an explode() because they give more control, and they can also be used to go down several levels of nesting. bookexerciseSpark’s RDDs are by default recomputed each time you run an action on them. Please note that both persist() and cache() are lazy operations themselves. The caching operation will, in fact, only take place when the first action is called. With successive action calls, the cached RDD will be used. introductionbookthree Vs: Volume, Variety, Velocity.four more shapes: trees, unstructured, cubes,graphs.three factors: Capacity,Throughput, Latency. partial function and functionA function is a strict mapping where every element in the domain is mapped to a unique element in the codomain.A partial function is a mapping where not every element in the domain necessarily has a defined value in the codomain.For a table, we need to throw in three additional constraints: relational integrity, domain integrity and atomic integrity. lessons learned from the pastbooknatural join, theta join, and outer join,Semi-outer joinA natural join is a type of join that combines rows from two tables based on columns with the same name and data type. The columns used for the join condition are not explicitly specified; instead, the database system automatically identifies the matching columns. A theta join is a generalization of the natural join, where the join condition is explicitly specified using a comparison operator. Normal formsThe first normal form was already covered earlier: it is in fact atomic integrity.The second normal form takes it to the next level: it requires that each column in a record contains information on the entire record. The third normal form additionally forbids functional dependencies on anything else than the primary key. SQLSQL is a declarative language, which means that the user specifies what they want, and not how to compute it: it is up to the underlying system to figure out how to best execute the query. It is also a set-based language, in the sense that it manipulates sets of records at a time, rather than single values as is common in other languages. It is also, to some limited extent, a functional language in the sense that it contains expressions that can nest in each other (nested queries). ACIDThere are four main properties (often called ACID):Atomicity,Consistency,Isolation,Durability. exerciseSQL1NF,2NF,3NF,BCNF;intersection; Storing databookCAP(Atomic) Consistency, Availability, Partition tolerance. document storesbookDocument stores provide a native database management system for semi-structured data. A document store typically specializes in either JSON or XML data, even though some companies (e.g., MarkLogic) offer support for both. It is important to understand that document stores are optimized for the typical use cases of many records of small to medium sizes. Typically, a collection can have millions or billions of documents, while each single document weighs no more than 16 MB (or a size in a similar magnitude). MongoDBIn MongoDB, the format is a binary version of JSON called BSON. The API of MongoDB, like many document stores, is based on the CRUD paradigm. CRUD means Create, Read, Update, Delete. MongoDB automatically adds to every inserted document a special field called “ id” and associated with a value called an Object ID and with a type of its own. hash indices and tree indicesHash indices are used to optimize point queries and more generally query that select on a specific value of a field. Secondary indicesBy default, MongoDB always builds a tree index for the id field. Users can request to build hash and tree indices for more fields. These indices are called secondary indices. exerciseBy default, MongoDB creates the _id index, which is an ascending unique index on the _id field, for all collections when the collection is created. You cannot remove the index on the _id field. Querying denormalized databookFeatures of a query languageFirst, it is declarative. This means that the users do not focus on how the query is computed, but on what it should return. Second, it is functional. This means that the query language is made of composable expressions that nest with each other, like a Lego game. Finally, it is set-based, in the sense that the values taken and returned by expressions are not only single values (scalars), but are large sequences of items (in the case of SQL, an item is a row). JSONiqIt is possible to filter any sequence with a predicate, where in the predicate refers to the current item being tested. example: \"json-doc(\"file.json\").o[].a.b[][.c = 3]”To access the n-th member of an array, you can use double-squarebrackets: “json-doc(“file.json”).o[[2]].a”. Do not confuse sequence positions (single square brackets) with array positions (double square brackets)! The empty sequence enjoys special treatment: if one of the sides (or both) is the empty sequence, then the arithmetic expression returns an empty sequence (no error). Note that unlike SQL, JSONiq logic expressions are two-valued and return either true or false. general comparisonuniversal and existential quantification: every and some;JSONiq has a shortcut for existential quantification on value comparisons. This is called general comparison. FLWOR expressionsOne of the most important and powerful features of JSONiq is the FLWOR expression. It corresponds to SELECT-FROM-WHERE queries in SQL. exerciseAccessing a JSON dataset can be done in two ways depending on the exact format:If this is a file that contains a single JSON object spread over multiple lines, use json-doc(URL).If this is a file that contains one JSON object per line (JSON Lines), use json-file(URL). HDFSbookHDFS data modelHDFS does not follow a key-value model: instead, an HDFS cluster organizes its files as a hierarchy, called the file namespace. Files are thus organized in directories, similar to a local file system. key-value model vs file hierarchyThe key-value model and file hierarchy are two different approaches to organizing and accessing data within storage systems. While the key-value model excels in flexibility and quick data access, file hierarchy provides a structured and predictable organization suitable for many traditional storage use cases. object storage vs block storageOrganizes data as objects, each containing both data and metadata. Objects are stored in a flat address space.Organizes data as fixed-size blocks, typically within storage volumes.Requires a file system to manage and organize data into files and directories. object storage and key-value modelObject storage is a data storage architecture that manages data as objects, each containing both data and metadata. Objects are stored in a flat address space without the hierarchy found in traditional file systems. In the key-value model, data is organized as pairs of keys and values. Each key uniquely identifies a value, and the system allows for the efficient retrieval and storage of data based on these key-value pairs.Object storage systems often use a key-value model internally to manage objects. Each object has a unique identifier (key), and the associated data and metadata form the corresponding value. physical architectureIn the case of HDFS, the central node is called the NameNode and the other nodes are called the DataNodes. In fact, more precisely, the NameNode and DataNodes are processes running on these nodes.The NameNode stores in particular three things:the file namespace,a mapping from each file to the list of its blocks, a mapping from each block, represented with its 64-bit identifier, to the locations of its replicas. exerciseobject storage vs block storagesyntaxbookjsonJSON stands for JavaScript Object Notation.JSON is made of exactly six building blocks: strings, numbers, Booleans, null, objects, and arrays.in JSON, escaping is done with backslash characters (\\).The way a number appears in syntax is called a lexical representation, or a literal.JSON places a few restrictions: a leading + is not allowed. Also, a leading 0 is not allowed except if the integer part is exactly 0 (in which case it is even mandatory).Objects are simply maps from strings to values. The keys of an object must be strings.The JSON standard recommends for keys to be unique within an object. unicodeUnicode is a standard that assigns a numeric code (called a code point) to each character in order to catalogue them across all languages of the world, even including emojis. The code point must be indicated in base 16. XMLXML stands for eXtensible Markup Language.XML’s most important building blocks are elements, attributes, text and comments.Unlike JSON keys, element names can repeat at will.At the top-level, a well-formed XML document must have exactly one element.Attributes appear in any opening elements tag and are basically keyvalue pairs. Values can be either double-quoted or single-quoted. The key is never quoted, and it is not allowed to have unquoted values. Within the same opening tag, there cannot be duplicate keys. Attributes can never appear in a closing tag. It is not allowed to create attributes that start with XML or xml, or any case combination. because this is reserved for namespace declarations.a single comment alone is not well-formed XML (remember: we need exactly one top-level element).XML documents can be identified as such with an optional text declaration containing a version number and an encoding.Another tag that might appear right below, or instead of, the text declaration is the doctype declaration. It must then repeat the name of the top-level element.Remember that in JSON, it is possible to escape sequences with a backslash character. In XML, this is done with an ampersand (&amp;) character.Escape sequences can be used anywhere in text, and in attribute values.there are a few places where they are mandatory: In text, &amp; and &lt; MUST be escaped. In double-quoted attribute values, ”, &amp; and &lt; MUST be escaped. In single-quoted attribute values, ’, &amp; and &lt; MUST be escaped. Namespaces in XMLA namespace is identified with a URI.The triplet (namespace, prefix, localname) is called a QName (for “qualified name”).For the purpose of the comparisons of two QNames (and thus of documents), the prefix is ignored: only the local name and the namespace are compared.First, unprefixed attributes are not sensitive to default namespaces: unlike elements, the namespace of an unprefixed attribute is always absent even if there is a default namespace. exercisexml namesRemember:Element names are case-sensitive.Element names must start with a letter or underscore.Element names cannot start with the letters xml (or XML, or Xml, etc).Element names can contain letters, digits, hyphens, underscores, and periods.Element names cannot contain spaces. JSON Key names The only restriction the JSON syntax imposes on the key names is that “ and \\ must be escaped. Wide column storesbookHDFS VS Wide column storesThe problem with HDFS is its latency: HDFS works well with very large files (at least hundreds of MBs so that blocks even start becoming useful), but will have performance issues if accessing millions of small XML or JSON files. Wide column stores were invented to provide more control over performance and in particular, in order to achieve high-throughput and low latency for objects ranging from a few bytes to about 10 MB, which are too big and numerous to be efficiently stored as so-called clobs (character large objects) or blobs (binary large objects) in a relational database system, but also too small and numerous to be efficiently accessed in a distributed file system. object storage VS Wide column storesa wide column store has additional benefits: a wide column store will be more tightly integrated with the parallel data processing systems.wide column stores have a richer logical model than the simple key-value model behind object storage; wide column stores also handle very small values (bytes and kBs) well thanks to batch processing. HBaseFrom an abstract perspective, HBase can be seen as an enhanced keyvalue store, in the sense that:a key is compound and involves a row, a column and a version; keys are sortable; values can be larger (clobs, blobs), up to around 10 MB. On the logical level, the data is organized in a tabular fashion: as a collection of rows. Each row is identified with a row ID. Row IDs can be compared, and the rows are logically sorted by row ID. Column qualifiers are arrays of bytes (rather than strings), and as for row IDs, there is a library to easily create column qualifiers from primitive values. HBase supports four kinds of low-level queries: get, put, scan and delete. Unlike a traditional key-value store, HBase also supports querying ranges of row IDs and ranges of timestamps. HBase offers a locking mechanism at the row level, meaning that different rows can be modified concurrently, but the cells in the same row cannot: only one user at a time can modify any given row. A table in HBase is physically partitioned in two ways: on the rows and on the columns. The rows are split in consecutive regions. Each region is identified by a lower and an upper row key, the lower row key being included and the upper row key excluded. A partition is called a store and corresponds to the intersection of a region and of a column family. All the cells within a store are eventually persisted on HDFS, in files that we will call HFiles. An HFile is, in fact, nothing else than a (boring) flat list of KeyValues, one per cell. What is important is that, in an HFile, all these KeyValues are sorted by key in increasing order. HDFS block and HBlockHBase uses index structures to quickly skip to the position of the HBase block which may hold the requested key. Note HBase block is not to be confused with HDFS block and the underlying file system block.By default, each HBase block is 64KB (configurable) in size and always contains whole key-value pairs, so, if a block needs more than 64KB to avoid splitting a key-value pair, it will just grow. Log-structured merge treesUpon flushing, all cells are written sequentially to a new HFile in ascending key order, HBlock by HBlock, concurrently building the index structure. In fact, sorting is not done in the last minute when flushing. Rather, what happens is that when cells are added to memory, they are added inside a data structure that maintains them in sorted order (such as tree maps) and then flushing is a linear traversal of the tree. Bloom filtersHBase has a mechanism to avoid looking for cells in every HFile. This mechanism is called a Bloom filter. It is basically a black box that can tell with absolute certainty that a certain key does not belong to an HFile, while it only predicts with good probability (albeit not certain) that it does belong to it. exerciseBloom filtersBloom filters are a data structure used to speed up queries, useful in the case in which it’s likely that the value we are looking doesn’t exist in the collection we are querying. Their main component is a bit array with all values initially set to 0. When a new element is inserted in the collection, its value is first run through a certain number of (fixed) hash functions, and the locations in the bit array corresponding to the outputs of these functions are set to 1. This means that when we query for a certain value, if the value has previously been inserted in the collection then all the locations corresponding to the hash function outputs will certainly already have been set to 1. On the contrary, if the element hasn’t been previously inserted, then the locations may or may not have already been set to 1 by other elements. Then, if prior to accessing the collection we run our queried value through the hash functions, check the locations corresponding to the outputs, and find any of them to be 0, we are guaranteed that the element is not present in the collection (No False Negatives), and we don’t have to waste time looking. If the corresponding locations are all set to 1, the element may or may not be present in the collection (possibility of False Positives), but in the worst case we’re just wasting time. As you have seen in the task above, HBase has to check all HFiles, along with the MemStore, when looking for a particular key. As an optimisation, Bloom filters are used to avoid checking an HFile if possible. Before looking inside a particular HFile, HBase first checks the requested key against the Bloom filter associated with that HFile. If it says that the key does not exist, the file is not read. a Bloom filter can produce false positive outcomes. Luckily, it never produces false negative outcomes. Log-structured merge-tree (LSM tree) (optional)As opposed to B+-tree which has a time complexity of O(log n) when inserting new elements, n being the total number of elements in the tree, LSM tree has O(1) for inserting, which is a constant cost. Data models and validationbookA data model is an abstract view over the data that hides the way it is stored physically. For example, a CSV file should be abstracted logically as a table. The JSON Information Setit is possible to take a tree and output it back to JSON syntax. This is called serialization. The XML Information SetA fundamental difference between JSON trees and XML trees is that for JSON, the labels (object keys) are on the edges connecting an object information item to each one of its children information items. In XML, the labels (these would be element and attribute names) are on the nodes (information items) directly. Item typesAlso, all atomic types have in common that they have a logical value space and a lexical value space. Atomic types can be in a subtype relationship: a type is a subtype of another type if its logical value space is a subset of the latter.However, in modern databases, it is customary to support unbounded integers.Decimals correspond to real numbers that can be written as a finite sequence of digits in base 10, with an optional decimal period.Support for the entire decimal value space can be costly in performance. In order to address this issue, a floating-point standard (IEEE 754) was invented and is still very popular today.Timestamp values are typically stored as longs (64-bit integers) expressing the number of milliseconds elapsed since January 1, 1970 by convention.XML Schema, JSound and JSONiq follow the ISO 8601 standard.The lexical representation of duration can vary, but there is a standard defined by ISO 8601 as well, starting with a P and prefixing sub-day parts with a T.Maps (not be confused with records, which are similar) are maps from any atomic value to any value, i.e., generalize objects to keys that are not necessarily strings (e.g., numbers, dates, etc). However, unlike records, the type of the values must be the same for all keys. JSound and JSON SchemaJSound is a schema language that was designed to be simple for 80% of the cases, making it particularly suitable in a teaching environment. It is independent of any programming language.JSON Schema is another technology for validating JSON documents.The type system of JSON Schema is thus less rich than that of JSound, but extra checks can be done with so-called formats, which include date, time, duration, email, and so on including generic regular expressions.It is possible to require the presence of a key by adding an exclamation mark in JSound. in JSON Schema, which uses a “required” property associated with the list of required keys to express the same.In the JSound compact syntax, extra keys are forbidden. Unlike JSound, in JSON Schema, extra properties are allowed by default. JSON Schema then allows to forbid extra properties with the “additionalProperties” property.There are a few more features available in the compact JSound syntax (not in JSON Schema) with the special characters @, ? and =. The question mark (?) allows for null values (which are not the same as absent values). The arobase (@) indicates that one or more fields are primary keys for a list of objects that are members of the same array. The equal sign (=) is used to indicate a default value that is automatically populated if the value is absent.Note that some values are quoted, which does not matter for validation: validation only checks whether lexical values are part of the type’s lexical space. Accepting any values in JSound can be done with the type “item”, which contains all possible values. In JSON Schema, in order to declare a field to accept any values, you can use either true or an empty object in lieu of the type. JSON Schema additionally allows to use false to forbid a field. In JSON Schema, it is also possible to combine validation checks with Boolean combinations using “anyOf”.JSound schema allows defining unions of types with the vertical bar inside type strings. In JSON Schema only (not in JSound), it is also possible to do a conjunction (logical and) with “allOf” as well as exclusive with “oneOf” as well as negation with “not”. XML Schemaall elements in an XML Schema are in a namespace, the XML Schema namespace. The namespace is prescribed by the XML Schema standard and must be this one. dataframeThere is a particular subclass of semi-structured datasets that are very interesting: valid datasets, which are collections of JSON objects valid against a common schema, with some requirements on the considered schemas. The datasets belonging to this particular subclass are called data frames, or dataframes.relational tables are data frames, while data frames are not necessarily relational tables: data frames can be (and are often) nested, but they are still relatively homogeneous to some extent.Thus, Data frames are a generalization of (normalized) relational tables allowing for (organized and structured) nestedness. data formatIn fact, if the data is structured as a (valid) data frame, then there are many, many different formats that it can be stored in, and in a way that is much more efficient than JSON. These formats are highly optimized and typically stored in binary form, for example Parquet, Avro, Root, Google’s protocol buffers, etc. Why is it possible to store the data more efficiently when it is valid and data-frame-friendly? One important reason is that the schema can be stored as a header in the binary format, and the data can be stored without repeating the fields in every record (as is done in textual JSON). exerciseDremelDremel is a query system developed at Google for deriving data stored in a nested data format such as XML, JSON, or Google Protocol Buffers into column storage, where it can be analyzed faster. paper notesDynamoCAP: AP. preference list and coordinatorA node handling a read or write operation is known as the coordinator. Typically, this is the first among the top N nodes in the preference list. quorum-like systemTo maintain consistency among its replicas, Dynamo uses a consistency protocol similar to those used in quorum systems. This protocol has two key configurable values: R and W. R is the minimum number of nodes that must participate in a successful read operation. W is the minimum number of nodes that must participate in a successful write operation. Setting R and W such that R + W &gt; N yields a quorum-like system. To remedy this it does not enforce strict quorum membership and instead it uses a “sloppy quorum”; all read and write operations are performed on the first N healthy nodes from the preference list. Merkle TreesA hash tree or Merkle tree is a binary tree in which every leaf node gets as its label a data block and every non-leaf node is labelled with the cryptographic hash of the labels of its child nodes. Some KeyValue stores use Merkle trees for efficiently detecting inconsistencies in data between replicas. vector clockDremelIn contrast to layers such as Pig19 and Hive,16 it executes queries natively without translating them into MR jobs. Lastly, and importantly, Dremel uses a column-striped storage representation, which enables it to read less data from secondary storage and reduce CPU cost due to cheaper compression. Repetition and definition levelswe define the repetition level as the number of repeated fields in the common prefix (including the first path element identifying the record). The definition level specifies the number of optional and repeated fields in the path (excluding the first path element).A definition level smaller than the maximal number of repeated and optional fields in a path denotes a NULL. HDFSEach block replica on a DataNode is represented by two files in the local host’s native file system. The first file contains the data itself and the second file is block’s metadata including checksums for the block data and the block’s generation stamp. If the NameNode does not receive a heartbeat from a DataNode in ten minutes the NameNode considers the DataNode to be out of service and the block replicas hosted by that DataNode to be unavailable. The NameNode then schedules creation of new replicas of those blocks on other DataNodes. Heartbeats from a DataNode also carry information about total storage capacity, fraction of storage in use, and the number of data transfers currently in progress. These statistics are used for the NameNode’s space allocation and load balancing decisions. The NameNode does not directly call DataNodes. It uses replies to heartbeats to send instructions to the DataNodes. A recently introduced feature of HDFS is the BackupNode. Like a CheckpointNode, the BackupNode is capable of creating periodic checkpoints, but in addition it maintains an inmemory, up-to-date image of the file system namespace that is always synchronized with the state of the NameNode. A replica stored on a DataNode may become corrupted because of faults in memory, disk, or network. HDFS generates and stores checksums for each data block of an HDFS file. Checksums are verified by the HDFS client while reading to help detect any corruption caused either by client, DataNodes, or network. When a client creates an HDFS file, it computes the checksum sequence for each block and sends it to a DataNode along with the data. A DataNode stores checksums in a metadata file separate from the block’s data file. When HDFS reads a file, each block’s data and checksums are shipped to the client. The client computes the checksum for the received data and verifies that the newly computed checksums matches the checksums it received. If not, the client notifies the NameNode of the corrupt replica and then fetches a different replica of the block from another DataNode. The design of HDFS I/O is particularly optimized for batch processing systems, like MapReduce, which require high throughput for sequential reads and writes. Currently HDFS provides a configurable block placement policy interface so that the users and researchers can experiment and test any policy that’s optimal for their applications. When a block becomes over replicated, the NameNode chooses a replica to remove. The NameNode will prefer not to reduce the number of racks that host replicas, and secondly prefer to remove a replica from the DataNode with the least amount of available disk space. When a block becomes under-replicated, it is put in the replication priority queue. A background thread periodically scans the head of the replication queue to decide where to place new replicas. HDFS block placement strategy does not take into account DataNode disk space utilization. This is to avoid placing new—more likely to be referenced—data at a small subset of the DataNodes. Each DataNode runs a block scanner that periodically scans its block replicas and verifies that stored checksums match the block data. Whenever a read client or a block scanner detects a corrupt block, it notifies the NameNode. The NameNode marks the replica as corrupt, but does not schedule deletion of the replica immediately. Instead, it starts to replicate a good copy of the block. Only when the good replica count reaches the replication factor of the block the corrupt replica is scheduled to be removed. So even if all replicas of a block are corrupt, the policy allows the user to retrieve its data from the corrupt replicas. A present member of the cluster that becomes excluded is marked for decommissioning. Once a DataNode is marked as decommissioning, it will not be selected as the target of replica placement, but it will continue to serve read requests. The NameNode starts to schedule replication of its blocks to other DataNodes. Once the NameNode detects that all blocks on the decommissioning DataNode are replicated, the node enters the decommissioned state. Then it can be safely removed from the cluster without jeopardizing any data availability. MapReduceThe master pings every worker periodically.If no response isrecei ved from awork er in acertain amount of time, the master marks the worker as failed. jsonJSON stands for JavaScript Object Notation and was inspired by the object literals of JavaScript. A JSON value can be an object, array, number, string, true, false, or null. The JSON syntax does not impose any restrictions on the strings used as names, does not require that name strings be unique, and does not assign any significance to the ordering of name/value pairs. Numeric values that cannot be represented as sequences of digits (such as Infinity and NaN) are not permitted. All strings in JSON must be double-quoted. rumblea query execution engine for large, heterogeneous, and nested collections of JSON objects built on top of Apache Spark. xmlXML, unlike HTML, is case-sensitive. is not the same as or . Every well-formed XML document has exactly one root element. XML elements can have attributes. An attribute is a name-value pair attached to the element’s start-tag. Names are separated from values by an equals sign and optional whitespace. Values are enclosed in single or double quotation marks. each element may have no more than one attribute with a given name. Element and other XML names may contain essentially any alphanumeric character. This includes the standard English letters A through Z and a through z as well as the digits 0 through 9. They may also include these three punctuation characters: _ The underscore ,- The hyphen, . The period.Finally, all names beginning with the string “XML” (in any combination of case) are reserved for standardization in W3C XML-related specifications. XML names may only start with letters, ideograms, or the underscore character. They may not start with a number, hyphen, or period. exam notes2022HTTP command and status code.Storage type choosing. HDFS and random access:For distributed data storage though, and for the use case at hand where we read a large dataset, analyze it, and write back the output as a new dataset, random access is not needed. A distributed file system is designed so that, in cruise mode, its bottleneck will be the data flow (throughput), not the latency. This aspect of the design is directly consistent with a full-scan pattern, rather than with a random access pattern, the latter being strongly latency-bound. json and comments:In JSON (JavaScript Object Notation), comments are not officially supported. Hbase and meta table.if empty json valid against to some schema.mapreduce split.mapreduce function: emit.if reduce function can be used as combine function. 1NF,2NF and 3NF:A candidate key is a minimal set of attributes that determines the other attributes included in the relation. A non-prime attribute is an attribute that is not part of the candidate key.Informally, the second normal form states that all attributes must depend on the entire candidate key.In other words, non-prime attributes must be functionally dependent on the key(s), but they must not depend on another non-prime attribute. 3NF non-prime attributes depend on “nothing but the key”. different type comparision in mongodb. If you have n dimensions, the CUBE operation will generate 2^n combinations. xml schema:You have already seen the xs:sequence element, which dictates that the elements it contains must appear in exactly the same order in which they appear within the sequence element. mongdb atomic operations:In MongoDB, a write operation is atomic on the level of a single document, even if the operation modifies multiple embedded documents within a single document.When a single write operation (e.g. db.collection.updateMany()) modifies multiple documents, the modification of each document is atomic, but the operation as a whole is not atomic. Json schema:tuple validation. referenceshttps://vertabelo.com/blog/normalization-1nf-2nf-3nf/","link":"/2024/01/03/bigdata13/"},{"title":"pai - review notes","text":"fundamentalsprobabilitysample space, event space, σ-algebra and probability spaceA probability space is a mathematical construct that consists of three elements: the sample space (S), the event space (E), and a probability measure (P). Additionally, a sigma-algebra (σ-algebra) is associated with the event space. The sample space is the set of all possible outcomes of an experiment.The event space is a collection of subsets of the sample space.A sigma-algebra is a collection of subsets of the sample space. It includes the sample space, is closed under complementation, and is closed under countable unions. The probability measure is a function that assigns probabilities to events. probability mass function (PMF) and cumulative distribution function (CDF),probability density function (PDF)The Probability Mass Function (PMF) is a function that gives the probability that a discrete random variable is exactly equal to a certain value. The Cumulative Distribution Function (CDF) of a random variable gives the probability thatX takes on a value less than or equal to x.The Probability Density Function (PDF) is applicable to continuous random variables.The total area under the PDF curve is equal to 1. Continuous Distributionsnormal distribution(Gaussian):The Gaussian CDF cannot be expressed in closed-form. Note that the mean of a Gaussian distribution coincides with the maximizer of its PDF, also called mode of a distribution. Joint Probability, Conditional Probability, Sum rule, Product rule(chain rule),the law of total probabilityJoint probability refers to the probability of the occurrence of two or more events simultaneously. Conditional probability is the probability of one event occurring given that another event has already occurred. The sum rule, or addition rule, gives the probability of the union of two events. The product rule, also known as the chain rule, provides a way to express the joint probability of multiple events. The law of total probability is a way to express the probability of an event B by considering all possible ways in whichB can occur. independence, conditional independenceConditional independence does not necessarily imply unconditional independence, and vice versa. Directed Graphical Models(Bayesian networks)Directed graphical models (also called Bayesian networks) are often used to visually denote the (conditional) independence relationships of a large number of random variables. Expectation, Covariance and Variance, Standard deviation,Law of total varianceChange of variables formulaProbabilistic inferenceBayes’ rule,Conjugate Priorsgaussian,Guassian random vectorAny affine transformation of a Gaussian random vector is a Gaussian random vector. Supervised Learning and Point EstimatesMaximum Likelihood Estimation,Maximum a Posteriori Estimation,The MLE and MAP estimate can be seen as a naïve approximation of probabilistic inference, represented by a point density which “collapses” all probability mass at the mode of the posterior distribution. exerciseaffine transformation,Jacobian matrix,PMLlinear regressionlinear regression(MLE), ridge regression(MAP)ridge,lassoleast absolute shrinkage and selection operator (lasso): Laplace prior, L1 regularization.Ridge: Gaussian prior, L2 regularization. The primary difference lies in the penalty terms: L1 regularization uses the sum of absolute values, and L2 regularization uses the sum of squared values.L1 regularization tends to result in exact zeros, leading to sparse solutions, whereas L2 regularization generally leads to smaller, non-zero coefficients. Bayesian linear regression (BLR)Aleatoric and Epistemic Uncertaintyepistemic uncertainty: corresponds to the uncertainty about our model due to the lack of data. aleatoric uncertainty: “irreducible noise”, cannot be explained by the inputs and any model from the model class. equation under the law of total variance. kernelFilteringThe process of keeping track of the state using noisy observations is also known as Bayesian filtering or recursive Bayesian estimation. Kalman filterA Kalman filter is simply a Bayes filter using a Gaussian distribution over the states and conditional linear Gaussians to describe the evolution of states and observations. Gaussian ProcessA Gaussian process is an infinite set of random variables such that any finite number of them are jointly Gaussian. kernel function, feature space, RKHS,Stationarity and isotropyA Gaussian process with a linear kernel is equivalent to Bayesian linear regression. For ν = 1/2, the Matérn kernel is equivalent to the Laplace kernel. For ν → ∞, the Matérn kernel is equivalent to the Gaussian kernel. Note that stationarity is a necessary condition for isotropy. In other words, isotropy implies stationarity. skip 4.3.4 model selectionMaximizing the Marginal LikelihoodMarginal likelihood maximization is an empirical Bayes method. Often it is simply referred to as empirical Bayes.this approach typically avoids overfitting even though we do not use a separate training and validation set. maximizing the marginal likelihood naturally encourages trading between a large likelihood and a large prior. Approximationsrandom Fourier features,Bochner’s theorem,Uniform convergence of Fourier featuresinducing points method,subset of regressors (SoR) approximation,fully independent training conditional (FITC) approximationVariational InferenceLaplace Approximation,Bayesian Logistic Regression,The Laplace approximation matches the shape of the true posterior around its mode but may not represent it accurately elsewhere — often leading to extremely overconfident predictions. Variational family,mean-field distributionInformation Theory,Surprise,entropy,Jensen’s Inequality,Cross-entropy,KL-divergence,Forward and Reverse KL-divergenceThe uniform distribution has the maximum entropy among all discrete distributions supported on {1, . . . , n}. In words, KL(p∥q) measures the additional expected surprise when observing samples from p that is due to assuming the (wrong) distribution q. It can be seen that the reverse KL-divergence tends to greedily select the mode and underestimating the variance which, in this case, leads to an overconfident prediction. The forward KL-divergence, in contrast, is more conservative. Note, however, that reverse-KL is not greedy in the same sense as Laplace approximation, as it does still take the variance into account and does not purely match the mode of p. Minimizing Forward-KL as Maximum Likelihood Estimation,Minimizing Forward-KL as Moment Matching First, we observe that minimizing the forward KL-divergence is equivalent to maximum likelihood estimation on an infinitely large sample size. A Gaussian qλ minimizing KL(p∥qλ) has the same first and second moment as p. Evidence Lower Bound,Gaussian VI vs Laplace approximation, Gradient of Evidence Lower Bound(score gradients and Reparameterization trick)maximizing the ELBO coincides with minimizing reverse-KL. maximizing the ELBO selects a variational distribution q that is close to the prior distribution p(·) while also maximizing the average likelihood of the data p(y1:n | x1:n, θ) for θ ∼ q. Note that for a noninformative prior p(·) ∝ 1, maximizing the ELBO is equivalent to maximum likelihood estimation. skip 5.5.2 Markov Chain Monte Carlo MethodsThe key idea of Markov chain Monte Carlo methods is to construct a Markov chain, which is efficient to simulate and has the stationary distribution p. Markov Chains(Stationarity,convergence), Markov property,time-homogeneous Markov chainsIntuitively, the Markov property states that future behavior is independent of past states given the present state. Stationary distribution,aperiodic,Ergodicity,Fundamental theorem of ergodic Markov chainsAfter entering a stationary distribution π, a Markov chain will always remain in the stationary distribution. It can be shown that there exists a unique stationary distribution π if the Markov chain is irreducible, that is, if every state is reachable from every other state with a positive probability when the Markov chain is run for enough steps. Even if a Markov chain has a unique stationary distribution, it must not converge to it. In words, a Markov chain is aperiodic iff for every state x, the transition graph has a closed path from x to x with length k for all k ∈ N greater than some k0 ∈ N. A Markov chain is ergodic iff there exists a t ∈ N0 such that for any x, x′ ∈ S we have p(t)(x′ | x) &gt; 0. A commonly used strategy to ensure that a Markov chain is ergodic is to add “self-loops” to every vertex in the transition graph. An ergodic Markov chain has a unique stationary distribution π (with full support) irrespectively of the initial distribution q0. This naturally suggests constructing an ergodic Markov chain such that its stationary distribution coincides with the posterior distribution. If we then sample “sufficiently long”, Xt is drawn from a distribution that is “very close” to the posterior distribution. How quickly does a Markov chain converge?(Total variation distance and Mixing time)Detailed Balance Equation,Ergodic TheoremA Markov chain that satisfies the detailed balance equation with respect to π is called reversible with respect to π. Ergodic Theorem is a way to generalize the (strong) law of large numbers to Markov chains. Elementary Sampling Methods,Metropolis-Hastings Algorithm,Metropolis-Hastings theorem, Gibbs SamplingA popular example of a Metropolis-Hastings algorithm is Gibbs sampling. Sampling as Optimization,Gibbs distribution,Langevin Dynamics,Metropolis adjusted Langevin algorithm (MALA) or Langevin Monte Carlo (LMC),unadjusted Langevin algorithm (ULA), Stochastic Gradient Langevin Dynamics, Hamiltonian Monte CarloA useful property is that Gibbs distributions always have full support. Observe that the posterior distribution can always be interpreted as a Gibbs distribution as long as prior and likelihood have full support. Langevin dynamics adapts the Gaussian proposals of the Metropolis-Hastings algorithm to search the state space in an “informed” direction. The simple idea is to bias the sampling towards states with lower energy, thereby making it more likely that a proposal is accepted. A natural idea is to shift the proposal distribution perpendicularly to the gradient of the energy function. The resulting variant of Metropolis-Hastings is known as the Metropolis adjusted Langevin algorithm (MALA) or Langevin Monte Carlo (LMC). The HMC algorithm is an instance of Metropolis-Hastings which uses momentum to propose distant points that conserve energy, with high acceptance probability. pathtarget: show that the stationary distribution of a Markov chain coincides with the posterior distribution.base: Detailed Balance Equation which shows that A Markov chain that satisfies the detailed balance equation with respect to π is called reversible with respect to π and if the Markov chain is reversible with respect to π then π is a stationary distribution. With posterior distribution p(x) = 1/Z q(x), substitute the posterior for π in the detailed balance equation, we can remove z, so we do not need to know the true posterior p to check that the stationary distribution of our Markov chain coincides with p, it suffices to know the finite measure q. However, until now, this does not allow us to estimate expectations over the posterior distribution. Note that although constructing such a Markov chain allows us to obtain samples from the posterior distribution, they are not independent. Thus, the law of large numbers and Hoeffding’s inequality do not apply, but there is a way to generalize the (strong) law of large numbers to Markov chains, which is Ergodic theorem. Next we need to consider how to construct Markov chain with the goal of approximating samples from the posterior distribution p. One way is Metropolis-Hastings Algorithm, in which we are given a proposal distribution and we use the acceptance distribution to decide whether to follow the proposal. Another way is Gibbs sampling. MCMC techniques can be generalized tocontinuous random variables / vectors. Observe that the posterior distribution can always be interpreted as a Gibbs distribution as long as prior and likelihood have full support. Langevin dynamics adapts the Gaussian proposals of the Metropolis-Hastings algorithm to search the state space in an “informed” direction. The simple idea is to bias the sampling towards states with lower energy, thereby making it more likely that a proposal is accepted. A natural idea is to shift the proposal distribution perpendicularly to the gradient of the energy function. The resulting variant of Metropolis-Hastings is known as the Metropolis adjusted Langevin algorithm (MALA) or Langevin Monte Carlo (LMC). skip 6.3.5 BDLArtificial Neural Networks,Activation Functions,Non-linear activation functions allow the network to represent arbitrary functions. This is known as the universal approximation theorem. Bayesian Neural Networks, Heteroscedastic Noise, Variational Inference,Markov Chain Monte Carlo, SWA,SWAG,Dropout and Dropconnect, Probabilistic EnsemblesIntuitively, variational inference in Bayesian neural networks can be interpreted as averaging the predictions of multiple neural networks drawn according to the variational posterior qλ. Calibration,expected calibration error (ECE), maximum calibration error (MCE), Histogram binning, Isotonic regression, Platt scaling, Temperature scalingWe say that a model is wellcalibrated if its confidence coincides with its accuracy across many predictions. Compare within each bin, how often the model thought the inputs belonged to the class (confidence) with how often the inputs actually belonged to the class (frequency). Sequential Decision-MakingActive LearningConditional Entropy,Joint entropyA very intuitive property of entropy is its monotonicity: when conditioning on additional observations the entropy can never increase. Mutual Information(information gain), the law of total expectation, data processing inequality, interaction information(Synergy and Redundancy), Submodularity of Mutual Information, Marginal gain, Submodularity, monotone,data processing inequality: which says that processing cannot increase the information contained in a signal. F is submodular iff “adding” x to the smaller set A yields more marginal gain than adding x to the larger set B. I is monotone submodular. Maximizing Mutual Information, Greedy submodular function maximization, Uncertainty Sampling, Marginal gain of maximizing mutual information,Bayesian active learning by disagreement (BALD)skip proof of Theorem 8.15 Therefore, if f is modeled by a Gaussian and we assume homoscedastic noise, greedily maximizing mutual information corresponds to simply picking the point x with the largest variance. This algorithm is also called uncertainty sampling. Experimental Design, Entropy Searchskip Bayesian OptimizationExploration-Exploitation Dilemma,Online Learning and Bandits, Multi-Armed Bandits, Regret, sublinear regret, Cesàro meanBayesian optimization can be interpreted as a variant of the MAB problem where there can be a potentially infinite number of actions (arms), but their rewards are correlated (because of the smoothness of the Gaussian process prior). Acquisition Functions, Upper Confidence Bound, Bayesian confidence intervals, Regret of GP-UCB, Information gain of common kernels, Frequentist confidence intervals, probability of improvement, expected improvement (EI), Thompson Sampling, probability matching, Information-Directed Samplingskip Information-Directed Sampling Markov Decision ProcessesPlanning deals with the problem of deciding which action an agent should play in a (stochastic) environment. A key formalism for probabilistic planning in known environments are so-called Markov decision processes. Policy,discounted payoff,state value function,state-action value function (also called Q-function)A policy is a function that maps each state x ∈ X to a probability distribution over the actions. Bellman Expectation EquationPolicy Evaluation, Fixed-point Iteration, contraction,Banach fixed-point theoremPolicy Optimization, Greedy Policies, Bellman Optimality Equation, Bellman’s theorem, Policy Iteration, Value Iteration,In particular, if for every state there is a unique action that maximizes the state-action value function, the policy π⋆ is deterministic and unique. Intuitively, the Bellman optimality equations express that the value of a state under an optimal policy must equal the expected return for the best action from that state. Value iteration converges to an ε-optimal solution in a polynomial number of iterations. Unlike policy iteration, value iteration does not converge to an exact solution in general. Partial Observability,Whereas MDPs are controlled Markov chains, POMDPs are controlled hidden Markov models. Tabular Reinforcement LearningTrajectories, episodic setting, continuous setting, On-policy and Off-policy Methods,on-policy methods are used when the agent has control over its own actions, in other words, the agent can freely choose to follow any policy. off-policy methods can be used even when the agent cannot freely choose its actions. Model-based Approaches, Balancing Exploration and Exploitation, ε-greedy, Softmax Exploration(Boltzmann exploration), Rmax algorithmskip Remark 11.3: Asymptotic convergence Note that ε-greedy is GLIE with probability 1 if the sequence (εt)t∈N0 satisfies the RM-conditions (A.56), e.g., if εt = 1/t. A significant benefit to model-based reinforcement learning is that it is inherently off-policy. That is, any trajectory regardless of the policy used to obtain it can be used to improve the model of the underlying Markov decision process. In the model-free setting, this not necessarily true. Model-free Approaches, On-policy Value Estimation, bootstrapping, temporal-difference learning, SARSA, Off-policy Value Estimation, Q-learning, optimistic Q-learningModel-free Reinforcement LearningValue Function Approximation, DQN, experience replay, maximization bias, Double DQNPolicy Approximation(policy gradient methods), policy value function, Policy Gradient, Score gradient estimator, Score gradients with baselines, Downstream returns, REINFORCE algorithmThe main advantage of policy gradient methods such as REINFORCE is that they can be used in continuous action spaces. However, REINFORCE is not guaranteed to find an optimal policy. Even when operating in very small domains, REINFORCE can get stuck in local optima. On-policy Actor-Critics,Advantage Function, Policy Gradient Theorem, Actor-Critic(Q actor-critic,Online actor-critic), advantage actor-critic, bias-variance tradeoff, trust-region policy optimization (TRPO), Proximal policy optimization (PPO)One approach in the online setting (i.e., non-episodic setting), is to simply use SARSA for learning the critic. To learn the actor, we use stochastic gradient descent with gradients obtained using single samples. Model-based Reinforcement LearningPlanning over Finite Horizons, receding horizon control (RHC), model predictive control (MPC), Random shooting methodscheat sheetfrom bookconditional distribution for gaussian;Gaussian process posterior;the predictive posterior at the test point;common kernels;the Hessian of the logistic loss;Surprise,entropy,Jensen’s Inequality,Cross-entropy,KL-divergence,ELBO,the law of large numbers and Hoeffding’s inequality,Hoeffding bound, from exercise Woodbury push-through identity; Solution to problem 3.6;","link":"/2024/01/25/pai11/"},{"title":"Measuring sentence similarity","text":"metricsBLEU (Bilingual Evaluation Understudy)BLEU computes a score based on the n-gram overlap between the generated text and the reference text, as well as the brevity penalty to handle cases where the generated text is too short. The score ranges from 0 to 1, where 1 indicates a perfect match with the reference translations. ROUGE (Recall-Oriented Understudy for Gisting Evaluation)ROUGE score measures the similarity between the machine-generated summary and the reference summaries using overlapping n-grams, word sequences that appear in both the machine-generated summary and the reference summaries. ROUGE score ranges from 0 to 1, with higher values indicating better summary quality. ROUGE scores are branched into ROUGE-N,ROUGE-L, and ROUGE-S.ROUGE-N measures the overlap of n-grams (contiguous sequences of n words) between the candidate text and the reference text. It computes the precision, recall, and F1-score based on the n-gram overlap.ROUGE-L measures the longest common subsequence (LCS) between the candidate text and the reference text. It computes the precision, recall, and F1-score based on the length of the LCS.ROUGE-S measures the skip-bigram (bi-gram with at most one intervening word) overlap between the candidate text and the reference text. It computes the precision, recall, and F1-score based on the skip-bigram overlap. referenceshttps://medium.com/@sthanikamsanthosh1994/understanding-bleu-and-rouge-score-for-nlp-evaluation-1ab334ecadcb","link":"/2024/02/21/llm0/"},{"title":"Sampling","text":"Samplingtop-p samplingThis method only considers the tokens whose cumulative probability exceed the probability p and then redistributes the probability mass across the remaining tokens so that the sum of probabilities is 1. temperatureWhat the temperature does is: it controls the relative weights in the probability distribution. It controls the extent to which differences in probability play a role in the sampling. At temperature t=0 this sampling technique turns into what we call greedy search/argmax sampling where the token with the highest probability is always selected. referencehttps://blog.ml6.eu/why-openais-api-models-cannot-be-forced-to-behave-fully-deterministically-4934a7e8f184","link":"/2024/03/29/llm1/"},{"title":"Problems record of using OpenAI&#39;s API","text":"GPTgpt-3.5-turbo-instruct generates empty text after calling several times.Tried adding space or adding newline, but didn’t work. gpt-3.5-turbo-1106 generates different results from same prompt even though T is set as 0.Tried setting seed but did’t work. Switched to another version mitigated this problem.","link":"/2024/04/02/llm2/"},{"title":"Natural Language Inference(Recognizing Textual Entailment)","text":"definitionNatural language inference (NLI) is the task of determining whether a “hypothesis” is true (entailment), false (contradiction), or undetermined (neutral) given a “premise”. benchmarksBenchmark datasets used for NLI include SNLI, MultiNLI, SciTail, SuperGLUE, RTE, WNLI.","link":"/2024/05/01/llm3/"}],"tags":[{"name":"deep learning","slug":"deep-learning","link":"/tags/deep-learning/"},{"name":"machine learning","slug":"machine-learning","link":"/tags/machine-learning/"},{"name":"Elasticsearch","slug":"Elasticsearch","link":"/tags/Elasticsearch/"},{"name":"Markdown","slug":"Markdown","link":"/tags/Markdown/"},{"name":"Algorithm","slug":"Algorithm","link":"/tags/Algorithm/"},{"name":"graph","slug":"graph","link":"/tags/graph/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"reinforcement learning","slug":"reinforcement-learning","link":"/tags/reinforcement-learning/"},{"name":"algorithm","slug":"algorithm","link":"/tags/algorithm/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"SQL","slug":"SQL","link":"/tags/SQL/"},{"name":"big data","slug":"big-data","link":"/tags/big-data/"},{"name":"git","slug":"git","link":"/tags/git/"},{"name":"hexo","slug":"hexo","link":"/tags/hexo/"},{"name":"computer vision","slug":"computer-vision","link":"/tags/computer-vision/"},{"name":"java","slug":"java","link":"/tags/java/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"probability","slug":"probability","link":"/tags/probability/"},{"name":"speech","slug":"speech","link":"/tags/speech/"},{"name":"tools","slug":"tools","link":"/tags/tools/"}],"categories":[{"name":"theory","slug":"theory","link":"/categories/theory/"},{"name":"algorithm","slug":"algorithm","link":"/categories/algorithm/"},{"name":"database","slug":"database","link":"/categories/database/"},{"name":"others","slug":"others","link":"/categories/others/"},{"name":"development tools","slug":"development-tools","link":"/categories/development-tools/"},{"name":"programming language","slug":"programming-language","link":"/categories/programming-language/"},{"name":"computer system","slug":"computer-system","link":"/categories/computer-system/"},{"name":"web development","slug":"web-development","link":"/categories/web-development/"},{"name":"practice","slug":"practice","link":"/categories/practice/"}],"pages":[{"title":"categories","text":"","link":"/categories/index.html"}]}