{"pages":[{"title":"categories","text":"","link":"/categories/index.html"}],"posts":[{"title":"第十九篇 深度学习(1)-梯度下降","text":"","link":"/2021/09/25/DL/"},{"title":"Elasticsearch","text":"ELK StackElasticsearch, Logstash and Kibana ElasticsearchElasticsearch is a NoSQL database.When you feed data into Elasticsearch, the data is placed into Apache Lucene indexes. Apache LuceneApache Lucene™ is a high-performance, full-featured search engine library written entirely in Java. APILogstashUsing more than 50 input plugins for different platforms, databases and applications, Logstash can be defined to collect and process data from these sources and send them to other systems for storage and analysis. projecthttps://trecpodcasts.github.io/https://doc.yonyoucloud.com/doc/mastering-elasticsearch/chapter-2/21_README.htmlhttps://cloud.tencent.com/developer/article/1600163https://www.elastic.co/cn/blog/how-to-improve-elasticsearch-search-relevance-with-boolean-querieshttps://www.elastic.co/guide/en/app-search/current/relevance-tuning-guide.htmlhttps://medium.com/mlearning-ai/enhancing-information-retrieval-via-semantic-and-relevance-matching-64973ff81818https://www.elastic.co/cn/blog/how-to-improve-elasticsearch-search-relevance-with-boolean-querieshttps://bigdataboutique.com/blog/optimizing-elasticsearch-relevance-a-detailed-guide-c9efd3NDCG：https://www.javatips.net/api/MyMediaLiteJava-master/src/org/mymedialite/eval/measures/NDCG.java","link":"/2023/04/02/Elasticsearch/"},{"title":"第十四篇 机器学习(3)-支持向量机","text":"","link":"/2021/08/29/ML-2/"},{"title":"第十五篇 机器学习(4)-决策树","text":"","link":"/2021/08/29/ML-3/"},{"title":"第十三篇 机器学习(2)-朴素贝叶斯","text":"","link":"/2021/08/29/ML-1/"},{"title":"第二十篇 机器学习(4)-uplift模型","text":"uplift模型uplift模型中文为增益模型，是工业界因果推断与机器学习结合最成熟的算法之一。传统的监督学习模型，往往是对输入x去预测一个y，而增益模型注重于x的变化对y的影响，以广告为例，传统的监督学习往往是给定这个特征去预测用户是否会点击，而增益模型注重的是给这个客户投放广告与否对客户是否购买广告商品所产生的影响。 因果推断","link":"/2021/09/26/ML-4/"},{"title":"第十二篇 机器学习(1)-逻辑回归","text":"逻辑回归逻辑回归名为回归，其实主要是用在分类上。","link":"/2021/08/29/ML/"},{"title":"第二十二篇 机器学习(5)--不平衡的分类问题","text":"","link":"/2021/11/15/ML-5/"},{"title":"My first blog","text":"This is my first blog which is just for test. Happy~I will present my learning process in this blog from today on. Fighting~","link":"/2018/12/21/My-first-blog/"},{"title":"第一篇 Markdown","text":"第一篇 Markdown （Typora）基础语法目录标题篇级数个#加空格，如# 表示后面的文字作为一级标题，## 表示后面的文字作为二级标题。 字体篇加粗：将想要加粗的文字在左右用两个*号括起来 ： **加粗** 斜体：将想要斜体的文字在左右用一个*号括起来 ： 斜体加粗：将想要斜体的文字在左右用三个*号括起来 删除线：将想要加删除线的文字在左右用两个~号括起来波浪线 段落篇 生成目录：[toc]， 但是hexo上传博客用此方法显示目录不成功ORZ。 分割线：三个及以上的-或者*，试一试 无序列表：就是这个前面的小黑点，一个+-或者*加上空格，后面跟上文字。 有序列表：数字标号的。数字加点加上空格 这是第一个 这是第二个 插入类 插入图片：![图片下方的文字](图片地址 &quot;图片标题&quot;) 拿一张试试： 插入超链接：[超链接名称](超链接地址 &quot;标题&quot;) 试试：我的博客 插入公式：$$包含TeX 或 LaTeX 格式的数学公式。 插入表格：|分列，-分割表头和内容，左右两边加：文字居中，只加一边文字偏哪边，默认偏左。 举例 1234表头|表头 --|--内容|内容内容|内容 ​ 试一个：没有成功诶，需要在两端也加上|才会显示表格，可是把最后一行作为表头显示了，emmm，应该是编辑器导致的差异。不过Typora可以很方便地直接右击插入表格。。。 表头 表头 内容 内容 内容 内容 插入代码块 单行代码：一个反引号`括起来，反引号就是Esc下面那个键，记得换成英文输入法。 print('hello world!') 多行代码：三个反引号`括起来。 12import numpy as npimport pandas as pd ​","link":"/2020/03/14/Markdown/"},{"title":"第十七篇 强化学习(1)-马尔可夫决策过程","text":"马尔可夫决策过程马尔可夫性质：当前状态可以完全表征过程。 对于任意有限的马尔可夫决策过程，都存在一个最优策略，不差于其他所有可能的策略。 贝尔曼方程","link":"/2021/09/02/RL/"},{"title":"NP and reduction","text":"PP is the set of all decision problems that can be solved inPolynomial time. Extended Church-Turing ThesisAny realistic model of computation can be efficientlysimulated by a Turing machine.(The ECT is probably false!Probable counter-example: Quantum computing,However, the ECT is true for the classic models of computationunderlying our laptops) NP problemWe do not know of polynomial-time algorithms for these problems, and we cannot prove that nopolynomial-time algorithm exists. NP stands for Nondeterministic Polynomial time. NP is a set of problems that there is a polynomial-time algorithm that verify if a solution to the problem is correct.Note that showing that a problem is in NP does not mean that the problem can be solved in polynomial time. It only means that a proposed solution can be verified in polynomial time. All problems in P are also in NP, we do not know if P=NP. NP-hardnessWe say that a problem Y is NP-hard if every problem in NP can be Karp-reduced to Y. To prove NP-hardness for Y we only have to findone other NP-hard problem X and reduce X to Y. NP-complete problemA large class of problems in this “gray area” has been characterized,and it has been proved that they are equivalent in the following sense: a polynomial-time algorithm for any one of them would imply the existence of a polynomial-time algorithm for all of them. These are the NP-complete problems. Every problem in NP can be reduced to X. We will call such an X an NP-complete problem. To prove a problem is np-complete, we need to prove it lies in Np and it is np-hard. In NP is proved by verifying the solution in polynomial time. Np-hard is proved by using Karp-reduction from known NP-hard problem. the NP-complete problems are the hardest problems in NP The Cook-Levin TheoremThe Sat problem is NP-hard. Hamiltonian CycleReduction from Sat to Hamiltonian Cycle. Travelling SalesmanReduction from Hamiltonian Cycle Graph ColoringGraph Coloring is NP-hard.2-coloring is not NP-hard. Vertex Coverwe say that a set of nodes S is a vertex cover if every edge e has at least one end in S. Set CoverGiven a set U of n elements, a collection S1,…, Sm of subsets of U, anda number k, does there exist a collection of at most k of these sets whoseunion is equal to all of U? CoNPThe complexity class CoNP consists of all problems where a “no” answer can be efficiently verified.For every NP-complete problem there is acorresponding CoNP-complete problem. We do not know if NP=CoNP. PSPACEThe complexity class PSPACE consists of all problems that canbe solved by an algorithm using at most a polynomial amount ofspace.It is strongly believed that NP != PSPACE, but we do not evenknow with certainty whether P != PSPACE or not! For many 2-player games like Geography, deciding if there is awinning strategy from a given position is a PSPACE problem BPP and ZPPBPPBPP (Bounded-error Probabilistic Polynomial-time)consists of all decision problems for which there is apolynomial-time randomized algorithms that is correct withprobability at least 2=3 on all instances. ZPPZPP (Zero-error Probabilistic Polynomial Time)consists of all decision problems for which there is a Las Vegasalgorithm running in expected polynomial time.It is widely believed that P = ZPP = BPP but all three could bedifferent.","link":"/2023/03/28/NP/"},{"title":"Randomized Algorithms","text":"Randomized Algorithms and approximation algorithmA randomized algorithm is an algorithm that uses randomness tomake some of its choices. Las Vegas AlgorithmsA Las Vegas algorithm is a randomized algorithm that always findsthe correct answer, but the running time of the algorithm mightvary significantly depending on the random choices made. Monte Carlo AlgorithmsA Monte Carlo algorithm is a randomized algorithm where theoutput of the algorithm may be incorrect, but we have a guaranteethat this only happens with small probability. Random Number Generator True RNG: gets randomness from measuring physical phenomena(e.g. background radiation) that we believe is sufficiently randomPseudo-RNG: starting from a small random seed, new numbers aregenerated in a completely deterministic way. Randomized Min-Cut Algorithmwhile G has more than 2 vertices:Pick a uniformly random edge of G and contract it.the total runtime is $O(n^2m)$. However this algorithm can be refined and running time improved to $O(n^2log(n))$ (relatively simple algorithm) or$O(m log^3(n))$ (more complicated algorithm). approximation algorithmThe Maximum Cut ProblemPartition of vertices of G into two non-empty sets A and B such that number of edges between A and B is maximized. It is a NP-hard problem. Let us lower the ambition and try to find a fast algorithm that finds a “reasonably good” cut: Random algorithm:Put each vertex in either A or B independently with probability 1/2.The random assignment algorithm cuts (in expectation) at least half of all the edges in the graph.This is an example of an approximation algorithm. finding “good but maybe not optimal” solutionsHeuristic AlgorithmsWork well in some or even many cases, but with no guaranteesabout how well they perform. Approximation AlgorithmsAlgorithms with provable guarantee that the solution found is relatively good compared to the optimum solution, for all instances.For a minimization problem, an algorithm has approximation ratio $\\alpha ≥ 1$ if for every instance it holds that $ Alg ≤ \\alpha Opt$.For a maximization problem, the inequality goes the other way: we have approximation ratio $\\alpha ≤ 1$ if $ Alg ≥ \\alpha Opt$. alpha-approximation algorithmapproximation ratio: $\\alpha$. Minimum Load BalancingNP-hard problem. Given Lengths t1; : : : ; tn of n jobs to be run on m machines, to find Minimum possible makespan of a schedule for the n jobs.Approximation Algorithm for Load Balancing:Assign job i to the machine j with the smallest load. how to prove itThe main difficulty in analyzing approximationalgorithms is to get some handle on the Opt value. We need to find Opt, but finding Opt is NP-hard, so we find lower bounds on Opt. Minimum Vertex CoverA vertex cover in a graph G is a set of vertices that “touches” every edge of G.What is the size of a minimum vertex cover of G? Approximation Algorithm for Minimum Vertex Cover:while there exists an edge e = (u; v) such that u /∈ S and v /∈ S, add (u,v) into S. The algorithm is a 2-approximation algorithm.“Unique Games Conjecture”: it is known that Vertex Cover cannot be approximated better thanwithin a factor 2. Minimum Set CoverA collection of sets S1; : : : ; Sm ⊆ U over some universe U, What is minimum number of Si’s whose union equals U? Analysis of Greedy Set Cover Finale?","link":"/2023/04/25/Randomized-Algorithms/"},{"title":"Regular Expressions","text":"Regular expressionsA formal language for specifying text strings rulesDisjunctions:Letters inside square brackets[]: [A-Z]pipe |: a|b|cNegation in Disjunction: [^Ss] ?: When placed after a character or a group, the question mark makes it optional, meaning that the character or group can occur zero or one time.When placed after a quantifier, such as , +, or ?, it modifies the quantifier to be non-greedy or lazy. A non-greedy quantifier matches as few characters as possible, while a greedy quantifier matches as many characters as possible. :0 or more of previous char+:1 or more of previous char.:any char Anchors:^: The begining. $: The end.","link":"/2023/07/11/Regular-Expressions/"},{"title":"Network Representation Learning","text":"backgroundRecording studying during KTH. First blog about network representation learning, a project belongs to machine learning, advanced course. LINEReproduce paper “LINE: Large-scale Information Network Embedding”. Alias Table MethodIt’s a method of effiently drawing samples from discrete distribution.reference:https://www.keithschwarz.com/darts-dice-coins/https://blog.csdn.net/haolexiao/article/details/65157026 Negative Samplingword2vecOriginal paper:Efficient estimation of word representations in vector space.reference:word2vec Explained: Deriving Mikolov et al.’sNegative-Sampling Word-Embedding Method Skip-Gram ModelOriginal papaer:Distributed Representations of Words and Phrasesand their Compositionality.The idea behind the word2vec models is that the words that appear in the same context (near each other) should have similar word vectors. Therefore, we should consider some notion of similarity in our objective when training the model. This is done using the dot product since when vectors are similar, their dot product is larger.reference:https://www.baeldung.com/cs/nlps-word2vec-negative-sampling graphSage","link":"/2022/12/23/Network-Representation-Learning/"},{"title":"PCA","text":"关于PCA为什么要中心化因为不做zero mean，根本做不了PCA。从线性变换的本质来说，PCA就是在线性空间做一个旋转（数据矩阵右乘协方差矩阵的特征向量矩阵），然后取低维子空间（实际上就是前n_components个特征向量张成的子空间）上的投影点来代替原本的点，以达到降维的目的，注意我说的，只做了旋转，没有平移，所以首先你要保证原本空间里的点是以原点为中心分布的，这就是zero mean的目的。另外如果自己手撸过PCA的算法就知道了，explained_variance_和explained_variance_ratio_是怎么实现的？explained_variance就是协方差矩阵的每个特征值除以sample数，而explained_variance_ratio_是每个特征值除以所有特征值之和。为什么这么简单呢？这也和zero mean有关，如果你用最大投影长度的证法去证明PCA就会在过程中很自然的发现这一点，在这里我就不展开了。 链接：https://www.zhihu.com/question/40956812/answer/848527057 PCA 去中心化不一定是必需的，这一结论成立的前提是严格使用协方差矩阵的定义式 S=XXT−nμμT，而不是用 XXT 来当作协方差矩阵。 Centering is an important pre-processing step because it ensures that the resulting components are only looking at the variance within the dataset, and not capturing the overall mean of the dataset as an important variable (dimension). Without mean-centering, the first principal component found by PCA might correspond with the mean of the data instead of the direction of maximum variance. https://towardsdatascience.com/tidying-up-with-pca-an-introduction-to-principal-components-analysis-f876599af383 Why is minimizing squared residuals equivalent to maximizing variance? Consider a datapoint (row of ). Then the contribution of that datapoint to the variance is , or equivalently the squared Euclidean length . Applying the Pythagorean theorem shows that this total variance equals the sum of variance lost (the squared residual) and variance remaining. Thus, it is equivalent to either maximize remaining variance or minimize lost variance to find the principal components. http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/ PCA can only be interpreted as the singular value decomposition of a data matrix when the columns have first been centered by their means. https://stats.stackexchange.com/questions/29781/when-conducting-multiple-regression-when-should-you-center-your-predictor-varia PCA focuses on “explaining” the data matrix using the sample means plus the eigencomponents. When the column mean is far from the origin, the first right singular value is usually quite highly correlated with column mean - thus using PCA concentrates on the second, third and sometimes higher order singular vectors. This is a loss of information when the mean is informative for the process under study. On the other hand, when the scatterplot of the data is roughly elliptical, the PCs typically align with the major axes of the ellipse. Due to the uncorrelatedness constraint, if the mean is far from the origin, the first singular vector will be close to the mean and the others will be tilted away form the major axes of the ellipse. Thus the first singular vector will not be informative about the spread of the data, and the second and third singular values will not be in the most informative directions. Generally, PCA will be more informative, particularly as a method for plotting the data, than uncentered SVD. https://online.stat.psu.edu/stat555/node/94/ Since X is zero centered we can think of them as capturing the spread of the data around the mean in a sense reminiscent of PCA. https://intoli.com/blog/pca-and-svd/ that reconstruction error is minimized by taking as columns of W some k orthonormal vectors maximizing the total variance of the projection. https://stats.stackexchange.com/questions/130721/what-norm-of-the-reconstruction-error-is-minimized-by-the-low-rank-approximation PCA is a regressional model without intercept1. Thus, principal components inevitably come through the origin. If you forget to center your data, the 1st principal component may pierce the cloud not along the main direction of the cloud, and will be (for statistics purposes) misleading.https://stats.stackexchange.com/questions/22329/how-does-centering-the-data-get-rid-of-the-intercept-in-regression-and-pca Centering brings in a big difference. PCA with centering maximizes SS deviations from the mean (i.e. variance); PCA on raw data maximizes SS deviations from the zero point.https://stats.stackexchange.com/questions/489037/principal-components-with-and-without-centering?noredirect=1&amp;lq=1 SVD and PCA https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca singular value decomposition SVD is basically a matrix factorization technique, which decomposes any matrix into 3 generic and familiar matrices. Eigenvalues and Eigenvectors The concept of eigenvectors is applicable only for square matrices. The vector space spanned by an eigenvector is called an eigenspace. A square matrix is called a diagonalizable matrix if it can be written in the format: $ A=PDP^{-1} $, D is the diagonal matrix comprises of the eigenvalues as diagonal elements A Symmetric Matrix where the matrix is equal to the transpose of itself. Special properties of a Symmetric Matrix with respect to eigenvalues and eigenvectors:Has only Real eigenvalues;Always diagonalizable;Has orthogonal eigenvectors. A matrix is called an Orthogonal Matrix if the transpose of the matrix is the inverse of that matrix.ince the eigenvectors of a Symmetric matrix are orthogonal to each other, matrix P in the diagonalized matrix A is an orthogonal matrix. So we say that any Symmetric Matrix is Orthogonally Diagonalizable. https://towardsdatascience.com/singular-value-decomposition-and-its-applications-in-principal-component-analysis-5b7a5f08d0bd % For the PCA derived from maximal preserved variance \\cite{lee2007nonlinear}, we have the covariance% of $\\mathbf{y}$, which is% \\begin{equation}% \\mathbf{C}_{\\mathbf{y} \\mathbf{y}}=E\\left{\\mathbf{y} \\mathbf{y}^T\\right}% \\end{equation}% This equation is valid only when $\\mathbf{y}$ is centered. The goal of PCA is to maximize the variance of the data along each of the principal components. Centering is an important step because it ensures that the resulting components are only looking at the variance of features, and not capturing the means of the features as important. Without mean-centering, the first principal component found by PCA might correspond with the mean of the data instead of the direction of maximum variance.","link":"/2022/12/29/PCA/"},{"title":"第十篇 SQL(1)-数据统计","text":"","link":"/2021/08/07/SQL/"},{"title":"Turing Machines and Undecidability","text":"mathematical model of computationTuring MachinesTuring machines are primarily a mathematical construction. A Turing machine (TM) consists of:A finite set of states: initial state(when the TM starts, it is in this state) and accepting states(if the TM ends in one of these states, it says yes); Transition rules that determine what to do based on current state and content of memory tape at current position; An alphabet, the set of symbols that can be stored on the memory tape. The Church-Turing HypothesisAnything which can be computed by any automatic method, can also be computed by a Turing machine. If a model can also do everything that a Turing machine can, then it is called a Turing-complete model of computation. The RAM ModelModels the way modern computers operate, with registers and memory cells that can be immediately accessed. Comparison-Based SortingWe will prove that any such algorithm must make Ω(n log n)comparisons (and thus take Ω(n log n) time) Undecidabilitydecision problemsThe answer is just a single bit of information,“yes” or “no”.A decision problem is decidable if there exists an algorithm for solving the problem without any efficiency considerations. The Halting ProblemTuring machine M, and input x to M. Does M halt when run on the input x?The Halting Problem is undecidable. Halting On Empty InputIt is undecidable. Halting on All InputsIt is undecidable. Recursively Enumerable ProblemsThere exists an “algorithm” which terminates whenever the answer is“yes” but does not terminate on “no” instances.Problems which have algorithms like this are called recursively enumerable. Turing reductionA Turing reduction (also called Cook reduction) from a problemX to a problem Y is an efficient algorithm for problem X which isallowed to assume an algorithm for Y as a black box. Vertex CoverIndependent SetSet Cover3-SatKarp ReductionsX ≤p Y if given any instance A of problem X, we can inpolynomial time construct an instance f (A) of problem Y such thatthe answer to A is the same as the answer to f (A). Turing reduction is a type of reduction that is stronger than Karp reduction. A problem A is Turing-reducible to problem B if there exists a Turing machine that can solve problem A by making a polynomial number of calls to a black-box subroutine for problem B. In other words, if we can use an algorithm for problem B to solve problem A, then A is Turing-reducible to B. Turing reduction preserves the complexity class of a problem, meaning that if A is Turing-reducible to B and B is in a certain complexity class, then A is also in that complexity class. Karp reduction, also known as polynomial-time reduction, is a weaker form of reduction than Turing reduction. A problem A is Karp-reducible to problem B if there exists a polynomial-time algorithm that can transform any instance of problem A into an instance of problem B such that the answer to the transformed instance is the same as the answer to the original instance of problem A. In other words, if we can use an algorithm for problem B to solve a transformed version of problem A in polynomial time, then A is Karp-reducible to B. Karp reduction preserves the complexity class up to polynomial factors, meaning that if A is Karp-reducible to B and B is in a certain complexity class, then A is also in that complexity class up to polynomial factors. Good referencehttps://www.cs.rochester.edu/u/nelson/courses/csc_173/computability/undecidable.htmlhttp://www2.lawrence.edu/fast/GREGGJ/CMSC515/chapt05/Reducibility.htmlhttps://www.cs.princeton.edu/courses/archive/spring05/cos423/lectures.php","link":"/2023/04/25/Turing-Machines/"},{"title":"第四篇 git相关(2)-git&github","text":"远程仓库远程仓库是指托管在因特网或其他网络中的你的项目的版本库。 githubGitHub是一个面向开源及私有软件项目的托管平台，因为只支持git作为唯一的版本库格式进行托管，故名GitHub。 一次练习最近在学java，也想巩固一下之前学习过的算法和学习新的算法，就打算用java实现，就用这个项目来加强git的使用练习。 首先在IDEA下java学习的项目里，新建了一个algorithms module，在此文件下，进入Git Bash，新建本地仓库，此时只有一个用来测试排序算法的java文件和一个iml项目配置文件。 目前java文件的内容还是空的， 编辑一下，加入main函数，文件被编辑之后，再次使用git status查看文件状态，会发现文件状态已经变成了modified，再次add，然后开始commit。 commit之后，会显示此次提交的一些信息。 前面展示了一些Git本地的基本操作，现在假如本地文件修改好了，或者工作暂停了，准备放到github上，那么先去github上Create a new repository，最初创建的时候只有一个readme文件，下面将本地仓库同步到远程仓库上。 建立完之后，在本地仓库，将远程仓库的URL复制下来，添加远程仓库。 现在本地仓库里是没有readme文件的，如果此时想要直接push，将本地仓库推送到远程仓库的话，看看会发生什么。 跟随这个报错信息的指示，使用pull，看又会发生什么。这个原因是因为目前本地仓库和远程仓库没有任何相同的文件，根本不相干，所以会被告知无法合并，更加方便的流程是先从远程仓库拉取下来，再把本地文件加入到远程仓库下载到本地的库，然后再提交。 那就没有解决办法了嘛？不是的，可以使用一个强制的方法，添加一个可选项–allow-unrelated-histories，问题终于得以解决。 现在再去github上看看，就会发现提交成功而且push成功啦，开森，撒花~之后就要坚持练习写代码啦，刚把得勒~","link":"/2020/03/29/git-1/"},{"title":"第六篇 git相关(3)-分支与合并","text":"git分支git分支是git的一大特性，git 的分支本质上仅仅是指向提交对象的可变指针，是包含所指对象校验和（长度为 40 的 SHA-1 值字符串）的文件，这也是为什么git分支的创建和销毁都异常高效的原因，创建新分支时，就是在当前所在的提交对象上创建一个指针，而git是通过一个名为HEAD的特殊指针来记录自己的当前位置的。 与git分支相关的命令有： 1234567git branch -- 查看本地分支git branch -v -- 查看本地分支每个分支的最后一次提交git branch -vv -- 查看本地分支每个分支跟踪的远程分支git checkout -b dev -- 新建dev分支，并切换到dev分支(此后HEAD会指向dev分支,工作目录也会变为dev分支指向的快照内容)git checkout master -- 切换到master分支(此后HEAD会指向master分支,工作目录也会变为master分支指向的快照内容)git merge dev -- 合并dev分支到当前分支git branch -d dev -- 删除dev分支 分支切换注意在切换分支时，git会重置工作目录，自动添加、删除、修改文件以确保此时你的工作目录和这个分支最后一次提交时的样子一模一样，因此在切换分支前，要注意暂存当前的工作进度，可以使用git stash命令暂存当前工作。 分支合并在合并时，可能会看到fast-forward这个词，这是指当前分支是合并分支的直接上游，两者合并不会产生冲突，而只是指针向前移动了，所以叫做fast-forward。 如果两个分支已经分叉开来，进行合并时，git会根据两个分支的共同祖先来做一个三方合并，此时的合并就不是简单的指针向前移动了，而是将三方合并的结果生成一个新的快照并创建一个新的提交指向它。 有时候合并会发生冲突，靠git自动合并已经无法解决，就需要人工去解决冲突，git会在有冲突的文件中加入标记，使用&lt;&lt;&lt;&lt;&lt;&lt;&lt;和&gt;&gt;&gt;&gt;&gt;&gt;&gt;、=======标识了冲突的位置，=======将两个分支的冲突位置内容分隔开来，为了解决冲突，只能选择其中一个，手动地将标识的片段改为你选择的内容即可，修改完冲突之后，还要使用git add来表示冲突已经解决，git commit来完成合并提交。 远程分支远程分支以 (remote)/(branch) 形式命名，本地分支与远程分支交互相关的命令如下： 1234git push (remote) (branch):(remote branch) --推从到远程分支git fetch (remote) --拉取远程仓库的内容，不会修改工作目录git pull (remote) --拉取并合并远程仓库的内容到本地git push (remote) --delete (branch) --删除远程分支","link":"/2021/07/03/git-2/"},{"title":"chatgpt","text":"chatgpt apiparameterstwo important parameters that you can use with OpenAI’s GPT API to help control text generation behavior: temperature and top_p sampling.Temperature is a parameter that controls the “creativity” or randomness of the text generated by GPT-3. A higher temperature (e.g., 0.7) results in more diverse and creative output, while a lower temperature (e.g., 0.2) makes the output more deterministic and focused.In practice, temperature affects the probability distribution over the possible tokens at each step of the generation process. A temperature of 0 would make the model completely deterministic, always choosing the most likely token. Top_p sampling is an alternative to temperature sampling. Instead of considering all possible tokens, GPT-3 considers only a subset of tokens (the nucleus) whose cumulative probability mass adds up to a certain threshold (top_p). referencehttps://community.openai.com/t/cheat-sheet-mastering-temperature-and-top-p-in-chatgpt-api-a-few-tips-and-tricks-on-controlling-the-creativity-deterministic-output-of-prompt-responses/172683","link":"/2023/06/28/chatgpt/"},{"title":"第三篇 git相关(1)-git基础原理与命令","text":"什么是gitGit is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. 使用Git假设已经在电脑上安装好了Git，并配好了环境，在windows下，文件夹下右击出现了 Git Bash Here，就说明已经安装好了。 使用git需要首先建立一个仓库，之后就可以在这个仓库中对代码进行各种操作，过程中会使用到各种git命令，下面就介绍一下每个git命令的具体作用。 Git 基础命令git init用于初始化一个仓库。在某个文件夹下打开Git Bash，运行完这个命令，文件夹下会生成一个.git文件夹，这个文件夹会记录以后的变更行为，但想要真正地追踪这些变更，还需要更多的操作，只有处于tracked状态下的文件，git才会追踪，后面会具体介绍。 git status用于查看仓库中所有文件的状态。Git中文件有4种状态：untracked， unmodified , modified, staged，后三种状态属于tracked，这几个状态体现了git的原理。 新建一个文件的时候，这个文件处于untracked状态，对这个文件使用了 git add之后，这个文件进入staged状态，也就是暂存区，使用了git commit之后，这个文件进入unmodified状态，这才是实际提交了改动，如果编辑了这个文件，对这个文件进行了更改，便进入了modified状态。 git add在上部分有介绍过，但是这个命令并不是添加文件到某个项目中的意思，准确地说这个命令是把想要提交的内容添加到准备提交的集合里，可以用这个命令来追踪新文件(add the file)，暂存文件（stage the file），或者其它在commit之前的操作。如果在运行了git add之后又修改了文件，没有再次运行git add，就运行了commit，commit的是修改之前的内容。那么是不是每一次都要反复操作git add呢，其实还有别的方法可以跳过暂存区这一步，下面会说明。 git commit是提交变化到仓库里，提交到仓库里的几乎总是可以恢复的，后面会介绍如何恢复。git commit -a就可以跳过暂存区这一步，因为-a包含了所有改动过的文件。git commit -m可以加上这次提交的描述， git rm用于删除文件，这里的删除有两种含义，从Git中删除和从工作目录中删除，如果只是想要Git不在追踪这个文件，需要使用git rm --cached。 git log用于回顾提交历史，运行这个命令可以看到提交的SHA-1 校验和，作者，提交时间和具体提交的内容。如果想要复原到某次提交时候的版本，这个命令是非常重要的，通过拿到每次提交的SHA-1 校验和，可以追踪到对应的版本。 git reset HEAD 用于取消暂存文件。 git checkout --用于撤销所作的修改。 Git 分支分支就是与主线相对的，每个人都可以使用各自的分支进行工作，而不影响主线。在许多版本控制系统中，是需要创建一个完整的源项目副本来创建分支的，而Git不是这样的，Git处理分支的方式非常轻量，分支之间的操作非常迅速。 要理解Git是如何处理分支的，就要理解Git是如何实现对文件的追踪的。Git保存的是不同时刻的快照（Snapshot）,进行提交操作时，Git会保存一个提交对象，这个对象包含了一个指向暂存内容快照的指针，还包含了作者、邮箱等内容以及指向它的父对象的指针，如果是第一次提交，是没有父对象的，而之后的提交，其父对象就是上一次提交。Git的分支，本质上就是指向提交对象的可变指针，所以不同的分支可以指向不同的内容，从而互不影响，而且Git的分支实质上就是一个包含所致对象校验和的文件，所以其创建和销毁都非常高效。 git branch就是创建分支的命令，这个命令会在当前的提交快照上创建一个指针。Git通过一个HEAD的特殊指针指向当前所在的本地分支，从而可以知道自己当前在哪一个分支上。 git branch -d用于删除分支。 git checkout是切换分支的命令。 git merge用于合并分支。当合并分支产生冲突时，Git会停下来，这时候需要手动解决这些冲突，解决完冲突之后，使用git add命令将冲突文件标记为冲突已解决。 远程仓库之前所述的内容，都是基于本地的操作，而如果想要在Git项目上进行协作，就需要一个公共的空间供项目参与者进行共同编辑，这就是远程仓库。远程仓库是指托管在因特网或其他网络中的你的项目的版本库，使用命令新建远程仓库的操作与本地是一致的，在github上create repository就可以新建一个远程仓库，本地和远程仓库之间通过SSH连接，完成SSH的公钥设置之后， 想要实现本地与远程仓库的内容交换，使用以下介绍的命令。 git remote add &lt;shortname&gt; &lt;url&gt;用于添加一个新的远程仓库。 git fetch用于从远程仓库中获取本地没有的数据，但是这个命令指挥把数据下载到本地仓库，并不会与本地仓库自动合并，想要实现自动合并，需要使用git pull命令。git clone可以把远程仓库的内容克隆到本地，并将远程仓库默认命名为”origin”。 git push &lt;remote&gt; &lt;branch&gt;命令用于将本地内容推送到远程仓库上，只有具备远程仓库的写入权限，并且没有人推送过之前，这条命令才会生效，如果别人先推送了，需要先抓取别人的工作并合并到自己的工作中之后才能推送。 git remote show &lt;remote&gt;用于查看某一个远程仓库的具体新息。 git 别名上述说了很多命令，有些命令也比较长，命令很多也比较难记下来，git提供了将这些命令起个别名的功能方便使用。 git config --global alias.co checkout就将checkout起了别名co，现在使用git co就相当于git checkout。","link":"/2020/03/15/git/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2018/12/21/hello-world/"},{"title":"第二十一篇 java(2)- 集合框架","text":"集合框架java集合框架包括 Collection 和 Map 两种，Collection 存储着对象的集合，而 Map 存储着键值对(两个对象)的映射表。 CollectionCollection下包括Set、List、Queue，各自又包含了使用不同方式的实现。 SetSet下包括TreeSet，HashSet，LinkedHashSet。其中TreeSet基于红黑树实现。","link":"/2021/09/26/java-1/"},{"title":"第七篇 Linux相关(2)-进程管理与性能分析","text":"进程管理相关命令进程启动12nohup --用于在系统后台不挂断地运行命令&amp; --放在命令后，表示后台执行 进程查看1234567ps -aux --显示进程状态top --实时显示进程状态pstree --树状图展示进程关系nice --调整进程优先级pidof --查询进程PIDkill --终止进程killall --终止某服务名称对应的所有进程 历史命令12history --显示执行过的命令历史!编码数字 --重复执行某一次的命令 性能分析系统状态1234uname --查看系统内核版本与系统架构,详细系统版本使用cat /etc/redhat-releaseuptime --查看系统的负载信息free --显示当前系统中内存的使用量信息who --查看当前登入主机的用户终端信息 网络状态123ifconfig --查看网卡配置和网络状态ifconfigping --测试主机之间的网络连通性netstat --显示如网络连接、路由表、接口状态等的网络相关信息 时间123date -- 显示当前日期date &quot;+%Y-%m-%d %H:%M:%S&quot; --以给定格式显示日期date &quot;+%j&quot; --今年第几天","link":"/2021/07/09/linux-1/"},{"title":"large model","text":"basic ideasZero-Shot Learningzero-shot learning, in which your model learns how to classify classes that it hasn’t seen before. Contrastive Language-Image Pretraining (CLIP)Just like traditional supervised models, CLIP has two stages: the training stage (learning) and the inference stage (making predictions).In the training stage, CLIP learns about images by “reading” auxiliary text (i.e. sentences) corresponding to each image. CLIP aims to minimize the difference between the encodings of the image and it’s corresponding text.In the inference stage, we setup the typical classification task by first obtaining a list of all possible labels.Each label will then be encoded by the pretrained text encoder from Step 1.Now that we have the label encodings, T₁ to Tₙ, we can take the image that we want to classify, feed it through the pretrained image encoder, and compute how similar the image encoding is to each text label encoding using a distance metric called cosine similarity. contrastive learningContrastive learning is a machine learning technique used to learn the general features of a dataset without labels by teaching the model which data points are similar or different.It looks at which pairs of data points are “similar” and “different” in order to learn higher-level features about the data, before even having a task such as classification or segmentation. SimCLRv2The entire process can be described concisely in three basic steps: For each image in our dataset, we can perform two augmentation combinations (i.e. crop + resize + recolor, resize + recolor, crop + recolor, etc.). We want the model to learn that these two images are “similar” since they are essentially different versions of the same image. To do so, we can feed these two images into our deep learning model (Big-CNN such as ResNet) to create vector representations for each image. The goal is to train the model to output similar representations for similar images. Lastly, we try to maximize the similarity of the two vector representations by minimizing a contrastive loss function. Meta-learningThe idea of meta-learning is to learn the learning process. In-context Learninguring in-context learning, we give the LM a prompt that consists of a list of input-output pairs that demonstrate a task. At the end of the prompt, we append a test input and allow the LM to make a prediction just by conditioning on the prompt and predicting the next tokens. Instruction learningInstruction learning is an idea proposed by the team led by Quoc V. Le at Google DeepMind in a paper titled ‘Finetuned Language Models Are Zero-Shot Learners’ in 2021. The purpose of instruction learning and prompt learning is to explore the knowledge inherent in language models. The difference is that prompts aim to stimulate the completion ability of the language model, such as generating the second half of a sentence based on the first half or filling in the blanks. Instructions aim to stimulate the understanding ability of the language model by providing more explicit instructions, enabling the model to take correct actions. The advantage of instruction learning is that after fine-tuning through multitask learning, it can also perform zero-shot learning on other tasks, while prompt learning is specific to one task. Its generalization ability is not as strong as instruction learning. Diffusion ModelIn machine learning, the Diffusion Model refers to a class of algorithms or models that utilize diffusion processes for various tasks, such as data clustering, image segmentation, or graph-based learning. The basic principle of the Diffusion Model in machine learning is to propagate information or labels through the connections or edges of a graph or network. The diffusion process starts with initial information or labels assigned to some nodes in the graph, and it gradually spreads and influences the neighboring nodes based on certain rules or algorithms. Stable DiffusionStable Diffusion is a deep learning, text-to-image model released in 2022. It is primarily used to generate detailed images conditioned on text descriptions. LLaMALLaMA, a collection of foundation language models ranging from 7B to 65B parameters. Prompt engineeringPrompt engineering is a relatively new discipline for developing and optimizing prompts to efficiently use language models (LMs) for a wide variety of applications and research topics.Prompt engineering focuses on crafting the optimal textual input by selecting the appropriate words, phrases, sentence structures, and punctuation. FastChatreferenceshttps://towardsdatascience.com/understanding-zero-shot-learning-making-ml-more-human-4653ac35ccabhttps://towardsdatascience.com/understanding-contrastive-learning-d5b19fd96607http://ai.stanford.edu/blog/understanding-incontext/https://www.8btc.com/article/6813626https://en.wikipedia.org/wiki/Stable_DiffusionLLaMA: Open and Efficient Foundation Language Modelshttps://www.promptingguide.ai/","link":"/2023/06/19/large-model/"},{"title":"第八篇 java(1)-面向对象","text":"类与对象类就是用户定义好的原型，可以通过类创建对象，类中定义好了一系列的属性或者是函数。对象是真正的实体。当为一个类创建了对象，也可以说是实例化了这个类，java中有几种创建类的方式。 当只是简单地声明一个类变量时，如Object a，不同于原始变量int、double等声明变量时就分配好了内存，这样的声明方式并没有创建好一个对象，需要通过new关键字来触发类构造器，并为这个对象分配好内存。所有类都至少有一个构造函数，如果没有定义，Java编译器会自动创建一个无参构造函数。这个构造器会调用其父类的无参构造函数。 封装、继承与多态修饰符在介绍封装、继承与多态之前，需要先了解java中的修饰符，修饰符有两类： 一类是控制访问权限的，一类是实现其他功能的。控制访问权限的修饰符有 12345678// 以下为类修饰符public --任何类均可访问。default --没有指定修饰符时的默认修饰符，只有同一个包中的类可以访问。// 以下为属性、方法修饰符public --任何类均可访问。private --只有声明的类中可以访问。protected --只有同一个包中的类和其子类可以访问。default --没有指定修饰符时的默认修饰符，只有同一个包中的类可以访问。 实现其他功能的修饰符有： 12345678910// 以下为类修饰符final --此类不能被其他类继承。abstract --抽象类，此类不能用来创建对象。// 以下为属性、方法修饰符final --属性、方法不能被重载。static --属性、方法属于类而不是对象。abstract -- 只能用在抽象类中的方法上。transient -- 序列化对象时跳过此属性和方法。synchronized -- 此方法一次只能被一个线程访问。volatile --此属性的值不是线程内部缓存，而是从主内存中读取。 封装封装可以将实现细节隐藏起来，其最主要的功能在于我们能修改自己的实现代码，而不用修改那些调用我们代码的程序片段。 继承在继承中，先了解两个概念：父类和子类，父类就是被继承的类，子类就是继承其他类的类，在jva中，继承使用extends关键字或implements关键字。java中类的继承是单一继承，一个子类只能拥有一个父类。所有 Java 的类均是由java.lang.Object 类继承而来的。通过使用关键字 extends ，子类可以继承父类的除 private 属性外所有的属性。Implements 关键字在类继承接口的情况下使用，可以多继承接口。 重写(Override)与重载(Overload)重写(Override)重写是子类对父类的允许访问的方法的实现过程进行重新编写！返回值和形参都不能改变。 子类在声明变量时，可以使用父类类型，这是因为在编译阶段，只是检查参数的引用类型，然而在运行时，Java 虚拟机 (JVM) 指定对象的类型并且运行该对象的方法。需要注意的是构造方法不能被重写。需要在子类中调用父类的被重写方法时，要使用 super 关键字。 重载(Overload)重载 (overloading) 是在一个类里面，方法名字相同，而参数不同。返回类型可以相同也可以不同。构造函数是可以重载的。 多态通过继承、重写、重载可以以多种不同的方式实现某个操作，便可以实现多态。除此之外，java中还有接口和抽象类以实现多态。 接口接口通常以interface来声明。一个实现接口的类，必须实现接口内所描述的所有方法，否则就必须声明为抽象类。","link":"/2021/07/16/java/"},{"title":"object detection","text":"datasetsPASCAL VOC 2007, VOC 2012, Microsoft COCO (Common Objects in Context).UA-DETRAC: https://detrac-db.rit.albany.edu/ https://www.kaggle.com/datasets/patrikskalos/ua-detrac-fix-masks-two-wheelers?resource=download https://colab.research.google.com/github/hardik0/Multi-Object-Tracking-Google-Colab/blob/main/Towards-Realtime-MOT-Vehicle-https://github.com/hardik0/Towards-Realtime-MOT/tree/masterTracking.ipynb#scrollTo=y6KZeLt9ViDehttps://github.com/wy17646051/UA-DETRAC-Format-Converter/tree/mainMIO-TCD:https://tcd.miovision.com/KITTI:https://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmarkTRANCOS: https://gram.web.uah.es/data/datasets/trancos/index.htmlSTREETS:https://www.kaggle.com/datasets/ryankraus/traffic-camera-object-detection: single classVERI-Wild: https://github.com/PKU-IMRE/VERI-Wild YOLOv7model reparameterizationModel re-parametrization techniques merge multiple computational modules into one at inference stage. The model re-parameterization technique can be regarded as an ensemble technique, and we can divide it into two categories, i.e., module-level ensemble and model-level ensemble. Model scalingModel scaling is a way to scale up or down an already designed model and make it fit in different computing devices.Network architecture search (NAS) is one of the commonly used model scaling methods. efficient layer aggregation networks(ELAN)dynamic label assignmentusing yolov7","link":"/2023/05/05/object-detection/"},{"title":"第十六篇 python(3)-matplotlib绘图","text":"","link":"/2021/08/31/python-2/"},{"title":"第二篇 Linux相关(1)-文件管理与文本编辑","text":"文件操作相关命令查看目录和文件12345678ls -- 当前目录下文件展示ll -- 当前目录下文件展示 详细tree --树状图形式展示pwd --显示当前工作目录find --按照指定条件来查找文件所对应的位置cd -- 打开目录，.表示当前目录，..表示上一级目录，-表示回到刚才所在的路径下。which 命令 --查看命令所在路径file 文件名 --查看文件类型 查看文件内容1234567891011cat 文件名 -- 从第一行开始显示文件内容head [-n number] 文件名 -- 只看头几行tail [-n number] 文件名 -- 只看尾巴几行，tail -f 可以实时查看文件更新内容more 文件名 -- 一页一页显示内容less 文件名 -- 一页一页显示内容,相比less可以向上翻页nl 文件名 -- 显示行号stat 文件名 --查看文件的具体存储细节和时间等信息wc 文件名 --统计指定文本文件的行数、字数或字节数grep 文件名 --按行提取文本内容cut 文件名 --按列提取文本内容diff [参数] 文件名称A 文件名称B --比较多个文件之间内容的差异 处理目录和文件12345678mkdir -- 创建目录rmdir -- 删除空目录cp source dest --复制source到destmv source dest -- 移动source到dest/重命名rm -- 删除touch --创建空白文件tar -czvf 压缩包名称.tar.gz 要打包的目录 -- 把指定的文件进行打包压缩tar -xzvf 压缩包名称.tar.gz -C 解压到的路径 --解压到指定路径 磁盘管理12345df -- 列出文件系统的整体磁盘使用量du -- 检查磁盘空间使用量du -ah --max-depth=1 查看当前目录下的第一级使用量mount --磁盘挂载umount --磁盘卸载 文本编辑器vi/vim命令模式(:)12345678910111213:i -- 进入编辑模式:wq -- 保存并退出:q -- 退出:q! --强制退出:w [filename] --另存为:set nu --显示行号:set nonu --取消行号:1,$s/word1/word2/g -- 从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 :n1,n2s/word1/word2/g -- 在第 n1 与 n2 行之间寻找 word1 这个字符串，并将该字符串取代为 word2 :10,20s#^#//#g -- 在 10 - 20 行添加 // 注释:10,20s#^//##g -- 在 10 - 20 行删除 // 注释:10,20s/^/#/g -- 在 10 - 20 行添加 # 注释::10,20s/#//g -- 在 10 - 20 行删除 # 注释 操作键:1234567891011gg -- 转到第一行G -- 转到最后一行nG -- 转到第n行dd -- 删除光标所在行yy -- 复制光标所在行[Ctrl] + [f] -- 往上翻页[Ctrl] + [b] -- 往下翻页/ -- 搜索，如/word就是搜索wordu -- 撤销上一步[Ctrl]+r -- 重复上一步p,P -- 粘贴复制内容,p是复制到光标所在下一行，P是复制到光标所在上一行。","link":"/2020/03/14/linux/"},{"title":"第十一篇 python(2)-flask+gunicorn+supervisor的python服务部署","text":"","link":"/2021/08/08/python-1/"},{"title":"第零篇 概率论相关(1)-先验概率、后验概率与似然","text":"先验概率、后验概率与似然今天看到一个比较好的关于先验、后验和似然的通俗解释，先验概率就是基于历史数据的统计经验，后验概率是在已知结果发生时推断原因的概率，似然概率是已知原因推断结果的概率。 根据上述解释，假设我们有一个数据集，这个数据集服从某一种分布，也可以理解为是一个黑盒子模型，黑盒子模型里面包含了很多参数，则似然概率就是已知参数得到某样本的概率，后验概率就是已知某样本得到参数的概率。 为了更理解这一概念，再来看一下著名的贝叶斯公式：$$P(\\theta \\mid x)=\\frac{p(x \\mid \\theta) p(\\theta)}{p(x)}$$其中$p(\\theta)$是先验概率， $P(\\theta \\mid x)$是后验概率，$p(x \\mid \\theta)$是似然函数。 这里区分一下两个概念，对于$p(x \\mid \\theta)$如果$\\theta$已知且不变，x是变量，则此函数称为概率函数，而如果x已知且保持不变，$\\theta$是变量，则此函数称为似然函数。 最大似然估计(MLE)最大似然，也就是说要让似然最大，则在数据集上的学习过程就是求模型参数使得当前观察到的样本概率最大，所以最大似然估计的目的就是根据已知的样本结果，反推最有可能导致这个结果的参数值。最大似然估计的适用场景是”模型已定、参数未知”，一个重要前提是样本集中的样本都是独立同分布的随机变量，因为只有独立同分布，样本集的似然函数才能等于各样本似然函数的乘积。 假设一个用于学习的样本集是：$D=\\left{x_{1}, x_{2}, \\cdots, x_{N}\\right}$，来估计参数向量θ，则$l(\\theta)=p(D \\mid \\theta)=p\\left(x_{1}, x_{2}, \\cdots, x_{N} \\mid \\theta\\right)=\\prod_{i=1}^{N} p\\left(x_{i} \\mid \\theta\\right)$，则使得似然函数最大的参数值求解过程为：$$\\hat{\\theta}=\\arg \\max _{\\theta} l(\\theta)=\\arg \\max {\\theta} \\prod{i=1}^{N} p\\left(x_{i} \\mid \\theta\\right)$$ 最大后验估计(MAP)最大后验估计与最大似然估计的不同之处在于最大后验估计中引入了先验概率，因此结合贝叶斯公式和最大似然估计，最大后验估计就转化为了：$$\\operatorname{argmaxp}(\\theta \\mid X)=\\operatorname{argmax} \\frac{p(X \\mid \\theta) p(\\theta)}{p(X)}=\\operatorname{argmaxp}(X \\mid \\theta) p(\\theta)=\\operatorname{argmax}\\left(\\prod_{x 1}^{x n} p(x i \\mid \\theta)\\right) p(\\theta)$$L2正则就是加入了高斯先验，L1正则就是加入了拉普拉斯先验。 贝叶斯估计在MLE和MAP中，都是假设模型参数$\\theta$未知，但都是固定的值，属于点估计，而在贝叶斯估计中，假设模型参数是未知的随机变量，而不是确定值，最终得到的参数不是具体的值，而是一个分布，然后用这个分布的期望来作为最终的参数值。 总结最后让我们用大佬讲义中的片段总结一下本篇的主要内容： 参考资料贝叶斯估计、最大似然估计、最大后验概率估计 极大似然估计、最大后验估计","link":"/2019/01/07/probability/"},{"title":"第十八篇 python(4)-多进程","text":"协程、线程与进程协程","link":"/2021/09/22/python-3/"},{"title":"python(5) subprocess and logging","text":"subprocessYou can use the Python subprocess module to create new processes, connect to their input and output, and retrieve their return codes and/or output of the process. subprocess runThe subprocess.run() method is a convenient way to run a subprocess and wait for it to complete. Once the subprocess is started, the run() method blocks until the subprocess completes and returns a CompletedProcess object, which contains the return code and output of the subprocess.The check argument is an optional argument of the subprocess.run() function in the Python subprocess module. It is a boolean value that controls whether the function should check the return code of the command being run.When check is set to True, the function will check the return code of the command and raise a CalledProcessError exception if the return code is non-zero. The exception will have the return code, stdout, stderr, and command as attributes. subprocess Popensubprocess.Popen is a lower-level interface to running subprocesses, while subprocess.run is a higher-level wrapper around Popen that is intended to be more convenient to use. Popen allows you to start a new process and interact with its standard input, output, and error streams. It returns a handle to the running process that can be used to wait for the process to complete, check its return code, or terminate it.In general, you should use run if you just need to run a command and capture its output and Popen if you need more control over the process, such as interacting with its input and output streams.The Popen class has several methods that allow you to interact with the process, such as communicate(), poll(), wait(), terminate(), and kill(). subprocess callsubprocess.call() is a function in the Python subprocess module that is used to run a command in a separate process and wait for it to complete. It returns the return code of the command, which is zero if the command was successful, and non-zero if it failed.subprocess.call() is useful when you want to run a command and check the return code, but do not need to capture the output. subprocess check_outputcheck_output is a function in the subprocess module that is similar to run(), but it only returns the standard output of the command, and raises a CalledProcessError exception if the return code is non-zero. Subprocess PipeA pipe is a unidirectional communication channel that connects one process’s standard output to another’s standard input. A pipe can connect the output of one command to the input of another, allowing the output of the first command to be used as input to the second command.Pipes can be created using the subprocess module with the Popen class by specifying the stdout or stdin argument as subprocess.PIPE. loggingLogging provides a set of convenience functions for simple logging usage. These are debug(), info(), warning(), error() and critical().The default level is WARNING, which means that only events of this level and above will be tracked, unless the logging package is configured to do otherwise. logging configlogging.basicConfig(format=’%(levelname)s %(asctime)s %(process)d %(message)s’, level=logging.DEBUG) referencehttps://www.datacamp.com/tutorial/python-subprocesshttps://docs.python.org/3/howto/logging.html","link":"/2023/06/28/python-4/"},{"title":"第九篇 python(1)-语法进阶","text":"yieldyield可以暂停一个函数的运行，返回值给函数调用者，并使得函数可以从上次离开的地方接着运行。通常我们可以借助yield来实现一个生成器。 生成器生成器是一个可以产生一个序列的函数，调用生成器函数会产生一个生成器对象，并不会开始启动函数，只有当执行__next__()时函数才会执行。生成器时一个一次性操作，和我们常见的列表、字典等可以迭代的对象不同，列表等是可以无限次迭代的。 装饰器python中函数是一等对象，所以函数可以像普通变量一样当作参数传给另一个函数的，装饰器可以在不改变另一个函数的情况下用来封装另一个函数以拓展这个被封装函数的功能，有了装饰器，我们就可以抽离出大量与函数功能本身无关的雷同代码到装饰器中并继续重用。 装饰器不仅可以是函数，也可以是类。使用类装饰器主要依靠类的__call__方法。我们可以直接定义一个装饰器函数或者装饰器类，但是有个缺点是原函数的元信息不见了，比如函数的docstring、__name__都会发生改变，此时我们可以使用functools.wraps，wraps本身也是一个装饰器，它能把原函数的元信息拷贝到装饰器里面的函数中。","link":"/2021/07/30/python/"},{"title":"thoughts","text":"","link":"/2022/03/13/thoughts/"},{"title":"第五篇 算法(1)--排序","text":"排序排序方法选择排序选择排序的思路是找到数组中最小的数和第一个元素交换位置，然后在剩下的元素中找到最小的元素和第二个元素交换位置，直到最后只剩一个元素，这样就是一个从小到大排好序的数组。选择排序的复杂度为O(n^2)。 插入排序插入排序的思路是将元素插入一个已经排好序的子列表中，直到整个列表都排序完成。插入排序的复杂度为O(n^2)。 冒泡排序冒泡排序的思路是每次对连续的邻居元素进行比较，两个元素是逆序排序就交换位置，否则保持不变，直到所有元素都被排序好。每次进行完一轮比较，就能将最大或者最小的元素移到其最终的位置上，当不再发生交换的时候，就说明元素已经被排好序了，冒泡排序的复杂度是O(n^2)。 归并排序归并排序的思路是利用递归的方法，将数组分为两半，各自进行归并排序的过程。其关键是如何将排好序的两个子数组也排好序，鉴于两个子数组是已经排好序了的，只需要将两个子数组依次比较。归并排序的复杂度是O(nlogn）。 快速排序快速排序的思路是挑出一个中心点，把数组分为两半，其中一半所有元素都小于这个中心点，另一半大于这个中心点，再对这两半进行递归处理，所以快速排序的关键在于这个中心点的选择了。快速排序的复杂度是O(nlogn）。 堆排序堆排序用了二叉堆，将一个数组中的所有元素添加到堆中，然后将堆中最大的元素连续移除以获得一个排好序的数组。一个二叉堆具有如下性质：是一个完全二叉树；每个节点都大于或等于它的子节点，二叉堆通常是用数组实现的，父母节点和子节点的位置满足一定的关系，假如一个在位置i的节点，它的左子节点就在位置2i+1上，右子节点在位置2i+2上。所以堆排序的关键在于二叉堆的建立和维护。堆排序的复杂度是O(nlogn)。 桶排序和基数排序桶排序和基数排序用于排序整数非常有效。 桶排序 ​ 桶排序的思路是加入数组中的元素在0到t的范围内，则把这些元素放入对应的标记上0到t的桶当中，每个桶中的元素值都是相同的。 基数排序 在桶排序中，如果元素范围过大的话，就会需要很多桶，此时就可以用基数排序。基数排序基于桶排序，只是基数排序只会用到十个桶，基于基数位置进行桶排序。 外排序当数据量大到无法一次性载入内存时，使用外排序。外排序的思路就是将大量数据拆分成小块数据，小块数据进行内排序之后，再分别合并排序。 相关java基础数组数组一旦创建了，大小就是固定的。java中声明数组变量的语法是’elementType[] arrayRefVar;’，声明数组只是创造了一个数组引用的存储位置，并没有为这个数组分配内存，创建一个数组可以使用new操作符，例如’array RefVar = new elementType[arraySize];’。 数组的拷贝数组变量是一个数组的引用，直接使用赋值语句只是让两个变量去指向同一个数组（同一片存储空间），要想真正地拷贝数组有几种方式：使用循环对数组中的元素一个一个地拷贝；使用System类中的静态方法arraycopy；使用clone方法。arraycopy的语法是’arraycopy(sourceArray,srcPos,targetArray,tarPos,length);’。","link":"/2020/04/04/sort/"},{"title":"toolnotes","text":"背景记录平时学习和开发工程中使用工具的一些备忘点。 正文用vim时，鼠标右键不能粘贴而是进入了visual模式，解决方法：：set mouse-=a 远程jupyter配置https://juejin.cn/post/7026371559971520525 For Debian / Ubuntu: .deb packages installed by apt and dpkgFor Rocky / Fedora / RHEL: .rpm packages installed by yumFor FreeBSD: .txz packages installed by pkg ssh-keygen -t rsa -b 4096 -C “your_email@example.com“ Kill PyTorch Distributed Training Processes:kill $(ps aux | grep YOUR_TRAINING_SCRIPT.py | grep -v grep | awk ‘{print $2}’) git reset –soft HEAD^ 撤销commitgit reset –hard HEAD^ 撤销add remote develophttps://devblogs.microsoft.com/python/remote-python-development-in-visual-studio-code/ references”https://leimao.github.io/blog/Kill-PyTorch-Distributed-Training-Processes/","link":"/2023/01/03/toolnotes/"},{"title":"speech embedding","text":"Contrastive Predictive CodingContrastive Predictive Coding (CPC) learns self-supervised representations by predicting the future in latent space by using powerful autoregressive models. The model uses a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It describes a form of unidirectional modeling in the feature space,where the model learns to predict the near future frames inan acoustic sequence while contrasting with frames from othersequences or frames from a more distant time. Autoregressive Predictive CodingThe APC approach uses an autoregressive model to encodetemporal information of past acoustic sequence; the model thenpredicts future frames like a recurrent-based LM whileconditioning on past frames. TERATERA, which stands for Transformer Encoder Representations from Alteration, is a self-supervised speech pre-trainingmethod. experiment designAmount of labeled data needed to perform well.with pre-trained and without pre-trained.","link":"/2023/05/05/speech-embedding/"},{"title":"speech","text":"signalspectrogram: A spectrogram of a time signal is a special two-dimensional representation that displays time in its horizontal axis and frequency in its vertical axis. short-time Fourier analysisWhy use it?Some regions of speech signals shorter than 100 milliseconds often appear to be periodic, so that we can use the exact definition of Fouriertransform. spectral leakageThis phenomenon is called spectral leakage because the amplitude of one harmonic leaks over the rest and masks its value. feature extractionRepresentation of speech signals in the frequency domain is especially useful because the frequency structure of a phoneme is generally unique. Sinusoids are important because speech signals can be decomposed as sums of sinusoids. For voiced sounds there is typically more energy at low frequenciesthan at high frequencies, also called roll-off. To make the spectrograms easier to read, sometimes the signal is first preemphasized (typically with a first-order difference FIR filter) to boost the high frequenciesto counter the roll-off of natural speech. Digital SystemsLinear Time-Invariant Systems and Linear Time-Varying Systems. The Fourier TransformZ-Transformdigital filterfilterbankA filterbank is a collection of filters that span the whole frequency spectrum. short-time analysis","link":"/2023/03/31/speech/"},{"title":"time complexity","text":"time complexitybig O notationBig O notation measures the asymptotic growth of a function. f (n) = O(g(n)) if for all sufficiently large n, f (n) is at most a constant factor larger than g(n). Ω and Θ notationWe say f (n) = Ω(g(n)) if g(n) = O(f (n)).We say f (n) = Θ(g(n)) if f (n) = O(g(n)) and g(n) = O(f (n)). types of complexityWorst-case complexity: what is the largest possible running time on any input of size n?Average-case complexity: what is the average running time on a random input of size n?Best-case complexity: what is the smallest possible running time on any input of size n? Graph algorithmways of representing graphsadjacency matrixFor graph with n vertices this is an n × n matrix A, where $A_{ij}$ = 1 if there is an edge from node i to node j, $A_{ij}$ = 0 otherwise.If the graph is undirected, the matrix is symmetric adjacency listsFor each vertex, keep a list of its neighbors. incidence matrixThe incidence matrix of an undirected graph with n vertices and m edges is an n × m matrix B where $B_{ij}$ = 1 if the i’th vertex is part of the j’th edge, $B_{ij}$ = 0 otherwise. Two fundamental graph exploration algorithmsDepth First Search (DFS)Breadth First Search (BFS)For the BFS tree, this gives the shortest (fewest number of steps) paths from s to all other nodes Greedy AlgorithmsGreedy algorithms are algorithms that build a solution step by step by always choosing the currently best option. Interval SchedulingInput: A list of intervalsOutput: Maximum number of these intervals that can be chosen without getting any overlaps.Solution:Pick the one that ends first.Prove correctness of such an algorithm: Common strategy for analyzing greedy algorithms: prove that the algorithm always “stays ahead” of the optimal solution. Job Scheduling With Minimum LatenessInput: A list of jobs, each job has a deadline di, and a duration ti(how long it takes to finish the job)Output: Smallest possible maximum lateness in a schedule for doingall jobs.Solution: Pick the job with smallest di. Shortest pathIt is helpful to instead consider a more general problem. Let us tryto find the shortest paths from s to all other vertices: Dijkstra’s algorithm: we have some set D of vertices we have foundthe shortest path to, and each step we add a new vertex to D. add the vertex outside D which is closest tos when using only vertices in D as intermediate vertices. Divide &amp; ConquerAlgorithms that split the input into significantly smaller parts, recursively solves each part, and then combines the subresults (somehow). Merge sortO(n log n). Polynomial multiplication$T(n) = O(n^{1.59})$.Using FFT, get time O(n log n) for Polynomial Multiplication Unit cost model and Bit cost modelUnit cost model: assume all numbers fit in machine registers so that basic arithmetic operations take constant time.Bit cost model: account for size of numbers and the time it takes to manipulate them. Integer multiplicationKaratsuba’s algorithm: $T(n) = O(n^{1.59})$. Master TheoremDynamic ProgrammingSplit a problem into smaller subproblems such that results from onesubproblem can be reused when solving others Fibonacci numbersThe Fibonacci numbers are a classic number sequence inmathematics, defined by the linear recurrencef0 = 0; f1 = 1; and fn = fn−1 + fn−2 for n ≥ 2 Weighted Interval SchedulingInput: A list of intervals [s1; t1]; [s2; t2]; : : : ; [sn; tn], each interval[si; ti] has a weight wiOutput: What is the maximum total weight of these intervals thatcan be chosen without getting any overlaps KnapsackInput: A capacity C and a list of objects, each object has a value viand weight wiOutput: Subset S of objects such that$\\sum_{i∈S} wi ≤ C$ and $\\sum_{i∈S} vi$ is maximized. top-down and bottom-up fashiontop-down fashion: we start at the end result andrecursively compute results for relevant subproblems. bottom-up fashion: we iteratively compute results for larger and larger subproblems. Characteristics of dynamic programmingA problem is amenable to dynamic program if we can define a setof subproblems such that: The number of different subproblems is as small as possible. There is some ordering of subproblems from “small” to “large” The value of a subproblem can be efficiently computed giventhe values of some set of smaller subproblems. Sequence AlignmentInput: Strings x and y of lengths m and n, parameters ‹ and ¸Output: Minimum cost of an alignment of x and y with parameters $\\sigma$ and $\\alpha$.$\\alpha is the cost of aligning two different characters with each other$\\sigma$ is the cost of not aligning a character Matrix Chain MultiplicationNetwork FlowThe Max-Flow problemInput: Flow network G.Output: Flow f maximizing the value v(f ).Solution: The Ford-Fulkerson Algorithm O(C(m + n)) or the scaling algorithm with O(m2log(C)) or Edmonds-Karp algorithm with O(nm(n + m)) . Edge CutsAn edge cut of a graph is a set of edges such that their removal would disconnect the graph. Minimum s-t-CutInput: A flow network G with source s and sink t.Output: An s-t cut A; B of G minimizing the capacity c(A; B). The Max-Flow-Min-Cut TheoremFor every flow network G, the maximum flow from s to t equals theminimum capacity of an s-t cut in G. Vertex CutsA vertex cut in a graph is a set of vertices such that if we removethem, the graph splits into more than one connected component. MatchingsA matching in a graph is a set M of edges such that no vertex appears in more than one edge of M.Of particular interest to us will be bipartite graphs. Maximum Bipartite MatchingInput: A bipartite graph GOutput: A matching M in G of maximum possible size. Edge-Disjoint PathsGiven a directed graph with source and sink, what is maximumnumber of edge-disjoint paths from s to t?(edge-disjoint = no edge used by more than one path) Project Selection?","link":"/2023/05/16/time-complexity/"},{"title":"image task","text":"Object detectionObject detection is the field of computer vision that deals with the localization and classification of objects contained in an image or video.Deep learning-based approaches use neural network architectures like RetinaNet, YOLO (You Only Look Once), CenterNet, SSD (Single Shot Multibox detector), Region proposals (R-CNN, Fast-RCNN, Faster RCNN, Cascade R-CNN) for feature detection of the object, and then identification into labels.The YOLO series current provide the SOTA of object detection in real-time. metricMean average precision (mAP).The AP metric is based on precision-recall metrics. The AP metric incorporates the Intersection over Union (IoU) measure to assess the quality of the predicted bounding boxes.This metric provides a balanced assessment of precision and recall by considering the area under the precision-recall curve. Intersection over Union(IoU)IoU is the ratio of the intersection area to the union area of the predicted bounding box and the ground truth bounding box. It measures the overlap between the ground truth and predicted bounding boxes. Model HistoryTraditionally, object detection is done by Viola Jones Detector \\cite{viola2001rapid}, Histogram of Oriented Gradients (HOG) detector, or Deformable Part-based Model (DPM) before deep learning took off. With deep learning, object detection generally is categorized into 2 categories: one-stage detector and two-stage detector. Two-stage detector is started by Regions with CNN features (RCNN). Spatial Pyramid Pooling Networks (SPPNet), Fast RCNN, Faster RCNN, and Feature Pyramid Networks (FPN) were proposed after it. Limited by the poor speed of the two-stage detector, the one-stage detector came with the first representative You Only Look Once (YOLO). Subsequent versions of YOLO, Single Shot MultiBox Detector (SSD), RetinaNet, CornerNet, CenterNet,DETR were proposed latter. YOLOv7 performs best compared to most detectors. RCNNThe object detection system consists of three modules. The first generates category-independent region proposals. These proposals define the set of candidate detections available to our detector. The second module is a large convolutional neural network that extracts a fixed-length feature vector from each region. The third module is a set of classspecific linear SVMs. YOLO seriesThe history of YOLO (You Only Look Once) dates back to 2015 when the original YOLO algorithm was introduced in “You Only Look Once: Unified, Real-Time Object Detection,” .The original YOLO architecture used a convolutional neural network (CNN) to process the entire image and output a fixed number of bounding boxes along with their associated class probabilities. It divided the image into a grid and applied convolutional operations to predict bounding boxes within each grid cell, considering multiple scales and aspect ratios.In subsequent years, YOLO underwent several iterations and improvements to enhance its accuracy and speed. YOLOv2 was introduced in 2016, featuring an updated architecture that incorporated anchor boxes and multi-scale predictions. YOLOv3 followed in 2018, introducing further advancements, including feature pyramid networks (FPN) and Darknet-53 as the backbone architecture. YOLO (You Only Look Once)Network architecture is inspired by the GoogLeNet model for image classification.The network has 24 convolutional layers followed by 2 fully connected layers. They pretrain our convolutional layers on the ImageNet 1000-class competition dataset.For pretraining they use the first 20 convolutional layers followed by a average-pooling layer and a fully connected layer. Then they add four convolutional layers and two fully connected layers with randomly initialized weights. The final layer predicts both class probabilities and bounding box coordinates. They optimize for sum-squared error in the output of the model by increasing the loss from bounding box coordinate predictions and decreasing the loss from confidence predictions for boxes that don’t contain objects and predicting the square root of the bounding box width and height instead of the width and height directly. They design the loss to handle the problem that the sum-squared error weights localization error equally with classification error and also equally weights errors in large boxes and small boxes. GoogLeNetGoogLeNet was designed to address the challenges of deep neural networks, such as computational efficiency and overfitting, while maintaining high accuracy in image classification tasks. It introduced several novel concepts and architectural innovations that made it stand out from previous CNN architectures at the time. The key feature of GoogLeNet is the Inception module, which utilizes parallel convolutional filters of different sizes (1x1, 3x3, 5x5) to capture features at various scales. This allows the network to learn and represent both local and global features effectively. Additionally, it incorporates 1x1 convolutions for dimensionality reduction and introduces a technique called “bottleneck” layers to reduce the computational complexity. InceptionIn the context of computer vision, “inception” refers to the Inception module or the Inception architecture used in deep convolutional neural networks (CNNs). The Inception module was introduced in the GoogLeNet architecture (also known as Inception-v1) as a key component for efficient and effective feature extraction. The Inception module aims to capture multi-scale features by employing multiple parallel convolutional filters of different sizes within the same layer. By using a combination of 1x1, 3x3, and 5x5 convolutional filters, the Inception module allows the network to learn and extract features at various spatial scales. Non-Maximum Suppression (NMS)Non-Maximum Suppression (NMS) is a post-processing technique used in object detection algorithms to reduce the number of overlapping bounding boxes and improve the overall detection quality. Image segmentationImage segmentation is a sub-domain of computer vision and digital image processing which aims at grouping similar regions or segments of an image under their respective class labels. Semantic segmentationSemantic segmentation refers to the classification of pixels in an image into semantic classes. Instance segmentationInstance segmentation models classify pixels into categories on the basis of “instances” rather than classes. Panoptic segmentationPanoptic segmentation can be expressed as the combination of semantic segmentation and instance segmentation where each instance of an object in the image is segregated and the object’s identity is predicted. Neural networks that perform segmentation typically use an encoder-decoder structure where the encoder is followed by a bottleneck and a decoder or upsampling layers directly from the bottleneck (like in the FCN). Important networks in the history of computer visionLeNet-5LeNet-5, developed by Yann LeCun et al. in 1998, was one of the first successful convolutional neural networks (CNNs) for handwritten digit recognition. It laid the foundation for modern CNN architectures and demonstrated the power of deep learning in computer vision tasks. “Gradient-Based Learning Applied to Document Recognition” by Yann LeCun et al. (1998). AlexNetAlexNet, introduced by Alex Krizhevsky et al. in 2012, was a breakthrough CNN architecture that won the ImageNet competition and popularized deep learning in computer vision. It demonstrated the effectiveness of deep CNNs for image classification tasks and paved the way for subsequent advancements.”ImageNet Classification with Deep Convolutional Neural Networks” by Alex Krizhevsky et al. (2012). VGGNetThe VGGNet, proposed by Karen Simonyan and Andrew Zisserman in 2014, is known for its simplicity and depth. It consisted of deep networks with stacked 3x3 convolutional layers, showing that increasing network depth led to improved performance on image classification tasks.”Very Deep Convolutional Networks for Large-Scale Image Recognition” by Karen Simonyan and Andrew Zisserman (2014). GoogLeNet (Inception-v1)GoogLeNet, presented by Christian Szegedy et al. in 2015, introduced the Inception module and demonstrated the importance of multi-scale feature extraction. It achieved high accuracy while maintaining computational efficiency, inspiring subsequent Inception versions and influencing network designs.“Going Deeper with Convolutions” by Christian Szegedy et al. (2015). ResNetResNet, developed by Kaiming He et al. in 2015, introduced the concept of residual learning. It utilized skip connections or shortcuts to address the vanishing gradient problem and enabled training of extremely deep networks, leading to significant performance gains in image classification and other tasks.”Deep Residual Learning for Image Recognition” by Kaiming He et al. (2015). DenseNet DenseNet, introduced by Gao Huang et al. in 2016, focused on dense connectivity patterns between layers. It aimed to alleviate the vanishing gradient problem, promote feature reuse, and encourage better gradient flow. DenseNet achieved competitive results while reducing the number of parameters compared to other architectures. “Densely Connected Convolutional Networks” by Gao Huang et al. (2016).’ Basic architectureCNNConvolutional networks are simply neural networks that use convolution in place of general matrix multiplication in at least one of their layers.Convolution leverages three important ideas that can help improve a machine learning system: sparse interactions, parameter sharing and equivariant representations. Moreover, convolution provides a means for working with inputs of variable size. We assume that the size of the input image is nn, and the size of the filter is ff (note that f is generally an odd number). The size of the output image after convolution is (n-f+1)* (n-f+1).During the convolution process, padding is sometimes necessary to avoid information loss. Additionally, adjusting the stride allows for compression of some information.If we want to perform convolution on a three-channel RGB image, the corresponding filter group would also have three channels. The process involves convolving each individual channel with its corresponding filter, summing up the results, and then adding the sums of the three channels together. The resulting sum of the 27 multiplications is considered as one pixel value of the output image. The filters for different channels can be different. When the input has specific height, width, and channel dimensions, the filters can have different height and width, but the number of channels must match the input.Pooling layers are commonly included in many CNNs. The purpose of pooling layers is to reduce the size of the model, improve computational speed, and simultaneously decrease noise to enhance the robustness of the extracted features. referenceshttps://zhuanlan.zhihu.com/p/183261974https://juejin.cn/post/7104845694225088525https://www.showmeai.tech/article-detail/221","link":"/2023/06/09/image-task/"}],"tags":[{"name":"deep learning","slug":"deep-learning","link":"/tags/deep-learning/"},{"name":"tools","slug":"tools","link":"/tags/tools/"},{"name":"machine learning","slug":"machine-learning","link":"/tags/machine-learning/"},{"name":"Markdown","slug":"Markdown","link":"/tags/Markdown/"},{"name":"reinforcement learning","slug":"reinforcement-learning","link":"/tags/reinforcement-learning/"},{"name":"Algorithm","slug":"Algorithm","link":"/tags/Algorithm/"},{"name":"algorithm","slug":"algorithm","link":"/tags/algorithm/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"KTH","slug":"KTH","link":"/tags/KTH/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"SQL","slug":"SQL","link":"/tags/SQL/"},{"name":"git","slug":"git","link":"/tags/git/"},{"name":"java","slug":"java","link":"/tags/java/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"computer vision","slug":"computer-vision","link":"/tags/computer-vision/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"probability","slug":"probability","link":"/tags/probability/"},{"name":"speech","slug":"speech","link":"/tags/speech/"}],"categories":[{"name":"theory","slug":"theory","link":"/categories/theory/"},{"name":"algorithm","slug":"algorithm","link":"/categories/algorithm/"},{"name":"others","slug":"others","link":"/categories/others/"},{"name":"database","slug":"database","link":"/categories/database/"},{"name":"development tools","slug":"development-tools","link":"/categories/development-tools/"},{"name":"programming language","slug":"programming-language","link":"/categories/programming-language/"},{"name":"computer system","slug":"computer-system","link":"/categories/computer-system/"},{"name":"web development","slug":"web-development","link":"/categories/web-development/"}]}