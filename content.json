{"posts":[{"title":"第十九篇 深度学习(1)-梯度下降","text":"","link":"/2021/09/25/DL/"},{"title":"Elasticsearch","text":"ELK StackElasticsearch, Logstash and Kibana ElasticsearchElasticsearch is a NoSQL database.When you feed data into Elasticsearch, the data is placed into Apache Lucene indexes. Apache LuceneApache Lucene™ is a high-performance, full-featured search engine library written entirely in Java. APILogstashUsing more than 50 input plugins for different platforms, databases and applications, Logstash can be defined to collect and process data from these sources and send them to other systems for storage and analysis. projecthttps://trecpodcasts.github.io/https://doc.yonyoucloud.com/doc/mastering-elasticsearch/chapter-2/21_README.htmlhttps://cloud.tencent.com/developer/article/1600163https://www.elastic.co/cn/blog/how-to-improve-elasticsearch-search-relevance-with-boolean-querieshttps://www.elastic.co/guide/en/app-search/current/relevance-tuning-guide.htmlhttps://medium.com/mlearning-ai/enhancing-information-retrieval-via-semantic-and-relevance-matching-64973ff81818https://www.elastic.co/cn/blog/how-to-improve-elasticsearch-search-relevance-with-boolean-querieshttps://bigdataboutique.com/blog/optimizing-elasticsearch-relevance-a-detailed-guide-c9efd3NDCG：https://www.javatips.net/api/MyMediaLiteJava-master/src/org/mymedialite/eval/measures/NDCG.java","link":"/2023/04/02/Elasticsearch/"},{"title":"第十三篇 机器学习(2)-朴素贝叶斯","text":"","link":"/2021/08/29/ML-1/"},{"title":"第十四篇 机器学习(3)-支持向量机","text":"","link":"/2021/08/29/ML-2/"},{"title":"第十二篇 机器学习(1)-逻辑回归","text":"逻辑回归逻辑回归名为回归，其实主要是用在分类上。","link":"/2021/08/29/ML/"},{"title":"第二十篇 机器学习(4)-uplift模型","text":"uplift模型uplift模型中文为增益模型，是工业界因果推断与机器学习结合最成熟的算法之一。传统的监督学习模型，往往是对输入x去预测一个y，而增益模型注重于x的变化对y的影响，以广告为例，传统的监督学习往往是给定这个特征去预测用户是否会点击，而增益模型注重的是给这个客户投放广告与否对客户是否购买广告商品所产生的影响。 因果推断","link":"/2021/09/26/ML-4/"},{"title":"第十五篇 机器学习(4)-决策树","text":"","link":"/2021/08/29/ML-3/"},{"title":"第二十二篇 机器学习(5)--不平衡的分类问题","text":"","link":"/2021/11/15/ML-5/"},{"title":"第一篇 Markdown","text":"第一篇 Markdown （Typora）基础语法目录标题篇级数个#加空格，如# 表示后面的文字作为一级标题，## 表示后面的文字作为二级标题。 字体篇加粗：将想要加粗的文字在左右用两个*号括起来 ： **加粗** 斜体：将想要斜体的文字在左右用一个*号括起来 ： 斜体加粗：将想要斜体的文字在左右用三个*号括起来 删除线：将想要加删除线的文字在左右用两个~号括起来波浪线 段落篇 生成目录：[toc]， 但是hexo上传博客用此方法显示目录不成功ORZ。 分割线：三个及以上的-或者*，试一试 无序列表：就是这个前面的小黑点，一个+-或者*加上空格，后面跟上文字。 有序列表：数字标号的。数字加点加上空格 这是第一个 这是第二个 插入类 插入图片：![图片下方的文字](图片地址 &quot;图片标题&quot;) 拿一张试试： 插入超链接：[超链接名称](超链接地址 &quot;标题&quot;) 试试：我的博客 插入公式：$$包含TeX 或 LaTeX 格式的数学公式。 插入表格：|分列，-分割表头和内容，左右两边加：文字居中，只加一边文字偏哪边，默认偏左。 举例 1234表头|表头 --|--内容|内容内容|内容 ​ 试一个：没有成功诶，需要在两端也加上|才会显示表格，可是把最后一行作为表头显示了，emmm，应该是编辑器导致的差异。不过Typora可以很方便地直接右击插入表格。。。 表头 表头 内容 内容 内容 内容 插入代码块 单行代码：一个反引号`括起来，反引号就是Esc下面那个键，记得换成英文输入法。 print('hello world!') 多行代码：三个反引号`括起来。 12import numpy as npimport pandas as pd ​","link":"/2020/03/14/Markdown/"},{"title":"NP and reduction","text":"PP is the set of all decision problems that can be solved inPolynomial time. Extended Church-Turing ThesisAny realistic model of computation can be efficientlysimulated by a Turing machine.(The ECT is probably false!Probable counter-example: Quantum computing,However, the ECT is true for the classic models of computationunderlying our laptops) NP problemWe do not know of polynomial-time algorithms for these problems, and we cannot prove that nopolynomial-time algorithm exists. NP stands for Nondeterministic Polynomial time. NP is a set of problems that there is a polynomial-time algorithm that verify if a solution to the problem is correct.Note that showing that a problem is in NP does not mean that the problem can be solved in polynomial time. It only means that a proposed solution can be verified in polynomial time. All problems in P are also in NP, we do not know if P=NP. NP-hardnessWe say that a problem Y is NP-hard if every problem in NP can be Karp-reduced to Y. To prove NP-hardness for Y we only have to findone other NP-hard problem X and reduce X to Y. NP-complete problemA large class of problems in this “gray area” has been characterized,and it has been proved that they are equivalent in the following sense: a polynomial-time algorithm for any one of them would imply the existence of a polynomial-time algorithm for all of them. These are the NP-complete problems. Every problem in NP can be reduced to X. We will call such an X an NP-complete problem. To prove a problem is np-complete, we need to prove it lies in Np and it is np-hard. In NP is proved by verifying the solution in polynomial time. Np-hard is proved by using Karp-reduction from known NP-hard problem. the NP-complete problems are the hardest problems in NP The Cook-Levin TheoremThe Sat problem is NP-hard. Hamiltonian CycleReduction from Sat to Hamiltonian Cycle. Travelling SalesmanReduction from Hamiltonian Cycle Graph ColoringGraph Coloring is NP-hard.2-coloring is not NP-hard. Vertex Coverwe say that a set of nodes S is a vertex cover if every edge e has at least one end in S. Set CoverGiven a set U of n elements, a collection S1,…, Sm of subsets of U, anda number k, does there exist a collection of at most k of these sets whoseunion is equal to all of U? CoNPThe complexity class CoNP consists of all problems where a “no” answer can be efficiently verified.For every NP-complete problem there is acorresponding CoNP-complete problem. We do not know if NP=CoNP. PSPACEThe complexity class PSPACE consists of all problems that canbe solved by an algorithm using at most a polynomial amount ofspace.It is strongly believed that NP != PSPACE, but we do not evenknow with certainty whether P != PSPACE or not! For many 2-player games like Geography, deciding if there is awinning strategy from a given position is a PSPACE problem BPP and ZPPBPPBPP (Bounded-error Probabilistic Polynomial-time)consists of all decision problems for which there is apolynomial-time randomized algorithms that is correct withprobability at least 2=3 on all instances. ZPPZPP (Zero-error Probabilistic Polynomial Time)consists of all decision problems for which there is a Las Vegasalgorithm running in expected polynomial time.It is widely believed that P = ZPP = BPP but all three could bedifferent.","link":"/2023/03/28/NP/"},{"title":"Neural Networks","text":"training nerual networkssuggestionsThe first step to training a neural net is to not touch any neural net code at all and instead begin by thoroughly inspecting your data. Our next step is to set up a full training + evaluation skeleton and gain trust in its correctness via a series of experiments. At this stage it is best to pick some simple model that you couldn’t possibly have screwed up somehow. Tips &amp; tricks for this stage:Always use a fixed random seed to guarantee that when you run the code twice you will get the same outcome.Make sure to disable any unnecessary fanciness. As an example, definitely turn off any data augmentation at this stage.When plotting the test loss run the evaluation over the entire (large) test set. Do not just plot test losses over batches and then rely on smoothing them in Tensorboard. We are in pursuit of correctness and are very willing to give up time for staying sane.Monitor metrics other than loss that are human interpretable and checkable (e.g. accuracy). Whenever possible evaluate your own (human) accuracy and compare to it.Train an input-independent baseline, (e.g. easiest is to just set all your inputs to zero). This should perform worse than when you actually plug in your data without zeroing it out. Does it? i.e. does your model learn to extract any information out of the input at all? The approach I like to take to finding a good model has two stages: first get a model large enough that it can overfit (i.e. focus on training loss) and then regularize it appropriately (give up some training loss to improve the validation loss). The reason I like these two stages is that if we are not able to reach a low error rate with any model at all that may again indicate some issues, bugs, or misconfiguration. In the early stages of setting baselines I like to use Adam with a learning rate of 3e-4. In my experience Adam is much more forgiving to hyperparameters, including a bad learning rate. referenceshttp://karpathy.github.io/2019/04/25/recipe/","link":"/2023/07/20/Neural-Networks/"},{"title":"My first blog","text":"This is my first blog which is just for test. Happy~I will present my learning process in this blog from today on. Fighting~","link":"/2018/12/21/My-first-blog/"},{"title":"Network Representation Learning","text":"backgroundRecording studying during KTH. First blog about network representation learning, a project belongs to machine learning, advanced course. LINEReproduce paper “LINE: Large-scale Information Network Embedding”. Alias Table MethodIt’s a method of effiently drawing samples from discrete distribution.reference:https://www.keithschwarz.com/darts-dice-coins/https://blog.csdn.net/haolexiao/article/details/65157026 Negative Samplingword2vecOriginal paper:Efficient estimation of word representations in vector space.reference:word2vec Explained: Deriving Mikolov et al.’sNegative-Sampling Word-Embedding Method Skip-Gram ModelOriginal papaer:Distributed Representations of Words and Phrasesand their Compositionality.The idea behind the word2vec models is that the words that appear in the same context (near each other) should have similar word vectors. Therefore, we should consider some notion of similarity in our objective when training the model. This is done using the dot product since when vectors are similar, their dot product is larger.reference:https://www.baeldung.com/cs/nlps-word2vec-negative-sampling graphSage","link":"/2022/12/23/Network-Representation-Learning/"},{"title":"Randomized Algorithms","text":"Randomized Algorithms and approximation algorithmA randomized algorithm is an algorithm that uses randomness tomake some of its choices. Las Vegas AlgorithmsA Las Vegas algorithm is a randomized algorithm that always findsthe correct answer, but the running time of the algorithm mightvary significantly depending on the random choices made. Monte Carlo AlgorithmsA Monte Carlo algorithm is a randomized algorithm where theoutput of the algorithm may be incorrect, but we have a guaranteethat this only happens with small probability. Random Number Generator True RNG: gets randomness from measuring physical phenomena(e.g. background radiation) that we believe is sufficiently randomPseudo-RNG: starting from a small random seed, new numbers aregenerated in a completely deterministic way. Randomized Min-Cut Algorithmwhile G has more than 2 vertices:Pick a uniformly random edge of G and contract it.the total runtime is $O(n^2m)$. However this algorithm can be refined and running time improved to $O(n^2log(n))$ (relatively simple algorithm) or$O(m log^3(n))$ (more complicated algorithm). approximation algorithmThe Maximum Cut ProblemPartition of vertices of G into two non-empty sets A and B such that number of edges between A and B is maximized. It is a NP-hard problem. Let us lower the ambition and try to find a fast algorithm that finds a “reasonably good” cut: Random algorithm:Put each vertex in either A or B independently with probability 1/2.The random assignment algorithm cuts (in expectation) at least half of all the edges in the graph.This is an example of an approximation algorithm. finding “good but maybe not optimal” solutionsHeuristic AlgorithmsWork well in some or even many cases, but with no guaranteesabout how well they perform. Approximation AlgorithmsAlgorithms with provable guarantee that the solution found is relatively good compared to the optimum solution, for all instances.For a minimization problem, an algorithm has approximation ratio $\\alpha ≥ 1$ if for every instance it holds that $ Alg ≤ \\alpha Opt$.For a maximization problem, the inequality goes the other way: we have approximation ratio $\\alpha ≤ 1$ if $ Alg ≥ \\alpha Opt$. alpha-approximation algorithmapproximation ratio: $\\alpha$. Minimum Load BalancingNP-hard problem. Given Lengths t1; : : : ; tn of n jobs to be run on m machines, to find Minimum possible makespan of a schedule for the n jobs.Approximation Algorithm for Load Balancing:Assign job i to the machine j with the smallest load. how to prove itThe main difficulty in analyzing approximationalgorithms is to get some handle on the Opt value. We need to find Opt, but finding Opt is NP-hard, so we find lower bounds on Opt. Minimum Vertex CoverA vertex cover in a graph G is a set of vertices that “touches” every edge of G.What is the size of a minimum vertex cover of G? Approximation Algorithm for Minimum Vertex Cover:while there exists an edge e = (u; v) such that u /∈ S and v /∈ S, add (u,v) into S. The algorithm is a 2-approximation algorithm.“Unique Games Conjecture”: it is known that Vertex Cover cannot be approximated better thanwithin a factor 2. Minimum Set CoverA collection of sets S1; : : : ; Sm ⊆ U over some universe U, What is minimum number of Si’s whose union equals U? Analysis of Greedy Set Cover Finale?","link":"/2023/04/25/Randomized-Algorithms/"},{"title":"第十七篇 强化学习(1)-马尔可夫决策过程","text":"马尔可夫决策过程马尔可夫性质：当前状态可以完全表征过程。 对于任意有限的马尔可夫决策过程，都存在一个最优策略，不差于其他所有可能的策略。 贝尔曼方程","link":"/2021/09/02/RL/"},{"title":"PCA","text":"关于PCA为什么要中心化因为不做zero mean，根本做不了PCA。从线性变换的本质来说，PCA就是在线性空间做一个旋转（数据矩阵右乘协方差矩阵的特征向量矩阵），然后取低维子空间（实际上就是前n_components个特征向量张成的子空间）上的投影点来代替原本的点，以达到降维的目的，注意我说的，只做了旋转，没有平移，所以首先你要保证原本空间里的点是以原点为中心分布的，这就是zero mean的目的。另外如果自己手撸过PCA的算法就知道了，explained_variance_和explained_variance_ratio_是怎么实现的？explained_variance就是协方差矩阵的每个特征值除以sample数，而explained_variance_ratio_是每个特征值除以所有特征值之和。为什么这么简单呢？这也和zero mean有关，如果你用最大投影长度的证法去证明PCA就会在过程中很自然的发现这一点，在这里我就不展开了。 链接：https://www.zhihu.com/question/40956812/answer/848527057 PCA 去中心化不一定是必需的，这一结论成立的前提是严格使用协方差矩阵的定义式 S=XXT−nμμT，而不是用 XXT 来当作协方差矩阵。 Centering is an important pre-processing step because it ensures that the resulting components are only looking at the variance within the dataset, and not capturing the overall mean of the dataset as an important variable (dimension). Without mean-centering, the first principal component found by PCA might correspond with the mean of the data instead of the direction of maximum variance. https://towardsdatascience.com/tidying-up-with-pca-an-introduction-to-principal-components-analysis-f876599af383 Why is minimizing squared residuals equivalent to maximizing variance? Consider a datapoint (row of ). Then the contribution of that datapoint to the variance is , or equivalently the squared Euclidean length . Applying the Pythagorean theorem shows that this total variance equals the sum of variance lost (the squared residual) and variance remaining. Thus, it is equivalent to either maximize remaining variance or minimize lost variance to find the principal components. http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/ PCA can only be interpreted as the singular value decomposition of a data matrix when the columns have first been centered by their means. https://stats.stackexchange.com/questions/29781/when-conducting-multiple-regression-when-should-you-center-your-predictor-varia PCA focuses on “explaining” the data matrix using the sample means plus the eigencomponents. When the column mean is far from the origin, the first right singular value is usually quite highly correlated with column mean - thus using PCA concentrates on the second, third and sometimes higher order singular vectors. This is a loss of information when the mean is informative for the process under study. On the other hand, when the scatterplot of the data is roughly elliptical, the PCs typically align with the major axes of the ellipse. Due to the uncorrelatedness constraint, if the mean is far from the origin, the first singular vector will be close to the mean and the others will be tilted away form the major axes of the ellipse. Thus the first singular vector will not be informative about the spread of the data, and the second and third singular values will not be in the most informative directions. Generally, PCA will be more informative, particularly as a method for plotting the data, than uncentered SVD. https://online.stat.psu.edu/stat555/node/94/ Since X is zero centered we can think of them as capturing the spread of the data around the mean in a sense reminiscent of PCA. https://intoli.com/blog/pca-and-svd/ that reconstruction error is minimized by taking as columns of W some k orthonormal vectors maximizing the total variance of the projection. https://stats.stackexchange.com/questions/130721/what-norm-of-the-reconstruction-error-is-minimized-by-the-low-rank-approximation PCA is a regressional model without intercept1. Thus, principal components inevitably come through the origin. If you forget to center your data, the 1st principal component may pierce the cloud not along the main direction of the cloud, and will be (for statistics purposes) misleading.https://stats.stackexchange.com/questions/22329/how-does-centering-the-data-get-rid-of-the-intercept-in-regression-and-pca Centering brings in a big difference. PCA with centering maximizes SS deviations from the mean (i.e. variance); PCA on raw data maximizes SS deviations from the zero point.https://stats.stackexchange.com/questions/489037/principal-components-with-and-without-centering?noredirect=1&amp;lq=1 SVD and PCA https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca singular value decomposition SVD is basically a matrix factorization technique, which decomposes any matrix into 3 generic and familiar matrices. Eigenvalues and Eigenvectors The concept of eigenvectors is applicable only for square matrices. The vector space spanned by an eigenvector is called an eigenspace. A square matrix is called a diagonalizable matrix if it can be written in the format: $ A=PDP^{-1} $, D is the diagonal matrix comprises of the eigenvalues as diagonal elements A Symmetric Matrix where the matrix is equal to the transpose of itself. Special properties of a Symmetric Matrix with respect to eigenvalues and eigenvectors:Has only Real eigenvalues;Always diagonalizable;Has orthogonal eigenvectors. A matrix is called an Orthogonal Matrix if the transpose of the matrix is the inverse of that matrix.ince the eigenvectors of a Symmetric matrix are orthogonal to each other, matrix P in the diagonalized matrix A is an orthogonal matrix. So we say that any Symmetric Matrix is Orthogonally Diagonalizable. https://towardsdatascience.com/singular-value-decomposition-and-its-applications-in-principal-component-analysis-5b7a5f08d0bd % For the PCA derived from maximal preserved variance \\cite{lee2007nonlinear}, we have the covariance% of $\\mathbf{y}$, which is% \\begin{equation}% \\mathbf{C}_{\\mathbf{y} \\mathbf{y}}=E\\left{\\mathbf{y} \\mathbf{y}^T\\right}% \\end{equation}% This equation is valid only when $\\mathbf{y}$ is centered. The goal of PCA is to maximize the variance of the data along each of the principal components. Centering is an important step because it ensures that the resulting components are only looking at the variance of features, and not capturing the means of the features as important. Without mean-centering, the first principal component found by PCA might correspond with the mean of the data instead of the direction of maximum variance.","link":"/2022/12/29/PCA/"},{"title":"Regular Expressions","text":"Regular expressionsA formal language for specifying text strings rulesDisjunctions:Letters inside square brackets[]: [A-Z]pipe |: a|b|cNegation in Disjunction: [^Ss] ?: When placed after a character or a group, the question mark makes it optional, meaning that the character or group can occur zero or one time.When placed after a quantifier, such as *, +, or ?, it modifies the quantifier to be non-greedy or lazy. A non-greedy quantifier matches as few characters as possible, while a greedy quantifier matches as many characters as possible.*:0 or more of previous char+:1 or more of previous char.:any char Anchors:^: The begining. $: The end.","link":"/2023/07/11/Regular-Expressions/"},{"title":"第十篇 SQL(1)-数据统计","text":"","link":"/2021/08/07/SQL/"},{"title":"Turing Machines and Undecidability","text":"mathematical model of computationTuring MachinesTuring machines are primarily a mathematical construction. A Turing machine (TM) consists of:A finite set of states: initial state(when the TM starts, it is in this state) and accepting states(if the TM ends in one of these states, it says yes); Transition rules that determine what to do based on current state and content of memory tape at current position; An alphabet, the set of symbols that can be stored on the memory tape. The Church-Turing HypothesisAnything which can be computed by any automatic method, can also be computed by a Turing machine. If a model can also do everything that a Turing machine can, then it is called a Turing-complete model of computation. The RAM ModelModels the way modern computers operate, with registers and memory cells that can be immediately accessed. Comparison-Based SortingWe will prove that any such algorithm must make Ω(n log n)comparisons (and thus take Ω(n log n) time) Undecidabilitydecision problemsThe answer is just a single bit of information,“yes” or “no”.A decision problem is decidable if there exists an algorithm for solving the problem without any efficiency considerations. The Halting ProblemTuring machine M, and input x to M. Does M halt when run on the input x?The Halting Problem is undecidable. Halting On Empty InputIt is undecidable. Halting on All InputsIt is undecidable. Recursively Enumerable ProblemsThere exists an “algorithm” which terminates whenever the answer is“yes” but does not terminate on “no” instances.Problems which have algorithms like this are called recursively enumerable. Turing reductionA Turing reduction (also called Cook reduction) from a problemX to a problem Y is an efficient algorithm for problem X which isallowed to assume an algorithm for Y as a black box. Vertex CoverIndependent SetSet Cover3-SatKarp ReductionsX ≤p Y if given any instance A of problem X, we can inpolynomial time construct an instance f (A) of problem Y such thatthe answer to A is the same as the answer to f (A). Turing reduction is a type of reduction that is stronger than Karp reduction. A problem A is Turing-reducible to problem B if there exists a Turing machine that can solve problem A by making a polynomial number of calls to a black-box subroutine for problem B. In other words, if we can use an algorithm for problem B to solve problem A, then A is Turing-reducible to B. Turing reduction preserves the complexity class of a problem, meaning that if A is Turing-reducible to B and B is in a certain complexity class, then A is also in that complexity class. Karp reduction, also known as polynomial-time reduction, is a weaker form of reduction than Turing reduction. A problem A is Karp-reducible to problem B if there exists a polynomial-time algorithm that can transform any instance of problem A into an instance of problem B such that the answer to the transformed instance is the same as the answer to the original instance of problem A. In other words, if we can use an algorithm for problem B to solve a transformed version of problem A in polynomial time, then A is Karp-reducible to B. Karp reduction preserves the complexity class up to polynomial factors, meaning that if A is Karp-reducible to B and B is in a certain complexity class, then A is also in that complexity class up to polynomial factors. Good referencehttps://www.cs.rochester.edu/u/nelson/courses/csc_173/computability/undecidable.htmlhttp://www2.lawrence.edu/fast/GREGGJ/CMSC515/chapt05/Reducibility.htmlhttps://www.cs.princeton.edu/courses/archive/spring05/cos423/lectures.php","link":"/2023/04/25/Turing-Machines/"},{"title":"bigdata - introduction","text":"big dataBig Data is a portfolio of technologies that were designed to store, manage and analyze data that is too large to fit on a single machine while accommodat-ing for the issue of growing discrepancy between capacity, throughput and latency. data independenceData independence means that the logical view on the data is cleanly separated, decoupled, from its physical storage. architectureStack: storage, compute, model ,language data modelwhat data looks like and what you can do with it. data shapestables, trees, cubes tableRow(Tuple), Column(Attribute), Primary Key,Value. relational tablesschema: A set of attributes.extension: A set/bag/list of tuples.Three constraints: Relational integrity, domain integrity and atomic integrity.Superkey, Candidate key(minimal superkey). Database NormalizationIn database management systems (DBMS), normal forms are a series of guidelines that help to ensure that the design of a database is efficient, organized, and free from data anomalies.First Normal Form (1NF):In 1NF, each table cell should contain only a single value, and each column should have a unique name.Second Normal Form (2NF): 2NF eliminates redundant data by requiring that each non-key attribute be dependent on the primary key.Third Normal Form (3NF): 3NF builds on 2NF by requiring that all non-key attributes are independent of each other. DenormalizationDenormalization is a database optimization technique in which we add redundant data to one or more tables. This can help us avoid costly joins in a relational database. Note that denormalization does not mean ‘reversing normalization’ or ‘not to normalize’. It is an optimization technique that is applied after normalization. SQLSQL was originally named SEQUEL, for Structured English QUEry Language. SQL is a declarative language, which means that the user specifies what they want, and not how to compute it: it is up to the underlying system to figure out how to best execute the query. View and TableThe view is a result of an SQL query and it is a virtual table, whereas a Table is formed up of rows and columns that store the information of any object and be used to retrieve that data whenever required. A view contains no data of its own but it is like a ‘window’ through which data from tables can be viewed or changed. The view is stored as a SELECT statement in the data dictionary. Creating a view fulfills the requirement without storing a separate copy of the data because a view does not store any data of its own and always takes the data from a base table. as the data is taken from the base table, accurate and up-to-date information is required. SQL:1999 added the with clause to define “statement scoped views”. They are not stored in the database schema: instead, they are only valid in the query they belong to. This makes it possible to improve the structure of a statement without polluting the global namespace.With is not a stand alone command like create view is: it must be followed by select. Natural Join and Inner JoinNatural Join joins two tables based on the same attribute name and datatypes. The resulting table will contain all the attributes of both the table but keep only one copy of each common column while Inner Join joins two tables on the basis of the column which is explicitly specified in the ON clause. The resulting table will contain all the attributes from both tables including the common column also. data storageStack: Storage, Encoding, Syntax, Data models, Validation, Processing, Indexing, Data stores,Querying, User interfaces. database and data laketwo main paradigms for storing and retrieving data:database and data lake. Data can be imported into the database (this is called ETL, for Extract-Transform-Load. ETL is often used as a verb).The data is internally stored as a proprietary format that is optimized to make queries faster. This includes in particular building indices on the data.On the other hand, data can also just be stored on some file system.This paradigm is called the data lake paradigm and gained a lot of popularity in the past two decades. It is slower, however users can start querying their data without the effort of ETLing. scaling up and scaling outFirst, one can buy a bigger machine: more memory, more or faster CPU cores, a larger disk, etc. This is called scaling up. Second, one can buy more, similar machines and share the work across them. This is called scaling out. Object storesAmazon’s object storage system is called Simple Storage Service, abbreviated S3. From a logical perspective, S3 is extremely simple: objects are organized in buckets. Buckets are identified with a bucket ID, and each object within a bucket is identified with an Object ID. CAP theoremConsistency: at any point in time, the same request to any server returns the same result, in order words, all nodes see the same data;Availability: the system is available for requests at all times with very high availability.Partition tolerance: the system continues to function even if the network linking its machines is occasionally partitioned.The CAP theorem is basically an impossibility triangle: a system cannot guarantee at the same time: usually are CP,AP or AC. REST APIsREST (Representational State Transfer) is an architectural style for designing networked applications.RESTful services often use HTTP as the communication protocol. A client and server communicated with the HTTP protocol interact in terms of methods applied to resources.A resource is referred to with what is called a URI. URI stands for Uniform Resource Identifier. A client can act on resources by invoking methods, with an optional body. The most important methods are: GET, PUT,DELETE,POST. REST is not a standard or protocol, this is an approach to or architectural style for writing API.REST is an architectural style, and RESTful is the interpretation of it. That is, if your back-end server has REST API and you make client-side requests (from a website/application) to this API, then your client is RESTful. All requests you make have their HTTP status codes. There are a lot of them and they are divided into 5 classes. The first number indicates which of them a code belongs to:1xx - informational2xx - success3xx - redirection4xx - client error5xx - server error Amazon S3 and Azure Blob Storage Key-value storeA key-value store differs from a typical relational database in three aspects: • Its API is considerably simpler than that of a relational database (which comes with query languages) • It does not ensure atomic consistency; instead, it guarantees eventual consistency, which we covered earlier in this Chapter. • A key-value store scales out well, in that it is very fast also at large scales. Amazon DynamoIt is itself based (with some modifications) on the Chord protocol, which is a Distributed Hash Table.On the physical level, a distributed hash table is made of nodes (the machines we have in a data center, piled up in racks) that work following a few design principles. The first design principle is incremental stability. This means that new nodes can join the system at any time, and nodes can leave the system at any time, sometimes gracefully, sometimes in a sudden crash.The second principle is symmetry: no node is particular in any way The third principle is decentralization: there is no “central node” that orchestrates the others.The fourth principle is heterogeneity: the nodes may have different CPU power, amounts of memory, etc. A central aspect of the design of a distributed hash table, and part in particular of the Chord protocol, is that every logical key is hashed to bits that we will call IDs. In the case of Dynamo, the hash is made of 128 bits (7 bytes).In the chord protocol, a technology called a finger table is used. Each node knows the next node clockwise, and the second node, and the 4th node, and the 8th node. Dynamo changes this design to so-called “preference lists”: each node knows, for every key (or key range), which node(s) are responsible (and hold a copy) of it. This is done by associating every key (key range) with a list of nodes, by decreasing priority (going down the ring clockwise). Distributed hash tables, including Dynamo, are typically AP. A fundamental conceptual tool in AP systems is the use of vector clocks.Vector clocks are a way to annotate the versions when they follow a DAG structure.A vector clock can logically be seen as a map from nodes (machines) to integers, i.e., the version number is incremented per machine rather than globally. vector clockLamport’s logical clockLamport’s Logical Clock was created by Leslie Lamport. It is a procedure to determine the order of events occurring. It provides a basis for the more advanced Vector Clock Algorithm. Due to the absence of a Global Clock in a Distributed Operating System Lamport Logical Clock is needed. Implementation Rules：[IR1]: If a -&gt; b [‘a’ happened before ‘b’ within the same process] then, Ci(b) =Ci(a) + d[IR2]: Cj = max(Cj, tm + d) [If there’s more number of processes, then tm = value of Ci(a), Cj = max value between Cj and tm + d] Vector Clocks in Distributed SystemsVector Clock is an algorithm that generates partial ordering of events and detects causality violations in a distributed system.How does the vector clock algorithm work : Initially, all the clocks are set to zero.Every time, an Internal event occurs in a process, the value of the processes’s logical clock in the vector is incremented by 1.Every time, a process receives a message, the value of the processes’s logical clock in the vector is incremented by 1, and moreover, each element is updated by taking the maximum of the value in its own vector clock and the value in the vector in the received message (for every element). To sum up, Vector clocks algorithms are used in distributed systems to provide a causally consistent ordering of events but the entire Vector is sent to each process for every message sent, in order to keep the vector clocks in sync. partial order relationNote that vector clocks can be compared to each other with a partial order relation ≤. A partial order relation is any relation that is reflexive, antisymmetric, and transitive. A total order relation is a partial order in which every element of the set is comparable with every other element of the set. All total order relations are partial order relations, but the converse is not always true. referenceshttps://www.geeksforgeeks.org/normal-forms-in-dbms/https://www.geeksforgeeks.org/denormalization-in-databases/https://www.geeksforgeeks.org/difference-between-view-and-table/https://modern-sql.com/feature/withhttps://www.geeksforgeeks.org/sql-natural-join/https://mlsdev.com/blog/81-a-beginner-s-tutorial-for-understanding-restful-apihttps://ghislainfourny.github.io/big-data-textbook/https://www.geeksforgeeks.org/lamports-logical-clock/","link":"/2023/09/19/bigdata1/"},{"title":"bigdata - Distributed file systems","text":"Distributed file systemsrequirements of a distributed file systemGoing back to our capacity-throughput-latency view of storage, a distributed file system is designed so that, in cruise mode, its bottleneck will be the data flow (throughput), not the latency. We saw that capacity increased much faster than throughput, and that this can be solved with parallelism. We saw that throughput increased much faster than latency decreased, and that this can be solved with batch processing. Distributed file systems support both parallelism and batch processing natively, forming the core part of the ideal storage system accessed by MapReduce or Apache Spark.The origins of such a system come back to the design of GoogleFS, the Google File System. Later on, an open source version of it was released as part of the Hadoop project, initiated by Doug Cutting at Yahoo, and called HDFS, for Hadoop Distributed File System. HDFSHDFS does not follow a key-value model: instead, an HDFS cluster organizes its files as a hierarchy, called the file namespace. Files are thus organized in directories, similar to a local file system. Unlike in S3, HDFS files are furthermore not stored as monolithic blackboxes, but HDFS exposes them as lists of blocks. As for the block size: HDFS blocks are typically 64 MB or 128 MB large, and are thus considerably larger than blocks on a local hard drive (around 4 kB).HDFS is designed to a run on a cluster of machines. architectureHDFS is implemented on a fully centralized architecture, in which one node is special and all others are interchangeable and connected to it.In the case of HDFS, the central node is called the NameNode and the other nodes are called the DataNodes. Every file is divided into chunks called blocks. All blocks have a size of exactly 128 MB, except the last one which is usually smaller. Each one of the blocks is then replicated and stored on several DataNodes. How many times? This is a parameter called the replication factor. By default, it is 3. The NameNode is responsible for the system-wide activity of the HDFS cluster. It store in particular three things: • the file namespace, that is, the hierarchy of directory names and file names, as well as any access control (ACL) information similar to Unix-based systems. • a mapping from each file to the list of its blocks. Each block, in this list, is represented with a 64-bit identifier; the content of the blocks is not on the NameNode. • a mapping from each block, represented with its 64-bit identifier, to the locations of its replicas, that is, the list of the DataNodes that store a copy of this block. The DataNodes store the blocks themselves. These blocks are stored on their local disks. DataNodes send regular heartbeats to the NameNode. The frequency of these heartbeats is configurable and is by default a few seconds (e.g., 3s, but this value may change across releases). This is a way to let the NameNode know that everything is alright.Finally, the DataNode also sends, every couple of hours (e.g., 6h, but this value may change across releases), a full report including all the blocks that it contains. A NameNode never initiates a connection to a DataNode. Finally, DataNodes are also capable of communicating with each other by forming replication pipelines. A pipeline happens whenever a new HDFS file is created. The client does not send a copy of the block to all the destination DataNodes, but only to the first one. This first DataNode is then responsible for creating the pipeline and propagating the block to its counterparts. When a replication pipeline is ongoing and a new block is being written to the cluster, the content of the block is not sent in one single 128 MB packet. Rather, it is sent in smaller packets (e.g., 64 kB) in a streaming fashion via a network protocol. replicasHaving this in mind, the first replica of the block, by default, gets written to the same machine that the client is running on. The second replica is written on a DataNode sitting in a different rack than the client, that we call B. The third replica is written to another DataNode on the same rack B.And further replicas are written mostly at random, but respecting two simple rules for resilience: at most one replica per node, and at most two replicas per rack. Fault toleranceHDFS has a single point of failure: the NameNode. If the metadata stored on it is lost, then all the data on the cluster is lost, because it is not possible to reassemble the blocks into files any more.For this reason, the metadata is backed up. More precisely, the file namespace containing the directory and file hierarchy as well as the mapping from files to block IDs is backed up to a so-called snapshot. What is done is that updates to the file system arriving after the snapshot has been made are instead stored in a journal, called edit log, that lists the updates sorted by time of arrival. The snapshot and edit log are stored either locally or on a networkattached drive (not HDFS itself). Logging and importing dataTwo tools are worth mentioning: Apache Flume lets you collect, aggregate and move log data to HDFS. Apache Sqoop lets you import data from a relational database management system to HDFS. referenceshttps://ghislainfourny.github.io/big-data-textbook/","link":"/2023/09/19/bigdata2/"},{"title":"bigdata3","text":"","link":"/2023/10/11/bigdata3/"},{"title":"chatgpt","text":"chatgpt apiparameterstwo important parameters that you can use with OpenAI’s GPT API to help control text generation behavior: temperature and top_p sampling.Temperature is a parameter that controls the “creativity” or randomness of the text generated by GPT-3. A higher temperature (e.g., 0.7) results in more diverse and creative output, while a lower temperature (e.g., 0.2) makes the output more deterministic and focused.In practice, temperature affects the probability distribution over the possible tokens at each step of the generation process. A temperature of 0 would make the model completely deterministic, always choosing the most likely token. Top_p sampling is an alternative to temperature sampling. Instead of considering all possible tokens, GPT-3 considers only a subset of tokens (the nucleus) whose cumulative probability mass adds up to a certain threshold (top_p). referencehttps://community.openai.com/t/cheat-sheet-mastering-temperature-and-top-p-in-chatgpt-api-a-few-tips-and-tricks-on-controlling-the-creativity-deterministic-output-of-prompt-responses/172683","link":"/2023/06/28/chatgpt/"},{"title":"第四篇 git相关(2)-git&amp;github","text":"远程仓库远程仓库是指托管在因特网或其他网络中的你的项目的版本库。 githubGitHub是一个面向开源及私有软件项目的托管平台，因为只支持git作为唯一的版本库格式进行托管，故名GitHub。 一次练习最近在学java，也想巩固一下之前学习过的算法和学习新的算法，就打算用java实现，就用这个项目来加强git的使用练习。 首先在IDEA下java学习的项目里，新建了一个algorithms module，在此文件下，进入Git Bash，新建本地仓库，此时只有一个用来测试排序算法的java文件和一个iml项目配置文件。 目前java文件的内容还是空的， 编辑一下，加入main函数，文件被编辑之后，再次使用git status查看文件状态，会发现文件状态已经变成了modified，再次add，然后开始commit。 commit之后，会显示此次提交的一些信息。 前面展示了一些Git本地的基本操作，现在假如本地文件修改好了，或者工作暂停了，准备放到github上，那么先去github上Create a new repository，最初创建的时候只有一个readme文件，下面将本地仓库同步到远程仓库上。 建立完之后，在本地仓库，将远程仓库的URL复制下来，添加远程仓库。 现在本地仓库里是没有readme文件的，如果此时想要直接push，将本地仓库推送到远程仓库的话，看看会发生什么。 跟随这个报错信息的指示，使用pull，看又会发生什么。这个原因是因为目前本地仓库和远程仓库没有任何相同的文件，根本不相干，所以会被告知无法合并，更加方便的流程是先从远程仓库拉取下来，再把本地文件加入到远程仓库下载到本地的库，然后再提交。 那就没有解决办法了嘛？不是的，可以使用一个强制的方法，添加一个可选项–allow-unrelated-histories，问题终于得以解决。 现在再去github上看看，就会发现提交成功而且push成功啦，开森，撒花之后就要坚持练习写代码啦，刚把得勒","link":"/2020/03/29/git-1/"},{"title":"第六篇 git相关(3)-分支与合并","text":"git分支git分支是git的一大特性，git 的分支本质上仅仅是指向提交对象的可变指针，是包含所指对象校验和（长度为 40 的 SHA-1 值字符串）的文件，这也是为什么git分支的创建和销毁都异常高效的原因，创建新分支时，就是在当前所在的提交对象上创建一个指针，而git是通过一个名为HEAD的特殊指针来记录自己的当前位置的。 与git分支相关的命令有： 1234567git branch -- 查看本地分支git branch -v -- 查看本地分支每个分支的最后一次提交git branch -vv -- 查看本地分支每个分支跟踪的远程分支git checkout -b dev -- 新建dev分支，并切换到dev分支(此后HEAD会指向dev分支,工作目录也会变为dev分支指向的快照内容)git checkout master -- 切换到master分支(此后HEAD会指向master分支,工作目录也会变为master分支指向的快照内容)git merge dev -- 合并dev分支到当前分支git branch -d dev -- 删除dev分支 分支切换注意在切换分支时，git会重置工作目录，自动添加、删除、修改文件以确保此时你的工作目录和这个分支最后一次提交时的样子一模一样，因此在切换分支前，要注意暂存当前的工作进度，可以使用git stash命令暂存当前工作。 分支合并在合并时，可能会看到fast-forward这个词，这是指当前分支是合并分支的直接上游，两者合并不会产生冲突，而只是指针向前移动了，所以叫做fast-forward。 如果两个分支已经分叉开来，进行合并时，git会根据两个分支的共同祖先来做一个三方合并，此时的合并就不是简单的指针向前移动了，而是将三方合并的结果生成一个新的快照并创建一个新的提交指向它。 有时候合并会发生冲突，靠git自动合并已经无法解决，就需要人工去解决冲突，git会在有冲突的文件中加入标记，使用&lt;&lt;&lt;&lt;&lt;&lt;&lt;和&gt;&gt;&gt;&gt;&gt;&gt;&gt;、=======标识了冲突的位置，=======将两个分支的冲突位置内容分隔开来，为了解决冲突，只能选择其中一个，手动地将标识的片段改为你选择的内容即可，修改完冲突之后，还要使用git add来表示冲突已经解决，git commit来完成合并提交。 远程分支远程分支以 (remote)/(branch) 形式命名，本地分支与远程分支交互相关的命令如下： 1234git push (remote) (branch):(remote branch) --推从到远程分支git fetch (remote) --拉取远程仓库的内容，不会修改工作目录git pull (remote) --拉取并合并远程仓库的内容到本地git push (remote) --delete (branch) --删除远程分支","link":"/2021/07/03/git-2/"},{"title":"第三篇 git相关(1)-git基础原理与命令","text":"什么是gitGit is a free and open source distributed version control system designed to handle everything from small to very large projects with speed and efficiency. 使用Git假设已经在电脑上安装好了Git，并配好了环境，在windows下，文件夹下右击出现了 Git Bash Here，就说明已经安装好了。 使用git需要首先建立一个仓库，之后就可以在这个仓库中对代码进行各种操作，过程中会使用到各种git命令，下面就介绍一下每个git命令的具体作用。 Git 基础命令git init用于初始化一个仓库。在某个文件夹下打开Git Bash，运行完这个命令，文件夹下会生成一个.git文件夹，这个文件夹会记录以后的变更行为，但想要真正地追踪这些变更，还需要更多的操作，只有处于tracked状态下的文件，git才会追踪，后面会具体介绍。 git status用于查看仓库中所有文件的状态。Git中文件有4种状态：untracked， unmodified , modified, staged，后三种状态属于tracked，这几个状态体现了git的原理。 新建一个文件的时候，这个文件处于untracked状态，对这个文件使用了 git add之后，这个文件进入staged状态，也就是暂存区，使用了git commit之后，这个文件进入unmodified状态，这才是实际提交了改动，如果编辑了这个文件，对这个文件进行了更改，便进入了modified状态。 git add在上部分有介绍过，但是这个命令并不是添加文件到某个项目中的意思，准确地说这个命令是把想要提交的内容添加到准备提交的集合里，可以用这个命令来追踪新文件(add the file)，暂存文件（stage the file），或者其它在commit之前的操作。如果在运行了git add之后又修改了文件，没有再次运行git add，就运行了commit，commit的是修改之前的内容。那么是不是每一次都要反复操作git add呢，其实还有别的方法可以跳过暂存区这一步，下面会说明。 git commit是提交变化到仓库里，提交到仓库里的几乎总是可以恢复的，后面会介绍如何恢复。git commit -a就可以跳过暂存区这一步，因为-a包含了所有改动过的文件。git commit -m可以加上这次提交的描述， git rm用于删除文件，这里的删除有两种含义，从Git中删除和从工作目录中删除，如果只是想要Git不在追踪这个文件，需要使用git rm --cached。 git log用于回顾提交历史，运行这个命令可以看到提交的SHA-1 校验和，作者，提交时间和具体提交的内容。如果想要复原到某次提交时候的版本，这个命令是非常重要的，通过拿到每次提交的SHA-1 校验和，可以追踪到对应的版本。 git reset HEAD 用于取消暂存文件。 git checkout --用于撤销所作的修改。 Git 分支分支就是与主线相对的，每个人都可以使用各自的分支进行工作，而不影响主线。在许多版本控制系统中，是需要创建一个完整的源项目副本来创建分支的，而Git不是这样的，Git处理分支的方式非常轻量，分支之间的操作非常迅速。 要理解Git是如何处理分支的，就要理解Git是如何实现对文件的追踪的。Git保存的是不同时刻的快照（Snapshot）,进行提交操作时，Git会保存一个提交对象，这个对象包含了一个指向暂存内容快照的指针，还包含了作者、邮箱等内容以及指向它的父对象的指针，如果是第一次提交，是没有父对象的，而之后的提交，其父对象就是上一次提交。Git的分支，本质上就是指向提交对象的可变指针，所以不同的分支可以指向不同的内容，从而互不影响，而且Git的分支实质上就是一个包含所致对象校验和的文件，所以其创建和销毁都非常高效。 git branch就是创建分支的命令，这个命令会在当前的提交快照上创建一个指针。Git通过一个HEAD的特殊指针指向当前所在的本地分支，从而可以知道自己当前在哪一个分支上。 git branch -d用于删除分支。 git checkout是切换分支的命令。 git merge用于合并分支。当合并分支产生冲突时，Git会停下来，这时候需要手动解决这些冲突，解决完冲突之后，使用git add命令将冲突文件标记为冲突已解决。 远程仓库之前所述的内容，都是基于本地的操作，而如果想要在Git项目上进行协作，就需要一个公共的空间供项目参与者进行共同编辑，这就是远程仓库。远程仓库是指托管在因特网或其他网络中的你的项目的版本库，使用命令新建远程仓库的操作与本地是一致的，在github上create repository就可以新建一个远程仓库，本地和远程仓库之间通过SSH连接，完成SSH的公钥设置之后， 想要实现本地与远程仓库的内容交换，使用以下介绍的命令。 git remote add &lt;shortname&gt; &lt;url&gt;用于添加一个新的远程仓库。 git fetch用于从远程仓库中获取本地没有的数据，但是这个命令指挥把数据下载到本地仓库，并不会与本地仓库自动合并，想要实现自动合并，需要使用git pull命令。git clone可以把远程仓库的内容克隆到本地，并将远程仓库默认命名为”origin”。 git push &lt;remote&gt; &lt;branch&gt;命令用于将本地内容推送到远程仓库上，只有具备远程仓库的写入权限，并且没有人推送过之前，这条命令才会生效，如果别人先推送了，需要先抓取别人的工作并合并到自己的工作中之后才能推送。 git remote show &lt;remote&gt;用于查看某一个远程仓库的具体新息。 git 别名上述说了很多命令，有些命令也比较长，命令很多也比较难记下来，git提供了将这些命令起个别名的功能方便使用。 git config --global alias.co checkout就将checkout起了别名co，现在使用git co就相当于git checkout。","link":"/2020/03/15/git/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2018/12/21/hello-world/"},{"title":"Introduction to deep learning in computer vision","text":"Basic architectureCNNConvolutional networks are simply neural networks that use convolution in place of general matrix multiplication in at least one of their layers.Convolution leverages three important ideas that can help improve a machine learning system: sparse interactions, parameter sharing and equivariant representations. Moreover, convolution provides a means for working with inputs of variable size. We assume that the size of the input image is nn, and the size of the filter is ff (note that f is generally an odd number). The size of the output image after convolution is (n-f+1)* (n-f+1).During the convolution process, padding is sometimes necessary to avoid information loss. Additionally, adjusting the stride allows for compression of some information.If we want to perform convolution on a three-channel RGB image, the corresponding filter group would also have three channels. The process involves convolving each individual channel with its corresponding filter, summing up the results, and then adding the sums of the three channels together. The resulting sum of the 27 multiplications is considered as one pixel value of the output image. The filters for different channels can be different. When the input has specific height, width, and channel dimensions, the filters can have different height and width, but the number of channels must match the input.Pooling layers are commonly included in many CNNs. The purpose of pooling layers is to reduce the size of the model, improve computational speed, and simultaneously decrease noise to enhance the robustness of the extracted features. Important networks in the history of computer visionLeNet-5LeNet-5, developed by Yann LeCun et al. in 1998, was one of the first successful convolutional neural networks (CNNs) for handwritten digit recognition. It laid the foundation for modern CNN architectures and demonstrated the power of deep learning in computer vision tasks. “Gradient-Based Learning Applied to Document Recognition” by Yann LeCun et al. (1998). LeNet’s network architecture has seven layers: convolutional layer (Convolutions, C1), pooling layer (Subsampling, S2), convolutional layer (C3), pooling layer (S4), fully connected convolutional layer ( C5), fully connected layer (F6), Gaussian connected layer (output).The input layer is a 28x28 one-dimensional image, and the Filter size is 5x5. The output channels of the first Filter and the second Filter are 6 and 16 respectively, and both use Sigmoid as the activation function.The window of the pooling layer is 2x2, the stride is 2, and the sampling is performed using average pooling. The number of neurons in the last fully connected layer is 120 and 84, respectively.The last output layer is the Gaussian connection layer, which uses the RBF function (radial Euclidean distance function) to calculate the Euclidean distance between the input vector and the parameter vector. AlexNetAlexNet, introduced by Alex Krizhevsky et al. in 2012, was a breakthrough CNN architecture that won the ImageNet competition and popularized deep learning in computer vision. It demonstrated the effectiveness of deep CNNs for image classification tasks and paved the way for subsequent advancements.”ImageNet Classification with Deep Convolutional Neural Networks” by Alex Krizhevsky et al. (2012).AlexNet’s architecture has eight layers, using a total of five convolutional layers and three fully connected layers, which is deeper than the LeNet model.The first to fifth layers are convolutional layers, where the first, second, and fifth convolutional layers are followed by pooling layers, and Maxpooling with a size of 3x3 and a stride of 2 is used.The sixth to eighth layers are fully connected layers. Changing the Sigmoid used by LeNet to ReLU can avoid the problem of vanishing gradient due to too deep neural network layers or too small gradients. VGGNetThe VGGNet, proposed by Karen Simonyan and Andrew Zisserman in 2014, is known for its simplicity and depth. It consisted of deep networks with stacked 3x3 convolutional layers, showing that increasing network depth led to improved performance on image classification tasks.”Very Deep Convolutional Networks for Large-Scale Image Recognition” by Karen Simonyan and Andrew Zisserman (2014).Compared with AlexNet, VGGNet adopts a deeper network. It is characterized by repeated use of the same set of basic modules, and uses small convolution kernels instead of medium and large convolution kernels in AlexNet. Its architecture consists of n VGG Blocks and 3 full connections composed of layers.The structure of VGG Block is composed of 3x3 convolutional layers (kernel size=3x3, stride=1, padding=”same”) of different numbers (the number is hyperparameters), and 2x2 Maxpooling (pool size=2, stride=2).VGGNet has many different structures, such as VGG11, VGG13, VGG16, VGG19, the difference lies in the number of layers of the network (the number of convolutional layers and the number of fully connected layers). The common VGGNet refers to VGG16. Network in Network“Network in Network” (NiN) refers to a neural network architecture proposed by Lin et al. in their paper titled “Network In Network” published in 2014. NiN is designed to enhance the expressive power of deep neural networks by incorporating micro neural networks called “MLPs (Multi-Layer Perceptrons)” or “1x1 Convolutions” within the network structure. The key idea behind NiN is to replace traditional convolutional layers with what they call “MLP Convolutional Layers” or “1x1 Convolutional Layers.” These layers consist of a series of fully connected layers (MLPs) applied at every pixel location of the input. The purpose is to capture complex local feature interactions and enable more non-linear transformations.By using 1x1 convolutions, NiN can model non-linear relationships within the channels of the input feature map. This allows for richer and more powerful representations compared to standard convolutional layers.The 1x1 convolutional layer not only integrates the information of different channels at the same position, but also can reduce or increase the dimension of the channel. GoogLeNet (Inception-v1)GoogLeNet, presented by Christian Szegedy et al. in 2015, introduced the Inception module and demonstrated the importance of multi-scale feature extraction. It achieved high accuracy while maintaining computational efficiency, inspiring subsequent Inception versions and influencing network designs.”Going Deeper with Convolutions” by Christian Szegedy et al. (2015).GoogLeNet was designed to address the challenges of deep neural networks, such as computational efficiency and overfitting, while maintaining high accuracy in image classification tasks. It introduced several novel concepts and architectural innovations that made it stand out from previous CNN architectures at the time. The key feature of GoogLeNet is the Inception module, which utilizes parallel convolutional filters of different sizes (1x1, 3x3, 5x5) to capture features at various scales. This allows the network to learn and represent both local and global features effectively. Additionally, it incorporates 1x1 convolutions for dimensionality reduction and introduces a technique called “bottleneck” layers to reduce the computational complexity. InceptionIn the context of computer vision, “inception” refers to the Inception module or the Inception architecture used in deep convolutional neural networks (CNNs). The Inception module was introduced in the GoogLeNet architecture (also known as Inception-v1) as a key component for efficient and effective feature extraction.The Inception module aims to capture multi-scale features by employing multiple parallel convolutional filters of different sizes within the same layer. By using a combination of 1x1, 3x3, and 5x5 convolutional filters, the Inception module allows the network to learn and extract features at various spatial scales. The Inception module extracts different features through convolution of three different sizes and 3x3 Maxpooling, and then concatenates these four results together with the channel axis. This way of increasing the width of the network can capture more features and details of the picture.But if the sizes of these four results are different, both the convolutional layer and the pooling layer use padding=”same” and stride=1 to ensure the size of the input feature map. ResNetResNet, developed by Kaiming He et al. in 2015, introduced the concept of residual learning. It utilized skip connections or shortcuts to address the vanishing gradient problem and enabled training of extremely deep networks, leading to significant performance gains in image classification and other tasks.”Deep Residual Learning for Image Recognition” by Kaiming He et al. (2015). DenseNet DenseNet, introduced by Gao Huang et al. in 2016, focused on dense connectivity patterns between layers. It aimed to alleviate the vanishing gradient problem, promote feature reuse, and encourage better gradient flow. DenseNet achieved competitive results while reducing the number of parameters compared to other architectures. “Densely Connected Convolutional Networks” by Gao Huang et al. (2016).’ ResNeXtResNeXt is a convolutional neural network (CNN) architecture that builds upon the concepts introduced by the ResNet (Residual Network) model. ResNeXt was proposed by Xie et al. in their paper titled “Aggregated Residual Transformations for Deep Neural Networks” in 2017. The main idea behind ResNeXt is to leverage the concept of “cardinality” to improve the representational power of the network. Cardinality refers to the number of independent pathways or branches within a block of the network. In ResNeXt, instead of using a single pathway in each block, multiple parallel pathways are employed. referenceshttps://juejin.cn/post/7104845694225088525https://www.showmeai.tech/article-detail/221https://medium.com/ching-i/%E5%8D%B7%E7%A9%8D%E7%A5%9E%E7%B6%93%E7%B6%B2%E7%B5%A1-cnn-%E7%B6%93%E5%85%B8%E6%A8%A1%E5%9E%8B-lenet-alexnet-vgg-nin-with-pytorch-code-84462d6cf60c","link":"/2023/06/09/image-task/"},{"title":"第二十一篇 java(2)- 集合框架","text":"集合框架java集合框架包括 Collection 和 Map 两种，Collection 存储着对象的集合，而 Map 存储着键值对(两个对象)的映射表。 CollectionCollection下包括Set、List、Queue，各自又包含了使用不同方式的实现。 SetSet下包括TreeSet，HashSet，LinkedHashSet。其中TreeSet基于红黑树实现。","link":"/2021/09/26/java-1/"},{"title":"第八篇 java(1)-面向对象","text":"类与对象类就是用户定义好的原型，可以通过类创建对象，类中定义好了一系列的属性或者是函数。对象是真正的实体。当为一个类创建了对象，也可以说是实例化了这个类，java中有几种创建类的方式。 当只是简单地声明一个类变量时，如Object a，不同于原始变量int、double等声明变量时就分配好了内存，这样的声明方式并没有创建好一个对象，需要通过new关键字来触发类构造器，并为这个对象分配好内存。所有类都至少有一个构造函数，如果没有定义，Java编译器会自动创建一个无参构造函数。这个构造器会调用其父类的无参构造函数。 封装、继承与多态修饰符在介绍封装、继承与多态之前，需要先了解java中的修饰符，修饰符有两类： 一类是控制访问权限的，一类是实现其他功能的。控制访问权限的修饰符有 12345678// 以下为类修饰符public --任何类均可访问。default --没有指定修饰符时的默认修饰符，只有同一个包中的类可以访问。// 以下为属性、方法修饰符public --任何类均可访问。private --只有声明的类中可以访问。protected --只有同一个包中的类和其子类可以访问。default --没有指定修饰符时的默认修饰符，只有同一个包中的类可以访问。 实现其他功能的修饰符有： 12345678910// 以下为类修饰符final --此类不能被其他类继承。abstract --抽象类，此类不能用来创建对象。// 以下为属性、方法修饰符final --属性、方法不能被重载。static --属性、方法属于类而不是对象。abstract -- 只能用在抽象类中的方法上。transient -- 序列化对象时跳过此属性和方法。synchronized -- 此方法一次只能被一个线程访问。volatile --此属性的值不是线程内部缓存，而是从主内存中读取。 封装封装可以将实现细节隐藏起来，其最主要的功能在于我们能修改自己的实现代码，而不用修改那些调用我们代码的程序片段。 继承在继承中，先了解两个概念：父类和子类，父类就是被继承的类，子类就是继承其他类的类，在jva中，继承使用extends关键字或implements关键字。java中类的继承是单一继承，一个子类只能拥有一个父类。所有 Java 的类均是由java.lang.Object 类继承而来的。通过使用关键字 extends ，子类可以继承父类的除 private 属性外所有的属性。Implements 关键字在类继承接口的情况下使用，可以多继承接口。 重写(Override)与重载(Overload)重写(Override)重写是子类对父类的允许访问的方法的实现过程进行重新编写！返回值和形参都不能改变。 子类在声明变量时，可以使用父类类型，这是因为在编译阶段，只是检查参数的引用类型，然而在运行时，Java 虚拟机 (JVM) 指定对象的类型并且运行该对象的方法。需要注意的是构造方法不能被重写。需要在子类中调用父类的被重写方法时，要使用 super 关键字。 重载(Overload)重载 (overloading) 是在一个类里面，方法名字相同，而参数不同。返回类型可以相同也可以不同。构造函数是可以重载的。 多态通过继承、重写、重载可以以多种不同的方式实现某个操作，便可以实现多态。除此之外，java中还有接口和抽象类以实现多态。 接口接口通常以interface来声明。一个实现接口的类，必须实现接口内所描述的所有方法，否则就必须声明为抽象类。","link":"/2021/07/16/java/"},{"title":"Large Language Model","text":"basic ideasZero-Shot Learningzero-shot learning, in which your model learns how to classify classes that it hasn’t seen before. Contrastive Language-Image Pretraining (CLIP)Just like traditional supervised models, CLIP has two stages: the training stage (learning) and the inference stage (making predictions).In the training stage, CLIP learns about images by “reading” auxiliary text (i.e. sentences) corresponding to each image. CLIP aims to minimize the difference between the encodings of the image and it’s corresponding text.In the inference stage, we setup the typical classification task by first obtaining a list of all possible labels.Each label will then be encoded by the pretrained text encoder from Step 1.Now that we have the label encodings, T₁ to Tₙ, we can take the image that we want to classify, feed it through the pretrained image encoder, and compute how similar the image encoding is to each text label encoding using a distance metric called cosine similarity. contrastive learningContrastive learning is a machine learning technique used to learn the general features of a dataset without labels by teaching the model which data points are similar or different.It looks at which pairs of data points are “similar” and “different” in order to learn higher-level features about the data, before even having a task such as classification or segmentation. SimCLRv2The entire process can be described concisely in three basic steps: For each image in our dataset, we can perform two augmentation combinations (i.e. crop + resize + recolor, resize + recolor, crop + recolor, etc.). We want the model to learn that these two images are “similar” since they are essentially different versions of the same image. To do so, we can feed these two images into our deep learning model (Big-CNN such as ResNet) to create vector representations for each image. The goal is to train the model to output similar representations for similar images. Lastly, we try to maximize the similarity of the two vector representations by minimizing a contrastive loss function. Meta-learningThe idea of meta-learning is to learn the learning process. In-context Learninguring in-context learning, we give the LM a prompt that consists of a list of input-output pairs that demonstrate a task. At the end of the prompt, we append a test input and allow the LM to make a prediction just by conditioning on the prompt and predicting the next tokens. Instruction learningInstruction learning is an idea proposed by the team led by Quoc V. Le at Google DeepMind in a paper titled ‘Finetuned Language Models Are Zero-Shot Learners’ in 2021. The purpose of instruction learning and prompt learning is to explore the knowledge inherent in language models. The difference is that prompts aim to stimulate the completion ability of the language model, such as generating the second half of a sentence based on the first half or filling in the blanks. Instructions aim to stimulate the understanding ability of the language model by providing more explicit instructions, enabling the model to take correct actions. The advantage of instruction learning is that after fine-tuning through multitask learning, it can also perform zero-shot learning on other tasks, while prompt learning is specific to one task. Its generalization ability is not as strong as instruction learning. Diffusion ModelIn machine learning, the Diffusion Model refers to a class of algorithms or models that utilize diffusion processes for various tasks, such as data clustering, image segmentation, or graph-based learning. The basic principle of the Diffusion Model in machine learning is to propagate information or labels through the connections or edges of a graph or network. The diffusion process starts with initial information or labels assigned to some nodes in the graph, and it gradually spreads and influences the neighboring nodes based on certain rules or algorithms. Stable DiffusionStable Diffusion is a deep learning, text-to-image model released in 2022. It is primarily used to generate detailed images conditioned on text descriptions. Prompt engineeringPrompt engineering is a relatively new discipline for developing and optimizing prompts to efficiently use language models (LMs) for a wide variety of applications and research topics.Prompt engineering focuses on crafting the optimal textual input by selecting the appropriate words, phrases, sentence structures, and punctuation. RLHF(Reinforcement Learning from Human Feedback)generation Auto-regressive language generation is based on the assumption that the probability distribution of a word sequence can be decomposed into the product of conditional next word distributions. The length T of the word sequence is usually determined on-the-fly and corresponds to the timestept=T the EOS token is generated from the probability distribution. decoding methodsGreedy searchGreedy search is the simplest decoding method. It selects the word with the highest probability as its next word. Beam searchBeam search reduces the risk of missing hidden high probability word sequences by keeping the most likely num_beams of hypotheses at each time step and eventually choosing the hypothesis that has the overall highest probability.Beam search will always find an output sequence with higher probability than greedy search, but is not guaranteed to find the most likely output.The most common n-grams penalty makes sure that no n-gram appears twice by manually setting the probability of next words that could create an already seen n-gram to 0. Nevertheless, n-gram penalties have to be used with care. An article generated about the city New York should not use a 2-gram penalty or otherwise, the name of the city would only appear once in the whole text!When using transformers library:beam_output = model.generate(**model_inputs,max_new_tokens=40,num_beams=5,no_repeat_ngram_size=2,early_stopping=True) Another important feature about beam search is that we can compare the top beams after generation and choose the generated beam that fits our purpose best.In transformers, we simply set the parameter num_return_sequences to the number of highest scoring beams that should be returned. Make sure though that num_return_sequences &lt;= num_beams! samplingIn its most basic form, sampling means randomly picking the next word according to its conditional probability distribution. temperaturea temperatureparameter to adjust the probability distribution of the output. The larger the parameter value, the smoother the distribution looks, that is, the gap between high probability and low probability is narrowed (not so sure about the output); of course, the smaller it is, the more obvious the gap between high probability and low probability (more sure about the output). If it tends to 0, it is the same as Greedy Search. Top-KIn Top-K sampling, the K most likely next words are filtered and the probability mass is redistributed among only those K next words.GPT2 adopted this sampling scheme, which was one of the reasons for its success in story generation. Top-PInstead of sampling only from the most likely K words, in Top-p sampling chooses from the smallest possible set of words whose cumulative probability exceeds the probability p. The probability mass is then redistributed among this set of words. While in theory, Top-p seems more elegant than Top-K, both methods work well in practice. Top-p can also be used in combination with Top-K, which can avoid very low ranked words while allowing for some dynamic selection. sample_outputs = model.generate(**model_inputs, max_new_tokens=40,do_sample=True,top_k=50,top_p=0.95,num_return_sequences=3) models:LLaMALLaMA, a collection of foundation language models ranging from 7B to 65B parameters. FastChatother modelshttps://github.com/baichuan-inc/baichuan-7B LLM benchmarksMMLUThe MMLU benchmark covers 57 general knowledge areas such as “Humanities”, “Social Sciences”, and “STEM”. Each question in it contains four possible options, and each question has only one correct answer.there are two main ways to get information from a model to evaluate it:Get the output probabilities for a particular set of tokens and compare them to the alternatives in the sample;Take the text generated by the model (iteratively generated one by one using the method described above), and compare these texts with the alternatives in the sample. C-EvalA Chinese knowledge and reasoning test set covering four major fields: humanities, social sciences, natural sciences, and other disciplines. It consists of 52 subjects, including calculus, linear algebra, and more, covering topics from secondary school to university-level studies, graduate studies, and professional examinations. The test set comprises a total of 13,948 questions. code generation benchmarksHumanEvalHumanEval is proposed to evaluate the functional correctness on a set of 164 handwritten programming problems with unit tests.Functional correctness is measured for synthesizing programs from docstrings.Each problem includes a function signature, docstring, body, and several unit tests. pass@k metric, is used where k code samples are generated per problem see if any sample passes the unit tests. MBPP (Mostly Basic Python Programming)The benchmark consists of around 1,000 crowd-sourced Python programming problems, designed to be solvable by entry-level programmers, covering programming fundamentals, standard library functionality, and so on. Each problem consists of a task description, code solution and 3 automated test cases. APPS(Automated Programming Progress Standard)The APPS dataset consists of 5000 training and 5000 test examples of coding problems. Most of the APPS tests problems are not formulated as single-function synthesis tasks, but rather as full-program synthesis.The APPS benchmark attempts to mirror how humans programmers are evaluated by posing coding problems in unrestricted natural language and evaluating the correctness of solutions. MultiPL-EMultiPL-E is a multi-programming language benchmark for evaluating the code generation performance of large language model (LLMs) of code. DS-1000a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas. referenceshttps://towardsdatascience.com/understanding-zero-shot-learning-making-ml-more-human-4653ac35ccabhttps://towardsdatascience.com/understanding-contrastive-learning-d5b19fd96607http://ai.stanford.edu/blog/understanding-incontext/https://www.8btc.com/article/6813626https://en.wikipedia.org/wiki/Stable_DiffusionLLaMA: Open and Efficient Foundation Language Modelshttps://www.promptingguide.ai/https://medium.com/geekculture/list-of-open-sourced-fine-tuned-large-language-models-llm-8d95a2e0dc76https://nl2code.github.io/https://yaofu.notion.site/C-Eval-6b79edd91b454e3d8ea41c59ea2af873https://huggingface.co/blog/zh/evaluating-mmlu-leaderboardhttps://github.com/datawhalechina/hugging-llm/blob/main/content/ChatGPT%E5%9F%BA%E7%A1%80%E7%A7%91%E6%99%AE%E2%80%94%E2%80%94%E7%9F%A5%E5%85%B6%E4%B8%80%E7%82%B9%E6%89%80%E4%BB%A5%E7%84%B6.mdhttps://huggingface.co/blog/how-to-generate","link":"/2023/06/19/large-model/"},{"title":"第七篇 Linux相关(2)-进程管理与性能分析","text":"进程管理相关命令进程启动12nohup --用于在系统后台不挂断地运行命令&amp; --放在命令后，表示后台执行 进程查看1234567ps -aux --显示进程状态top --实时显示进程状态pstree --树状图展示进程关系nice --调整进程优先级pidof --查询进程PIDkill --终止进程killall --终止某服务名称对应的所有进程 历史命令12history --显示执行过的命令历史!编码数字 --重复执行某一次的命令 性能分析系统状态1234uname --查看系统内核版本与系统架构,详细系统版本使用cat /etc/redhat-releaseuptime --查看系统的负载信息free --显示当前系统中内存的使用量信息who --查看当前登入主机的用户终端信息 网络状态123ifconfig --查看网卡配置和网络状态ifconfigping --测试主机之间的网络连通性netstat --显示如网络连接、路由表、接口状态等的网络相关信息 时间123date -- 显示当前日期date &quot;+%Y-%m-%d %H:%M:%S&quot; --以给定格式显示日期date &quot;+%j&quot; --今年第几天","link":"/2021/07/09/linux-1/"},{"title":"第二篇 Linux相关(1)-文件管理与文本编辑","text":"文件操作相关命令查看目录和文件12345678ls -- 当前目录下文件展示ll -- 当前目录下文件展示 详细tree --树状图形式展示pwd --显示当前工作目录find --按照指定条件来查找文件所对应的位置cd -- 打开目录，.表示当前目录，..表示上一级目录，-表示回到刚才所在的路径下。which 命令 --查看命令所在路径file 文件名 --查看文件类型 查看文件内容1234567891011cat 文件名 -- 从第一行开始显示文件内容head [-n number] 文件名 -- 只看头几行tail [-n number] 文件名 -- 只看尾巴几行，tail -f 可以实时查看文件更新内容more 文件名 -- 一页一页显示内容less 文件名 -- 一页一页显示内容,相比less可以向上翻页nl 文件名 -- 显示行号stat 文件名 --查看文件的具体存储细节和时间等信息wc 文件名 --统计指定文本文件的行数、字数或字节数grep 文件名 --按行提取文本内容cut 文件名 --按列提取文本内容diff [参数] 文件名称A 文件名称B --比较多个文件之间内容的差异 处理目录和文件12345678mkdir -- 创建目录rmdir -- 删除空目录cp source dest --复制source到destmv source dest -- 移动source到dest/重命名rm -- 删除touch --创建空白文件tar -czvf 压缩包名称.tar.gz 要打包的目录 -- 把指定的文件进行打包压缩tar -xzvf 压缩包名称.tar.gz -C 解压到的路径 --解压到指定路径 磁盘管理12345df -- 列出文件系统的整体磁盘使用量du -- 检查磁盘空间使用量du -ah --max-depth=1 查看当前目录下的第一级使用量mount --磁盘挂载umount --磁盘卸载 文本编辑器vi/vim命令模式(:)12345678910111213:i -- 进入编辑模式:wq -- 保存并退出:q -- 退出:q! --强制退出:w [filename] --另存为:set nu --显示行号:set nonu --取消行号:1,$s/word1/word2/g -- 从第一行到最后一行寻找 word1 字符串，并将该字符串取代为 word2 :n1,n2s/word1/word2/g -- 在第 n1 与 n2 行之间寻找 word1 这个字符串，并将该字符串取代为 word2 :10,20s#^#//#g -- 在 10 - 20 行添加 // 注释:10,20s#^//##g -- 在 10 - 20 行删除 // 注释:10,20s/^/#/g -- 在 10 - 20 行添加 # 注释::10,20s/#//g -- 在 10 - 20 行删除 # 注释 操作键:1234567891011gg -- 转到第一行G -- 转到最后一行nG -- 转到第n行dd -- 删除光标所在行yy -- 复制光标所在行[Ctrl] + [f] -- 往上翻页[Ctrl] + [b] -- 往下翻页/ -- 搜索，如/word就是搜索wordu -- 撤销上一步[Ctrl]+r -- 重复上一步p,P -- 粘贴复制内容,p是复制到光标所在下一行，P是复制到光标所在上一行。","link":"/2020/03/14/linux/"},{"title":"object tracking","text":"object trackingMultiple Object Tracking(MOT) is the task of detecting various objects of interest in a video, tracking these detected objects in subsequent frames by assigning them a unique ID, and maintaining these unique IDs as the objects move around in a video in successive frames.Generally, multiple object tracking happens in two stages: object detection and object association. Object detection is the process of identifying all potential objects of interest in the current frame using object detectors such as Faster-RCNN or YOLO. Object association is the process of linking objects detected in the current frame with its corresponding objects from previous frames, referred to as tracklets. Object or instance association is usually done by predicting the object’s location at the current frame based on previous frames’ tracklets using the Kalman Filter followed by one-to-one linear assignment typically using the Hungarian Algorithm to minimise the total differences between the matching results. MetricsMOTP (Multiple Object Tracking Precision)MOTP (Multi-Object Tracking Precision) expresses how well exact positions of the object are estimated. It is the total error in estimated position for matched ground truth-hypothesis pairs over all frames, averaged by the total number of matches made. This metric is not responsible for recognizing object configurations and evaluating object trajectories. MOTA (Multiple Object Tracking Accuracy)MOTA (Multi-Object Tracking Accuracy) shows how many errors the tracker system has made in terms of Misses, False Positives, Mismatch errors, etc. Therefore, it can be derived from three error ratios: the ratio of Misses, the ratio of False positives, and the ratio of Mismatches over all the frames. IDF1 score (IDF1)IDF1 score (IDF1) is the ratio of correctly identified detections over the average of ground truth and predicted detections. BenchmarksOTBKITTIMOT16Methods(models)IOU trackerThe Intersection-Over-Union (IOU) tracker uses the IOU values among the detector’s bounding boxes between the two consecutive frames to perform the association between them or assign a new target ID if no match found. Simple Online And Realtime Tracking (SORT)Simple Online And Realtime Tracking (SORT) is a lean implementation of a tracking-by detection framework.SORT uses the position and size of the bounding boxes for both motion estimation and data association through frames. SORT combines location and motion cues by adopting a Kalman filter to predict the location of the tracklets in the new frame, then computes the IoU between the detection boxes and the predicted boxes as the similarity. DeepSORTDeepSORT replaces the association metric with a more informed metric that combines motion and appearance information. In particular, a “deep appearance” distance metric is added. The core idea is to obtain a vector that can be used to represent a given image. DeepSort adopts a stand-alone RE-ID model to extract appearance features from the detection boxes. After similarity computation matching strategy assigns identities to the objects. This can be done by the Hungarian Algorithm or greedy assignment. FairMOTFairMOT is a new tracking approach built on top of the anchor-free object detection architecture CenterNet.It has a simple network structure that consists of two homogeneous branches for detecting objects and extracting re-ID features. TransMOTTransMOT is a new spatial-temporal graph Transformer that solves all these issues. It arranges the trajectories of all the tracked objects as a series of sparse weighted graphs that are constructed using the spatial relationships of the targets. TransMOT then uses these graphs to create a spatial graph transformer encoder layer, a temporal transformer encoder layer, and a spatial transformer decoder layer to model the spatial-temporal relationships of the objects. ByteTrackBYTE is an effective association method that utilizes all detection boxes from high scores to low ones in the matching process.BYTE is built on the premise that the similarity with tracklets provides a strong cue to distinguish the objects and background in low score detection boxes. BYTE first matches the high score detection boxes to the tracklets based on motion similarity. It uses Kalman Filter to predict the location of the tracklets in the new frame. The motion similarity is computed by the IoU of the predicted box and the detection box. Then, it performs the second matching between the unmatched tracklets. The primary innovation of BYTETrack is keeping non-background low confidence detection boxes which are typically discarded after the initial filtering of detections and use these low-score boxes for a secondary association step. Typically, occluded detection boxes have lower confidence scores than the threshold, but still contain some information about the objects which make their confidence score higher than purely background boxes. Hence, these low confidence boxes are still meaningful to keep track of during the association stage. Comparison of DeepSort and ByteTrackDeepSort uses a pre-trained object detection model to detect objects in each frame and a Siamese network to match the detected objects based on their appearance features. It also uses Kalman filters to predict the locations of the objects in the next frame. ByteTrack, on the other hand, uses a lightweight Siamese network architecture that takes in two input frames and outputs a similarity score. It also uses a simple but effective data augmentation technique to improve its performance on challenging datasets. using ByteTrackByteTracker initiates a new tracklet only if a detection is not matched with any previous tracklet and the bounding box score is higher than a threshold. referenceshttps://www.datature.io/blog/introduction-to-bytetrack-multi-object-tracking-by-associating-every-detection-boxhttps://pub.towardsai.net/multi-object-tracking-metrics-1e602f364c0chttps://learnopencv.com/object-tracking-and-reidentification-with-fairmot/https://medium.com/augmented-startups/top-5-object-tracking-methods-92f1643f8435https://medium.com/@pedroazevedo6/object-tracking-state-of-the-art-2022-fe9457b77382","link":"/2023/08/03/object-tracking/"},{"title":"opencv","text":"basic functionsreading and writingcv2.VideoWriter_fourcc(‘M’, ‘P’, ‘4’, ‘V’)cv2.VideoWriter(filename,fourcc,fps,frameSize[,isColor])cv2.VideoWriter.write(image)","link":"/2023/08/10/opencv/"},{"title":"pai1","text":"","link":"/2023/10/12/pai1/"},{"title":"第零篇 概率论相关(1)-先验概率、后验概率与似然","text":"先验概率、后验概率与似然今天看到一个比较好的关于先验、后验和似然的通俗解释，先验概率就是基于历史数据的统计经验，后验概率是在已知结果发生时推断原因的概率，似然概率是已知原因推断结果的概率。 根据上述解释，假设我们有一个数据集，这个数据集服从某一种分布，也可以理解为是一个黑盒子模型，黑盒子模型里面包含了很多参数，则似然概率就是已知参数得到某样本的概率，后验概率就是已知某样本得到参数的概率。 为了更理解这一概念，再来看一下著名的贝叶斯公式：$$P(\\theta \\mid x)=\\frac{p(x \\mid \\theta) p(\\theta)}{p(x)}$$其中$p(\\theta)$是先验概率， $P(\\theta \\mid x)$是后验概率，$p(x \\mid \\theta)$是似然函数。 这里区分一下两个概念，对于$p(x \\mid \\theta)$如果$\\theta$已知且不变，x是变量，则此函数称为概率函数，而如果x已知且保持不变，$\\theta$是变量，则此函数称为似然函数。 最大似然估计(MLE)最大似然，也就是说要让似然最大，则在数据集上的学习过程就是求模型参数使得当前观察到的样本概率最大，所以最大似然估计的目的就是根据已知的样本结果，反推最有可能导致这个结果的参数值。最大似然估计的适用场景是”模型已定、参数未知”，一个重要前提是样本集中的样本都是独立同分布的随机变量，因为只有独立同分布，样本集的似然函数才能等于各样本似然函数的乘积。 假设一个用于学习的样本集是：$D=\\left{x_{1}, x_{2}, \\cdots, x_{N}\\right}$，来估计参数向量θ，则$l(\\theta)=p(D \\mid \\theta)=p\\left(x_{1}, x_{2}, \\cdots, x_{N} \\mid \\theta\\right)=\\prod_{i=1}^{N} p\\left(x_{i} \\mid \\theta\\right)$，则使得似然函数最大的参数值求解过程为：$$\\hat{\\theta}=\\arg \\max {\\theta} l(\\theta)=\\arg \\max {\\theta} \\prod{i=1}^{N} p\\left(x{i} \\mid \\theta\\right)$$ 最大后验估计(MAP)最大后验估计与最大似然估计的不同之处在于最大后验估计中引入了先验概率，因此结合贝叶斯公式和最大似然估计，最大后验估计就转化为了：$$\\operatorname{argmaxp}(\\theta \\mid X)=\\operatorname{argmax} \\frac{p(X \\mid \\theta) p(\\theta)}{p(X)}=\\operatorname{argmaxp}(X \\mid \\theta) p(\\theta)=\\operatorname{argmax}\\left(\\prod_{x 1}^{x n} p(x i \\mid \\theta)\\right) p(\\theta)$$L2正则就是加入了高斯先验，L1正则就是加入了拉普拉斯先验。 贝叶斯估计在MLE和MAP中，都是假设模型参数$\\theta$未知，但都是固定的值，属于点估计，而在贝叶斯估计中，假设模型参数是未知的随机变量，而不是确定值，最终得到的参数不是具体的值，而是一个分布，然后用这个分布的期望来作为最终的参数值。 总结最后让我们用大佬讲义中的片段总结一下本篇的主要内容： 参考资料贝叶斯估计、最大似然估计、最大后验概率估计 极大似然估计、最大后验估计","link":"/2019/01/07/probability/"},{"title":"第十六篇 python(3)-matplotlib绘图","text":"","link":"/2021/08/31/python-2/"},{"title":"python(5) subprocess and logging","text":"subprocessYou can use the Python subprocess module to create new processes, connect to their input and output, and retrieve their return codes and/or output of the process. subprocess runThe subprocess.run() method is a convenient way to run a subprocess and wait for it to complete. Once the subprocess is started, the run() method blocks until the subprocess completes and returns a CompletedProcess object, which contains the return code and output of the subprocess.The check argument is an optional argument of the subprocess.run() function in the Python subprocess module. It is a boolean value that controls whether the function should check the return code of the command being run.When check is set to True, the function will check the return code of the command and raise a CalledProcessError exception if the return code is non-zero. The exception will have the return code, stdout, stderr, and command as attributes. subprocess Popensubprocess.Popen is a lower-level interface to running subprocesses, while subprocess.run is a higher-level wrapper around Popen that is intended to be more convenient to use. Popen allows you to start a new process and interact with its standard input, output, and error streams. It returns a handle to the running process that can be used to wait for the process to complete, check its return code, or terminate it.In general, you should use run if you just need to run a command and capture its output and Popen if you need more control over the process, such as interacting with its input and output streams.The Popen class has several methods that allow you to interact with the process, such as communicate(), poll(), wait(), terminate(), and kill(). subprocess callsubprocess.call() is a function in the Python subprocess module that is used to run a command in a separate process and wait for it to complete. It returns the return code of the command, which is zero if the command was successful, and non-zero if it failed.subprocess.call() is useful when you want to run a command and check the return code, but do not need to capture the output. subprocess check_outputcheck_output is a function in the subprocess module that is similar to run(), but it only returns the standard output of the command, and raises a CalledProcessError exception if the return code is non-zero. Subprocess PipeA pipe is a unidirectional communication channel that connects one process’s standard output to another’s standard input. A pipe can connect the output of one command to the input of another, allowing the output of the first command to be used as input to the second command.Pipes can be created using the subprocess module with the Popen class by specifying the stdout or stdin argument as subprocess.PIPE. loggingLogging provides a set of convenience functions for simple logging usage. These are debug(), info(), warning(), error() and critical().The default level is WARNING, which means that only events of this level and above will be tracked, unless the logging package is configured to do otherwise. logging configlogging.basicConfig(format=’%(levelname)s %(asctime)s %(process)d %(message)s’, level=logging.DEBUG) referencehttps://www.datacamp.com/tutorial/python-subprocesshttps://docs.python.org/3/howto/logging.html","link":"/2023/06/28/python-4/"},{"title":"第十八篇 python(4)-多进程","text":"协程、线程与进程协程","link":"/2021/09/22/python-3/"},{"title":"第十一篇 python(2)-flask+gunicorn+supervisor的python服务部署","text":"","link":"/2021/08/08/python-1/"},{"title":"第九篇 python(1)-语法进阶","text":"yieldyield可以暂停一个函数的运行，返回值给函数调用者，并使得函数可以从上次离开的地方接着运行。通常我们可以借助yield来实现一个生成器。 生成器生成器是一个可以产生一个序列的函数，调用生成器函数会产生一个生成器对象，并不会开始启动函数，只有当执行__next__()时函数才会执行。生成器时一个一次性操作，和我们常见的列表、字典等可以迭代的对象不同，列表等是可以无限次迭代的。 装饰器python中函数是一等对象，所以函数可以像普通变量一样当作参数传给另一个函数的，装饰器可以在不改变另一个函数的情况下用来封装另一个函数以拓展这个被封装函数的功能，有了装饰器，我们就可以抽离出大量与函数功能本身无关的雷同代码到装饰器中并继续重用。 装饰器不仅可以是函数，也可以是类。使用类装饰器主要依靠类的__call__方法。我们可以直接定义一个装饰器函数或者装饰器类，但是有个缺点是原函数的元信息不见了，比如函数的docstring、__name__都会发生改变，此时我们可以使用functools.wraps，wraps本身也是一个装饰器，它能把原函数的元信息拷贝到装饰器里面的函数中。","link":"/2021/07/30/python/"},{"title":"segmentation","text":"Image segmentationImage segmentation is a sub-domain of computer vision and digital image processing which aims at grouping similar regions or segments of an image under their respective class labels. Semantic segmentationSemantic segmentation refers to the classification of pixels in an image into semantic classes. Instance segmentationInstance segmentation models classify pixels into categories on the basis of “instances” rather than classes. Panoptic segmentationPanoptic segmentation can be expressed as the combination of semantic segmentation and instance segmentation where each instance of an object in the image is segregated and the object’s identity is predicted. Neural networks that perform segmentation typically use an encoder-decoder structure where the encoder is followed by a bottleneck and a decoder or upsampling layers directly from the bottleneck (like in the FCN).","link":"/2023/07/18/segmentation/"},{"title":"第五篇 算法(1)--排序","text":"排序排序方法选择排序选择排序的思路是找到数组中最小的数和第一个元素交换位置，然后在剩下的元素中找到最小的元素和第二个元素交换位置，直到最后只剩一个元素，这样就是一个从小到大排好序的数组。选择排序的复杂度为O(n^2)。 插入排序插入排序的思路是将元素插入一个已经排好序的子列表中，直到整个列表都排序完成。插入排序的复杂度为O(n^2)。 冒泡排序冒泡排序的思路是每次对连续的邻居元素进行比较，两个元素是逆序排序就交换位置，否则保持不变，直到所有元素都被排序好。每次进行完一轮比较，就能将最大或者最小的元素移到其最终的位置上，当不再发生交换的时候，就说明元素已经被排好序了，冒泡排序的复杂度是O(n^2)。 归并排序归并排序的思路是利用递归的方法，将数组分为两半，各自进行归并排序的过程。其关键是如何将排好序的两个子数组也排好序，鉴于两个子数组是已经排好序了的，只需要将两个子数组依次比较。归并排序的复杂度是O(nlogn）。 快速排序快速排序的思路是挑出一个中心点，把数组分为两半，其中一半所有元素都小于这个中心点，另一半大于这个中心点，再对这两半进行递归处理，所以快速排序的关键在于这个中心点的选择了。快速排序的复杂度是O(nlogn）。 堆排序堆排序用了二叉堆，将一个数组中的所有元素添加到堆中，然后将堆中最大的元素连续移除以获得一个排好序的数组。一个二叉堆具有如下性质：是一个完全二叉树；每个节点都大于或等于它的子节点，二叉堆通常是用数组实现的，父母节点和子节点的位置满足一定的关系，假如一个在位置i的节点，它的左子节点就在位置2i+1上，右子节点在位置2i+2上。所以堆排序的关键在于二叉堆的建立和维护。堆排序的复杂度是O(nlogn)。 桶排序和基数排序桶排序和基数排序用于排序整数非常有效。 桶排序 ​ 桶排序的思路是加入数组中的元素在0到t的范围内，则把这些元素放入对应的标记上0到t的桶当中，每个桶中的元素值都是相同的。 基数排序 在桶排序中，如果元素范围过大的话，就会需要很多桶，此时就可以用基数排序。基数排序基于桶排序，只是基数排序只会用到十个桶，基于基数位置进行桶排序。 外排序当数据量大到无法一次性载入内存时，使用外排序。外排序的思路就是将大量数据拆分成小块数据，小块数据进行内排序之后，再分别合并排序。 相关java基础数组数组一旦创建了，大小就是固定的。java中声明数组变量的语法是’elementType[] arrayRefVar;’，声明数组只是创造了一个数组引用的存储位置，并没有为这个数组分配内存，创建一个数组可以使用new操作符，例如’array RefVar = new elementType[arraySize];’。 数组的拷贝数组变量是一个数组的引用，直接使用赋值语句只是让两个变量去指向同一个数组（同一片存储空间），要想真正地拷贝数组有几种方式：使用循环对数组中的元素一个一个地拷贝；使用System类中的静态方法arraycopy；使用clone方法。arraycopy的语法是’arraycopy(sourceArray,srcPos,targetArray,tarPos,length);’。","link":"/2020/04/04/sort/"},{"title":"speech embedding","text":"Contrastive Predictive CodingContrastive Predictive Coding (CPC) learns self-supervised representations by predicting the future in latent space by using powerful autoregressive models. The model uses a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It describes a form of unidirectional modeling in the feature space,where the model learns to predict the near future frames inan acoustic sequence while contrasting with frames from othersequences or frames from a more distant time. Autoregressive Predictive CodingThe APC approach uses an autoregressive model to encodetemporal information of past acoustic sequence; the model thenpredicts future frames like a recurrent-based LM whileconditioning on past frames. TERATERA, which stands for Transformer Encoder Representations from Alteration, is a self-supervised speech pre-trainingmethod. experiment designAmount of labeled data needed to perform well.with pre-trained and without pre-trained.","link":"/2023/05/05/speech-embedding/"},{"title":"time complexity","text":"time complexitybig O notationBig O notation measures the asymptotic growth of a function. f (n) = O(g(n)) if for all sufficiently large n, f (n) is at most a constant factor larger than g(n). Ω and Θ notationWe say f (n) = Ω(g(n)) if g(n) = O(f (n)).We say f (n) = Θ(g(n)) if f (n) = O(g(n)) and g(n) = O(f (n)). types of complexityWorst-case complexity: what is the largest possible running time on any input of size n?Average-case complexity: what is the average running time on a random input of size n?Best-case complexity: what is the smallest possible running time on any input of size n? Graph algorithmways of representing graphsadjacency matrixFor graph with n vertices this is an n × n matrix A, where $A_{ij}$ = 1 if there is an edge from node i to node j, $A_{ij}$ = 0 otherwise.If the graph is undirected, the matrix is symmetric adjacency listsFor each vertex, keep a list of its neighbors. incidence matrixThe incidence matrix of an undirected graph with n vertices and m edges is an n × m matrix B where $B_{ij}$ = 1 if the i’th vertex is part of the j’th edge, $B_{ij}$ = 0 otherwise. Two fundamental graph exploration algorithmsDepth First Search (DFS)Breadth First Search (BFS)For the BFS tree, this gives the shortest (fewest number of steps) paths from s to all other nodes Greedy AlgorithmsGreedy algorithms are algorithms that build a solution step by step by always choosing the currently best option. Interval SchedulingInput: A list of intervalsOutput: Maximum number of these intervals that can be chosen without getting any overlaps.Solution:Pick the one that ends first.Prove correctness of such an algorithm: Common strategy for analyzing greedy algorithms: prove that the algorithm always “stays ahead” of the optimal solution. Job Scheduling With Minimum LatenessInput: A list of jobs, each job has a deadline di, and a duration ti(how long it takes to finish the job)Output: Smallest possible maximum lateness in a schedule for doingall jobs.Solution: Pick the job with smallest di. Shortest pathIt is helpful to instead consider a more general problem. Let us tryto find the shortest paths from s to all other vertices: Dijkstra’s algorithm: we have some set D of vertices we have foundthe shortest path to, and each step we add a new vertex to D. add the vertex outside D which is closest tos when using only vertices in D as intermediate vertices. Divide &amp; ConquerAlgorithms that split the input into significantly smaller parts, recursively solves each part, and then combines the subresults (somehow). Merge sortO(n log n). Polynomial multiplication$T(n) = O(n^{1.59})$.Using FFT, get time O(n log n) for Polynomial Multiplication Unit cost model and Bit cost modelUnit cost model: assume all numbers fit in machine registers so that basic arithmetic operations take constant time.Bit cost model: account for size of numbers and the time it takes to manipulate them. Integer multiplicationKaratsuba’s algorithm: $T(n) = O(n^{1.59})$. Master TheoremDynamic ProgrammingSplit a problem into smaller subproblems such that results from onesubproblem can be reused when solving others Fibonacci numbersThe Fibonacci numbers are a classic number sequence inmathematics, defined by the linear recurrencef0 = 0; f1 = 1; and fn = fn−1 + fn−2 for n ≥ 2 Weighted Interval SchedulingInput: A list of intervals [s1; t1]; [s2; t2]; : : : ; [sn; tn], each interval[si; ti] has a weight wiOutput: What is the maximum total weight of these intervals thatcan be chosen without getting any overlaps KnapsackInput: A capacity C and a list of objects, each object has a value viand weight wiOutput: Subset S of objects such that$\\sum_{i∈S} wi ≤ C$ and $\\sum_{i∈S} vi$ is maximized. top-down and bottom-up fashiontop-down fashion: we start at the end result andrecursively compute results for relevant subproblems. bottom-up fashion: we iteratively compute results for larger and larger subproblems. Characteristics of dynamic programmingA problem is amenable to dynamic program if we can define a setof subproblems such that: The number of different subproblems is as small as possible. There is some ordering of subproblems from “small” to “large” The value of a subproblem can be efficiently computed giventhe values of some set of smaller subproblems. Sequence AlignmentInput: Strings x and y of lengths m and n, parameters ‹ and ¸Output: Minimum cost of an alignment of x and y with parameters $\\sigma$ and $\\alpha$.$\\alpha is the cost of aligning two different characters with each other$\\sigma$ is the cost of not aligning a character Matrix Chain MultiplicationNetwork FlowThe Max-Flow problemInput: Flow network G.Output: Flow f maximizing the value v(f ).Solution: The Ford-Fulkerson Algorithm O(C(m + n)) or the scaling algorithm with O(m2log(C)) or Edmonds-Karp algorithm with O(nm(n + m)) . Edge CutsAn edge cut of a graph is a set of edges such that their removal would disconnect the graph. Minimum s-t-CutInput: A flow network G with source s and sink t.Output: An s-t cut A; B of G minimizing the capacity c(A; B). The Max-Flow-Min-Cut TheoremFor every flow network G, the maximum flow from s to t equals theminimum capacity of an s-t cut in G. Vertex CutsA vertex cut in a graph is a set of vertices such that if we removethem, the graph splits into more than one connected component. MatchingsA matching in a graph is a set M of edges such that no vertex appears in more than one edge of M.Of particular interest to us will be bipartite graphs. Maximum Bipartite MatchingInput: A bipartite graph GOutput: A matching M in G of maximum possible size. Edge-Disjoint PathsGiven a directed graph with source and sink, what is maximumnumber of edge-disjoint paths from s to t?(edge-disjoint = no edge used by more than one path) Project Selection?","link":"/2023/05/16/time-complexity/"},{"title":"speech","text":"signalspectrogram: A spectrogram of a time signal is a special two-dimensional representation that displays time in its horizontal axis and frequency in its vertical axis. short-time Fourier analysisWhy use it?Some regions of speech signals shorter than 100 milliseconds often appear to be periodic, so that we can use the exact definition of Fouriertransform. spectral leakageThis phenomenon is called spectral leakage because the amplitude of one harmonic leaks over the rest and masks its value. feature extractionRepresentation of speech signals in the frequency domain is especially useful because the frequency structure of a phoneme is generally unique. Sinusoids are important because speech signals can be decomposed as sums of sinusoids. For voiced sounds there is typically more energy at low frequenciesthan at high frequencies, also called roll-off. To make the spectrograms easier to read, sometimes the signal is first preemphasized (typically with a first-order difference FIR filter) to boost the high frequenciesto counter the roll-off of natural speech. Digital SystemsLinear Time-Invariant Systems and Linear Time-Varying Systems. The Fourier TransformZ-Transformdigital filterfilterbankA filterbank is a collection of filters that span the whole frequency spectrum. short-time analysis","link":"/2023/03/31/speech/"},{"title":"thoughts","text":"","link":"/2022/03/13/thoughts/"},{"title":"toolnotes","text":"背景记录平时学习和开发工程中使用工具的一些备忘点。 正文用vim时，鼠标右键不能粘贴而是进入了visual模式，解决方法：：set mouse-=a 远程jupyter配置https://juejin.cn/post/7026371559971520525 For Debian / Ubuntu: .deb packages installed by apt and dpkgFor Rocky / Fedora / RHEL: .rpm packages installed by yumFor FreeBSD: .txz packages installed by pkg ssh-keygen -t rsa -b 4096 -C “your_email@example.com“ Kill PyTorch Distributed Training Processes:kill $(ps aux | grep YOUR_TRAINING_SCRIPT.py | grep -v grep | awk ‘{print $2}’) git reset –soft HEAD^ 撤销commitgit reset –hard HEAD^ 撤销add tokenizing in Unix: “tr” command 分词文件排序和统计 tr -sc ’A-Za-z’ ’\\n’ &lt; $file_name| sort | uniq -c conda: conda create -n python=3.7 yourenv pip git find . -name “*.py”|xargs git add – 导出项目依赖：pip install pipreqspipreqs ./ –encoding=utf-8 –force PlotNeuralNet https://pub.towardsai.net/creating-stunning-neural-network-visualizations-with-chatgpt-and-plotneuralnet-adab37589e5 remote develophttps://devblogs.microsoft.com/python/remote-python-development-in-visual-studio-code/ references”https://leimao.github.io/blog/Kill-PyTorch-Distributed-Training-Processes/","link":"/2023/01/03/toolnotes/"},{"title":"transformer","text":"IntroductionThe Transformer is a deep learning architecture introduced in the paper “Attention is All You Need” by Vaswani et al., published in 2017.The Transformer is based on the self-attention mechanism, which allows it to capture long-range dependencies in sequences more effectively than traditional recurrent neural networks (RNNs) or convolutional neural networks (CNNs). The key components of the Transformer are:Self-Attention Mechanism,Encoder-Decoder Architecture,Multi-Head Attention,Positional Encoding,Feed-Forward Neural Networks. Self-Attention MechanismThe self-attention mechanism allows the model to weigh the importance of different words in a sentence while encoding the sequence. It computes the attention scores for each word in the input sequence based on its relationships with other words. By attending to relevant words, the model can focus on the most informative parts of the sequence. The first step in calculating self-attention is to create three vectors from each of the encoder’s input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process. The second step in calculating self-attention is to calculate a score. Say we’re calculating the self-attention for the first word in this example, “Thinking”. We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.The score is calculated by taking the dot product of the query vector with the key vector of the respective word we’re scoring. So if we’re processing the self-attention for the word in position #1, the first score would be the dot product of q1 and k1. The second score would be the dot product of q1 and k2. The third and fourth steps are to divide the scores by 8 (the square root of the dimension of the key vectors used in the paper – 64. This leads to having more stable gradients. There could be other possible values here, but this is the default), then pass the result through a softmax operation. Softmax normalizes the scores so they’re all positive and add up to 1. The fifth step is to multiply each value vector by the softmax score. The sixth step is to sum up the weighted value vectors.This produces the output of the self-attention layer at this position (for the first word). Multi-Head AttentionTo capture different types of dependencies and relationships, the Transformer uses multi-head attention. It performs self-attention multiple times with different learned projection matrices, allowing the model to attend to various aspects of the input. With multi-headed attention we have not only one, but multiple sets of Query/Key/Value weight matrices (the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder). Each of these sets is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders/decoders) into a different representation subspace.If we do the same self-attention calculation we outlined above, just eight different times with different weight matrices, we end up with eight different Z matrices.We concat the matrices then multiply them by an additional weights matrix WO to condense these eight down into a single matrix. sequence-to-sequence modelA sequence-to-sequence model is a model that takes a sequence of items (words, letters, features of an images…etc) and outputs another sequence of items.the model is composed of an encoder and a decoder.The encoder processes each item in the input sequence, it compiles the information it captures into a vector (called the context). After processing the entire input sequence, the encoder sends the context over to the decoder, which begins producing the output sequence item by item.The context is a vector (an array of numbers, basically) in the case of machine translation. The encoder and decoder tend to both be recurrent neural networks.By design, a RNN takes two inputs at each time step: an input (in the case of the encoder, one word from the input sentence), and a hidden state. The word, however, needs to be represented by a vector. To transform a word into a vector, we turn to the class of methods called “word embedding” algorithms.The context vector turned out to be a bottleneck for these types of models. It made it challenging for the models to deal with long sentences.A solution was proposed in Bahdanau et al., 2014 and Luong et al., 2015. These papers introduced and refined a technique called “Attention”.Attention allows the model to focus on the relevant parts of the input sequence as needed. attentionAn attention model differs from a classic sequence-to-sequence model in two main ways:First, the encoder passes a lot more data to the decoder. Instead of passing the last hidden state of the encoding stage, the encoder passes all the hidden states to the decoder;Second, an attention decoder does an extra step before producing its output. In order to focus on the parts of the input that are relevant to this decoding time step, the decoder does the following:Look at the set of encoder hidden states it received,Give each hidden state a score,Multiply each hidden state by its softmaxed score, thus amplifying hidden states with high scores, and drowning out hidden states with low scores. Encoder-Decoder ArchitectureThe Transformer architecture consists of two main components: the encoder and the decoder. The encoder takes an input sequence and processes it, while the decoder generates an output sequence based on the encoded representation. One detail in the architecture of the encoder that we need to mention before moving on, is that each sub-layer (self-attention, ffnn) in each encoder has a residual connection around it, and is followed by a layer-normalization step. The encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence.The following steps repeat the process until a special symbol is reached indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did. And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word. In the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to -inf) before the softmax step in the self-attention calculation.The “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack. The decoder stack outputs a vector of floats. How do we turn that into a word? That’s the job of the final Linear layer which is followed by a Softmax Layer.The Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector.The softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step. Layer NormalizationIn traditional normalization techniques like Batch Normalization, the activations of a layer are normalized by computing the mean and variance over a batch of examples. This normalization helps stabilize and accelerate the training process, especially for deeper networks. However, it introduces a dependency on the batch size during training, which can be problematic in scenarios where batch sizes vary or during inference when processing individual samples. Layer Normalization addresses this dependency by computing the mean and variance across all the units within a single layer for each training example. This means that normalization is done independently for each sample and does not rely on batch statistics. Positional EncodingSince Transformers do not inherently have positional information like RNNs, positional encodings are added to the input embeddings. These positional encodings provide the model with information about the order of the elements in the input sequence,or the distance between different words in the sequence. Trainingloss functioncross entropyThe cross-entropy loss calculates the negative log-likelihood of the true class’s predicted probability. Kullback–Leibler divergenceKullback-Leibler (KL) divergence, also known as relative entropy, is a measure of how one probability distribution diverges from another.KL divergence measures the average amount of information lost when using Q to approximate P. It is not symmetric. decodinggreedy decodingIn greedy decoding, at each step of sequence generation, the model selects the most likely output token based on its predicted probability distribution. It chooses the token with the highest probability without considering the impact on future decisions. This means that the model makes locally optimal choices at each step without considering the global context of the entire sequence.For example, in machine translation, a model using greedy decoding will predict each target word one at a time, selecting the word with the highest probability given the source sentence and previously generated words. The process continues iteratively until an end-of-sentence token is generated. Beam searchIn beam search, instead of selecting only the most likely token at each step, the algorithm maintains a fixed-size list, known as the “beam,” containing the most promising candidate sequences. The beam size determines how many candidate sequences are considered at each decoding step.At the beginning of the decoding process, the beam is initialized with a single token representing the start of the sequence. At each step, the model generates the probabilities for the next possible tokens and expands the beam with the top-k most likely candidate sequences based on their cumulative probabilities. The k represents the beam size, and higher values of k result in a more diverse exploration of possibilities. referenceshttp://fancyerii.github.io/2019/03/09/transformer-illustrated/#%E6%A6%82%E8%BF%B0https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/http://jalammar.github.io/illustrated-transformer/https://colah.github.io/posts/2015-09-Visual-Information/https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained","link":"/2023/07/19/transformer/"},{"title":"object detection","text":"Object detectionObject detection is the field of computer vision that deals with the localization and classification of objects contained in an image or video.Deep learning-based approaches use neural network architectures like RetinaNet, YOLO (You Only Look Once), CenterNet, SSD (Single Shot Multibox detector), Region proposals (R-CNN, Fast-RCNN, Faster RCNN, Cascade R-CNN) for feature detection of the object, and then identification into labels.The YOLO series current provide the SOTA of object detection in real-time. Object detection usually consists of the following parts:Input: Refers to the input of the pictureBackbone: A skeleton pre-trained on ImageNetNeck: Usually used to extract feature maps of different levelsHead: Predict the object category and the detector of bndBox, usually divided into two types: Dense Prediction (one stage), Sparse Prediction (two stage). metricmAPMean average precision (mAP) is the average value of AP of each category.The AP metric is the area under curve (AUC) of PR curve (Precision-Recall curve).This metric provides a balanced assessment of precision and recall by considering the area under the precision-recall curve. PR curve is a curve drawn with Recall as the X axis and Precision as the Y axis. The higher the Precision and Recall, the better the performance of the model, so the closer to the upper right corner, the better. The AP metric incorporates the Intersection over Union (IoU) measure to assess the quality of the predicted bounding boxes. If the IOU is greater than the threshold (Threshold, usually set to 0.5), and the same Ground Truth can only be calculated once, it will be considered as a TP. Intersection over Union(IoU)IoU is the ratio of the intersection area to the union area of the predicted bounding box and the ground truth bounding box. It measures the overlap between the ground truth and predicted bounding boxes. Flops and FPSFLOPS (Floating-Point Operations Per Second) is a measure of a computer’s or a processor’s performance in terms of the number of floating-point operations it can perform per second.Higher FLOPS values generally indicate faster computational capabilities.FPS (Frames Per Second) is a measure of how many individual frames (images) a video system can display or process per second. Non-Maximum Suppression (NMS)Non-Maximum Suppression (NMS) is a post-processing technique used in object detection algorithms to reduce the number of overlapping bounding boxes and improve the overall detection quality. Model HistoryTraditionally, object detection is done by Viola Jones Detector \\cite{viola2001rapid}, Histogram of Oriented Gradients (HOG) detector, or Deformable Part-based Model (DPM) before deep learning took off. With deep learning, object detection generally is categorized into 2 categories: one-stage detector and two-stage detector. Two-stage detector is started by Regions with CNN features (RCNN). Spatial Pyramid Pooling Networks (SPPNet), Fast RCNN, Faster RCNN, and Feature Pyramid Networks (FPN) were proposed after it. Limited by the poor speed of the two-stage detector, the one-stage detector came with the first representative You Only Look Once (YOLO). Subsequent versions of YOLO, Single Shot MultiBox Detector (SSD), RetinaNet, CornerNet, CenterNet,DETR were proposed latter. YOLOv7 performs best compared to most detectors. RCNNThe object detection system consists of three modules. The first generates category-independent region proposals. These proposals define the set of candidate detections available to our detector. The second module is a large convolutional neural network that extracts a fixed-length feature vector from each region. The third module is a set of classspecific linear SVMs. YOLO seriesThe history of YOLO (You Only Look Once) dates back to 2015 when the original YOLO algorithm was introduced in “You Only Look Once: Unified, Real-Time Object Detection,” .The original YOLO architecture used a convolutional neural network (CNN) to process the entire image and output a fixed number of bounding boxes along with their associated class probabilities. It divided the image into a grid and applied convolutional operations to predict bounding boxes within each grid cell, considering multiple scales and aspect ratios.In subsequent years, YOLO underwent several iterations and improvements to enhance its accuracy and speed. YOLOv2 was introduced in 2016, featuring an updated architecture that incorporated anchor boxes and multi-scale predictions. YOLOv3 followed in 2018, introducing further advancements, including feature pyramid networks (FPN) and Darknet-53 as the backbone architecture. YOLO (You Only Look Once)Network architecture is inspired by the GoogLeNet model for image classification.The network has 24 convolutional layers followed by 2 fully connected layers. They pretrain our convolutional layers on the ImageNet 1000-class competition dataset.For pretraining they use the first 20 convolutional layers followed by a average-pooling layer and a fully connected layer. Then they add four convolutional layers and two fully connected layers with randomly initialized weights. The final layer predicts both class probabilities and bounding box coordinates. They optimize for sum-squared error in the output of the model by increasing the loss from bounding box coordinate predictions and decreasing the loss from confidence predictions for boxes that don’t contain objects and predicting the square root of the bounding box width and height instead of the width and height directly. They design the loss to handle the problem that the sum-squared error weights localization error equally with classification error and also equally weights errors in large boxes and small boxes. YOLOv2The improvements of YOLOv2 in YOLOv1:The author adds a batch normalization layer after each convolutional layer, no longer uses dropout.YOLOv1 uses a 224x224 image classifier. YOLO2 increases the resolution to 448x448.Because YOLOv1 has difficulty learning to adapt to the shape of different objects during training, resulting in poor performance in precise positioning. YOLOv2 also tries to use rectangles of different shapes as anchor boxes (Anchor Box).Unlike YOLOv1, Anchor Box does not directly predict the coordinate value of bndBox, but predicts the offset (offset value of coordinates) and confidence scores (confidence) of Anchor Box.In Faster R-CNN and SSD, the size of the Anchor Box is manually selected.YOLOv2 uses the k-means clustering method to perform cluster analysis on the bndBox of the objects in the training set.YOLOv2 uses a new basic model (feature extractor) Darknet-19, including 19 convolutional layers, 5 maxpooling layers. YOLO9000YOLO9000 is a model that can detect more than 9,000 categories proposed on the basis of YOLOv2. Its main contribution is to propose a joint training strategy for classification and detection.For the detection data set, it is used to learn the bounding box (bndBox), confidence (confidence) and object classification of the predicted object, while for the classification data set, it is only used to learn classification, but it can greatly expand the capabilities of the model the type of object detected. The author proposes a hierarchical classification method (Hierarchical classification),which establishes a tree structure WordTree according to the affiliation between categories.When softmax is performed, it is not performed on all categories, but on the categories of the same level.When making predictions, it traverses down from the root node, selects the child node with the highest probability at each level, and calculates the product of all conditional probabilities from the node to the root node. Stop when the product of the conditional probability is less than a certain threshold, and use the current node to represent the predicted category. YOLOv3On the basis of YOLOv2, YOLOv3 improves the network backbone, uses multi-scale feature maps (feature maps) for detection, and uses multiple independent Logistic regression classifiers instead of softmax to predict category classification.YOLOv3 proposes a new backbone: Darknet-53, from layer 0 to layer 74, a total of 53 convolutional layers, and the rest are Resnet layers.Darknet-53 joins Resnet Network (Residual Network) to solve the gradient problem.YOLOv3 draws on the Feature Pyramid Network (FPN) method, uses multi-scale feature maps to detect objects of different sizes, and improves the prediction ability of small objects.The feature map of each scale will predict 3 Anchor priors, and the size of the Anchor priors is clustered using K-means. Feature Pyramid Networks (FPN)The main idea behind FPNs is to leverage the nature of convolutional layers — which reduce the size of the feature space and increase the coverage of each feature in the initial image — to output predictions at different scales.FPNs provide semantically strong features at multiple scales which make them extremely well suited for object detection. YOLOv4Bag-of-Freebies refers to the techniques used in network training, which does not affect the time of reasoning and prediction, mainly including:Data augmentation: Random erase, CutOut, Hide-and-seek, Grid mask, GAN, MixUp, CutMix;Regularization methods: DropOut, DropConnect;Dealing with data imbalance: focal loss, Online hard example mining, Hard negative example mining;Handle bndBox regression problems: MSE, IOU, GIOU, DIOU/CIOU. Bag-of-specials refers to the techniques used in network design or post-processing, which slightly increases the time of reasoning and prediction, but can improve the accuracy, mainly including:Receptive field: SPP, ASPP, RFB;Feature Fusion: FPN, PAN;Attention mechanism: attention module;Activation functions: Swish, Mish;NMS: Soft-NMS、DIoU NMS. The architecture of the YOLOv4 model consists of three partsBackBone: CSPDarknet53; Neck: SPP+PAN; HEAD: YOLO HEAD. Cross Stage Partial Network (CSPNet)The main purpose of CSPNet is to enable the network architecture to obtain richer gradient fusion information and reduce the amount of calculation.The method is to first divide the feature map of the Base layer into two parts, and then pass through transition -&gt; concatenation -&gt; transition. parts merged.This approach allows CSPNet to solve three problems:Increase the learning ability of CNN, even if the model is lightweight, it can maintain accuracy;Remove the computing bottleneck structure with high computing power (reduce computing);Reduce memory usage. SPP+PANSPP (Spatial Pyramid Pooling): Concate all feature maps in the last layer of the network, and then continue to connect CNN module.PANet (Path Aggregation Network): Improve on the basis of FPN. CutMixCutMix is ​​a data enhancement method proposed in 2019. The method is to cut off a part of the area but not fill it with 0 pixels, but randomly fill the area pixel values ​​​​of other data in the training set.Mixup: Mix two random samples proportionally, and the classification results are distributed proportionally.utout: Randomly cut out some areas in the sample and fill them with 0 pixel values, and the classification result remains unchanged. Mosaic data augmentationWhilst common transforms in object detection tend to be augmentations such as flips and rotations, the YOLO authors take a slightly different approach by applying Mosaic augmentation; which was previously used by YOLOv4, YOLOv5 and YOLOX models.The objective of mosaic augmentation is to overcome the observation that object detection models tend to focus on detecting items towards the centre of the image. The key idea is that, if we stitch multiple images together, the objects are likely to be in positions and contexts that are not normally observed in images seen in the dataset; which should force the features learned by the model to be more position invariant. It uses random scaling and cropping to mix and stitch 4 kinds of pictures for training. When using Mosaic training, the data of 4 pictures can be directly calculated, so that the size of the Mini-batch does not need to be large. Post-mosaic affine transformsAs we noted earlier, the mosaics that we are creating are significantly bigger than the image sizes we will use to train our model, so we will need to do some sort of resizing here. Whilst this would work, this is likely to result in some very small objects, as we are essentially resizing four images to the size of one - which is likely to become a problem where the domain already contains very small bounding boxes. Additionally, each of our mosaics are structurally quite similar, with an image in each quadrant. Recalling that our aim was to make the model more robust to position changes, this may not actually help that much; as the model is likely just to start looking in the middle of each quadrant.To overcome this, one approach that we can take is to simply take a random crop from our mosaic. This will still provide the variability in positioning whilst preserving the size and aspect ratio of the target objects. At this point, it may also be a good opportunity to add in some other transforms such as scaling and rotation to add even more variability. DropBlock regularizationDropout, which randomly deletes the number of neurons, but the network can still learn the same information from adjacent activation units.DropBlock randomly deletes the entire local area, and the network will focus on learning certain features to achieve correct classification and get better generalization effects. Class label smoothingIn multi-classification tasks, the output is usually normalized with softmax, and then one-hot label is used to calculate the cross-entropy loss function to train the model. However, the use of one-hot vector representation can easily lead to the problem of network overfitting, so Label Smoothing is to make the one-hot label softer, so that the phenomenon of overfitting can be effectively suppressed when calculating the loss, and the generalization ability of the model can be improved. Mish activationMish is a continuously differentiable non-monotonic activation function. Compared with ReLU, Mish’s gradient is smoother, and it allows a smaller negative gradient when it is negative, which can stabilize the network gradient flow and has better generalization ability.$f(x) = xtanh(ln(1+e^x))$. Multiinput weighted residual connections (MiWRC)YOLOv4 refers to the architecture and method of EfficientDet , and uses the multi-input weighted residual connection (MiWRC).The backbone of EfficientDet uses EfficientNet, Neck is BiFPN.EfficientNet-B0 is constructed by multiple MBConv Blocks. MBConv Block refers to the Inverted Residual Block of MobileNet V2.The design of MBConv is to first increase the dimension and then reduce the dimension, which is different from the operation of the residual block to first reduce the dimension and then increase the dimension. This design allows MobileNetV2 to better use the residual connection to improve Accuracy.The idea of ​​MiWRC is derived from BiFPN. In FPN, the features obtained by each layer are regarded as equal, while MiWRC believes that the features of different layers should have different importance, and different weight ratios should be given to the features of different scales. loss2 problems with using IOU loss:When the predict box (predict bndBox) and the target box (ground truth) do not intersect, the IOU is 0, which cannot reflect the distance between the two boxes. At this time, the loss function is not derivable, that is to say, the gradient cannot be calculated, so it cannot Optimizing the case where two boxes do not intersect;The IOU cannot reflect the coincidence size of the prediction frame and the target frame.Subsequent GIoU, DIoU, CIoU are based on IOU loss to add a penalty item: GIOU loss (Generalized IOU loss):C is the minimum bounding box of the target box Ground Truth and the prediction box Predict. $L_{GIOU}=1-IOU+\\frac{|C-B\\cupB^{gt}|}{|C|}$. DIOU loss (Distance IOU loss) considers the overlapping area and the center point distance, and adds a penalty term to minimize the center point distance between the two boxes.CIOU loss (Complete IOU loss) adds a penalty item based on DIOU, taking into account the factor of aspect ratio. CmBN (Cross mini-Batch Normalization) BN is to normalize the current mini-batch, but often the batch size is very small, and uneven sampling may occur, which may cause problems in normalization. Therefore, there are many Batch Normalization methods for small batch sizes.The idea of ​​CBN is to calculate the previous mini-batch together, but not keep too many mini-batches. The method is to normalize the results of the current and the current 3 mini-batches.The CmBN newly created by YOLOv4 is based on CBN for modification, and does not update calculations between mini-batches, but updates network parameters after a batch is completed. Self-Adversarial Training (SAT) SAT is a data enhancement method innovated by the author, which is completed in two stages:First, forward-propagate the training samples, and then modify the image pixels (without modifying the network weights) during back-propagation to reduce the performance of model detection. In this way, the neural network can perform adversarial attacks on itself. Creates the illusion that there is no detected object in the picture. This first stage is actually increasing the difficulty of training samples.The second stage is to use the modified pictures to train the model. Eliminate grid sensitivity The author observed a video of object detection and found that because the center point of the detected object is mostly located close to the center of the Grid, it is difficult to detect when it is on the edge of the Grid. The author believes that the problem that the center point of the detected object is mostly located close to the center point of the Grid is because of the gradient of the Sigmoid function. Therefore, the author made some changes in the Sigmoid function, multiplying Sigmoid by a value greater than 1, and taking into account the sensitivity of different Grid sizes to boundary effects, using (1+x)Sigmoid — (0.5x), where When the Grid resolution is higher, the x will be higher. Cosine annealing scheduler Cosine annealing is to use the cosine function to adjust the learning rate. At the beginning, the learning rate will be slowly reduced, then accelerated halfway, and finally slowed down again. Optimal hyperparameters Use Genetic Algorithms (Evolutionary Algorithms) to select hyperparameters. The method is to randomly combine hyperparameters for training, then select the best 10% hyperparameters and then randomly combine and train them, and finally select the best model. SAM-block (Spatial Attention Module) SAM is derived from the CBAM (Convolutional Block Attention Module) paper, which provides two attention mechanism techniques. DIoU-NMS In the classic NMS, the detection frame with the highest confidence and other detection frames will calculate the corresponding IOU value one by one, and the detection frame whose value exceeds the threshold is filtered out. But in the actual situation, when two different objects are very close, due to the relatively large IOU value, after the NMS algorithm, there is often only one detection frame left, which may cause missed detection.DIoU-NMS considers not only the IOU value, but also the distance between the center points of two boxes. If the IOU between the two frames is relatively large, but the distance between them is relatively far, it will be considered as the detection frame of different objects and will not be filtered out. YOLOv7Anchor boxesYOLOv7 family is an anchor-based model.In these models, the general philosophy is to first create lots of potential bounding boxes, then select the most promising options to match to our target objects; slightly moving and resizing them as necessary to obtain the best possible fit.The basic idea is that we draw a grid on top of each image and, at each grid intersection (anchor point), generate candidate boxes (anchor boxes) based on a number of anchor sizes. That is, the same set of boxes is repeated at each anchor point. However, one issue with this approach is that our target, ground truth, boxes can range in size — from tiny to huge! Therefore, it is usually not possible to define a single set of anchor sizes that can be matched to all targets. For this reason, anchor-based model architectures usually employ a Feature-Pyramid-Network (FPN) to assist with this. Center PriorsIf we put 3 anchor boxes in each anchor point of each of the grids, we end up with a lot of boxes.The issue is that most of these predictions are not going to contain an object, which we classify as ‘background’.To make the problem cheaper computationally, the YOLOv7 loss finds first the anchor boxes that are likely to match each target box and treats them differently — these are known as the center prior anchor boxes. This process is applied at each FPN head, for each target box, across all images in batch at once. model reparameterizationModel re-parametrization techniques merge multiple computational modules into one at inference stage. The model re-parameterization technique can be regarded as an ensemble technique, and we can divide it into two categories, i.e., module-level ensemble and model-level ensemble. Model scalingModel scaling is a way to scale up or down an already designed model and make it fit in different computing devices.Network architecture search (NAS) is one of the commonly used model scaling methods. efficient layer aggregation networks(ELAN)VovNet/OSANetVovNet, short for “Variance-based Overparameterized Convolutional Networks,” is a convolutional neural network (CNN) architecture proposed by Lee et al. in their paper “Variance-based Overparameterization for Robustness” in 2019. VovNet is designed to improve the robustness of deep neural networks, particularly in the context of image classification tasks.The key idea behind VovNet is to introduce variance-based overparameterization to enhance the representation power of CNNs. Overparameterization involves increasing the number of parameters in a neural network, which can improve the model’s ability to learn complex patterns and features.VovNet achieves variance-based overparameterization by introducing multiple “VovNet blocks.” Each VovNet block is designed to capture different levels of granularity within the input data. Instead of using a single set of convolutional filters for all spatial dimensions, VovNet employs different filters for each spatial dimension. This allows the network to capture variations in features at different scales, leading to more robust representations. One-shot aggregation (OSA) module is designed which is more efficient than Dense Block in DenseNet.By cascading OSA module, an efficient object detection network VoVNet is formed.One-shot aggregation (OSA) module is designed to aggregate its feature in the last layer at once.It has much less Memory access cost (MAC) than that with dense block.Also, OSA improves GPU computation efficiency. The input sizes of intermediate layers of OSA module are constant. Hence, it is unnecessary to adopt additional 1×1 conv bottleneck to reduce dimension. The means it consists of fewer layers. CSPVOVNetIt combines CSPNet and VoVNet and considers the gradient path for improvement, so that the weights of different layers can learn more diverse features to improve accuracy. Deep supervisionWhen training deep networks, auxiliary head and auxiliary classifiers are often added to the middle layer of the neural network to improve stability, convergence speed, and avoid gradient disappearance problems, that is, to use auxiliary loss for shallow layers. Network weights for training, this technique is called Deep Supervision. dynamic label assignmentLabel Assigner is a mechanism that considers the network prediction results together with the ground truth and then assigns soft labels. In the past, the definition of the target label was usually to use a hard label that follows the ground truth. In recent years, it has also been used to perform some optimization operations on the prediction results of the model and the ground truth to obtain a soft label. This mechanism is called label assigner in this paper.The author discusses three methods of assigning soft labels on the auxiliary head and lead head: Independent, Lead head guided label assigner,Coarse-to-fine lead head guided label assigner.Independent:Auxiliary head and lead head perform label assignment with ground truth respectively, which is the most used method at present.Lead head guided label assigner:Since the lead head has a stronger learning ability than the auxiliary head, the soft label obtained by optimizing the prediction result of the lead head and the ground truth can better express the distribution and correlation between the data and the ground truth.Then use the soft label as the target of the auxiliary head and lead head for training, so that the shallower auxiliary head can directly learn the information that the lead head has learned, while the lead head pays more attention to the unlearned residual information. Coarse-to-fine lead head guided label assigner:This part is also the soft label obtained by optimizing the prediction result of the lead head and the ground truth. The difference is that two different soft labels will be generated: coarse label and fine label, where the fine label is the same as the soft label of the lead head , coarse label is used for auxiliary head. Optimal Transport AssignmentThe simplest approach is to define an Intersection over Union (IoU) threshold and decide based on that. While this generally works, it becomes problematic when there are occlusions, ambiguity or when multiple objects are very close together. Optimal Transport Assignment (OTA) aims to solve some of these problems by considering label assignment as a global optimization problem for each image.YOLOv7 implements simOTA (introduced in the YOLOX paper), a simplified version of the OTA problem. Model EMAWhen training a model, it can be beneficial to set the values for the model weights by taking a moving average of the parameters that were observed across the entire training run, as opposed to using the parameters obtained after the last incremental update. This is often done by maintaining an exponentially weighted average (EMA) of the model parameters, in practice, this usually means maintaining another copy of the model to store these averaged weights. This technique has been employed in several training schemes for popular models such as training MNASNet, MobileNet-V3 and EfficientNet. The approach to EMA taken by the YOLOv7 authors is slightly different to other implementations as, instead of using a fixed decay, the amount of decay changes based on the number of updates that have been made. Loss algorithmwe can break down the algorithm used in the YOLOv7 loss calculation into the following steps: For each FPN head (or each FPN head and Aux FPN head pair if Aux heads used):Find the Center Prior anchor boxes.Refine the candidate selection through the simOTA algorithm. Always use lead FPN heads for this.Obtain the objectness loss score using Binary Cross Entropy Loss between the predicted objectness probability and the Complete Intersection over Union (CIoU) with the matched target as ground truth. If there are no matches, this is 0.If there are any selected anchor box candidates, also calculate (otherwise they are just 0): The box (or regression) loss, defined as the mean(1 - CIoU) between all candidate anchor boxes and their matched target. The classification loss, using Binary Cross Entropy Loss between the predicted class probabilities for each anchor box and a one-hot encoded vector of the true class of the matched target.If model uses auxiliary heads, add each component obtained from the aux head to the corresponding main loss component (i.e., x = x + aux_wt*aux_x). The contribution weight (aux_wt) is defined by a predefined hyperparameter.Multiply the objectness loss by the corresponding FPN head weight (predefined hyperparameter). Multiply each loss component (objectness, classification, regression) by their contribution weight (predefined hyperparameter). Sum the already weighted loss components. Multiply the final loss value by the batch size. using yolov7github address: https://github.com/WongKinYiu/yolov7;Format converter:https://github.com/wy17646051/UA-DETRAC-Format-Converter potential ideasefficiencyIn order to enhance the real-time detection of the network, researchers generally analyze the number of parameters, calculation amount and calculation density from the aspects of model parameters, calculation amount, memory access times, input-output channel ratio, element-wise operation, etc. In fact, these research methods are similar to ShuffleNetV2 at that time. NAS(Neural Architecture Search)NAS was an inspiring work out of Google that lead to several follow up works such as ENAS, PNAS, and DARTS. It involves training a recurrent neural network (RNN) controller using reinforcement learning (RL) to automatically generate architectures. Vision TransformerThe core conclusion in the original ViT paper is that when there is enough data for pre-training, ViT’s performance will exceed CNN, breaking through the limitation of transformer lack of inductive bias, you can use it in Better transfer results in downstream tasks. However, when the training data set is not large enough, the performance of ViT is usually worse than that of ResNets of the same size, because Transformer lacks inductive bias compared with CNN, that is, a priori knowledge, a good assumption in advance. improve choosing anchor boxdatasetsPASCAL VOC 2007, VOC 2012, Microsoft COCO (Common Objects in Context).UA-DETRAC: https://detrac-db.rit.albany.edu/ https://www.kaggle.com/datasets/patrikskalos/ua-detrac-fix-masks-two-wheelers?resource=download https://colab.research.google.com/github/hardik0/Multi-Object-Tracking-Google-Colab/blob/main/Towards-Realtime-MOT-Vehicle-https://github.com/hardik0/Towards-Realtime-MOT/tree/masterTracking.ipynb#scrollTo=y6KZeLt9ViDehttps://github.com/wy17646051/UA-DETRAC-Format-Converter/tree/mainMIO-TCD:https://tcd.miovision.com/KITTI:https://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmarkTRANCOS: https://gram.web.uah.es/data/datasets/trancos/index.htmlSTREETS:https://www.kaggle.com/datasets/ryankraus/traffic-camera-object-detection: single classVERI-Wild: https://github.com/PKU-IMRE/VERI-Wild https://universe.roboflow.com/7-class/11-11-2021-09.41https://universe.roboflow.com/szabo/densitytrafficcontroller-1axlmhttps://universe.roboflow.com/future-institute-of-technology-1wuwl/indian-vehicle-set-1https://universe.roboflow.com/cv-2022-kyjj6/tesihttps://universe.roboflow.com/vehicleclassification-kxtkb/vehicle_classification-fvssnhttps://universe.roboflow.com/urban-data/urban-datahttps://www.kaggle.com/datasets/ashfakyeafi/road-vehicle-images-datasethttps://github.com/MaryamBoneh/Vehicle-Detection Referenceshttps://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173https://medium.com/ching-i/yolo%E6%BC%94%E9%80%B2-1-33220ebc1d09https://medium.com/ching-i/yolo%E6%BC%94%E9%80%B2-2-85ee99d114a1https://medium.com/@chingi071/yolo%E6%BC%94%E9%80%B2-3-yolov4%E8%A9%B3%E7%B4%B0%E4%BB%8B%E7%B4%B9-5ab2490754efhttps://zhuanlan.zhihu.com/p/183261974https://sh-tsang.medium.com/review-vovnet-osanet-an-energy-and-gpu-computation-efficient-backbone-network-for-real-time-3b26cd035887https://medium.com/ching-i/yolo%E6%BC%94%E9%80%B2-yolov7-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80-97b0e914bdbehttps://towardsdatascience.com/yolov7-a-deep-dive-into-the-current-state-of-the-art-for-object-detection-ce3ffedeeaebhttps://towardsdatascience.com/neural-architecture-search-limitations-and-extensions-8141bec7681fhttps://learnopencv.com/fine-tuning-yolov7-on-custom-dataset/#The-Training-Experiments-that-We-Will-Carry-Outhttps://learnopencv.com/yolov7-object-detection-paper-explanation-and-inference/","link":"/2023/05/05/object-detection/"}],"tags":[{"name":"deep learning","slug":"deep-learning","link":"/tags/deep-learning/"},{"name":"tools","slug":"tools","link":"/tags/tools/"},{"name":"machine learning","slug":"machine-learning","link":"/tags/machine-learning/"},{"name":"Markdown","slug":"Markdown","link":"/tags/Markdown/"},{"name":"Algorithm","slug":"Algorithm","link":"/tags/Algorithm/"},{"name":"KTH","slug":"KTH","link":"/tags/KTH/"},{"name":"algorithm","slug":"algorithm","link":"/tags/algorithm/"},{"name":"reinforcement learning","slug":"reinforcement-learning","link":"/tags/reinforcement-learning/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"SQL","slug":"SQL","link":"/tags/SQL/"},{"name":"big data","slug":"big-data","link":"/tags/big-data/"},{"name":"git","slug":"git","link":"/tags/git/"},{"name":"computer vision","slug":"computer-vision","link":"/tags/computer-vision/"},{"name":"java","slug":"java","link":"/tags/java/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"probability","slug":"probability","link":"/tags/probability/"},{"name":"python","slug":"python","link":"/tags/python/"},{"name":"speech","slug":"speech","link":"/tags/speech/"}],"categories":[{"name":"theory","slug":"theory","link":"/categories/theory/"},{"name":"algorithm","slug":"algorithm","link":"/categories/algorithm/"},{"name":"others","slug":"others","link":"/categories/others/"},{"name":"database","slug":"database","link":"/categories/database/"},{"name":"development tools","slug":"development-tools","link":"/categories/development-tools/"},{"name":"programming language","slug":"programming-language","link":"/categories/programming-language/"},{"name":"computer system","slug":"computer-system","link":"/categories/computer-system/"},{"name":"web development","slug":"web-development","link":"/categories/web-development/"}],"pages":[{"title":"categories","text":"","link":"/categories/index.html"}]}