<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>s-serenity</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="s-serenity"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="s-serenity"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Recording and sharing my learning process."><meta property="og:type" content="blog"><meta property="og:title" content="s-serenity"><meta property="og:url" content="http://yoursite.com/"><meta property="og:site_name" content="s-serenity"><meta property="og:description" content="Recording and sharing my learning process."><meta property="og:locale" content="en_US"><meta property="og:image" content="http://yoursite.com/img/og_image.png"><meta property="article:author" content="s-serenity"><meta property="article:tag" content="Algorithm study development"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://yoursite.com/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://yoursite.com"},"headline":"s-serenity","image":["http://yoursite.com/img/og_image.png"],"author":{"@type":"Person","name":"s-serenity"},"publisher":{"@type":"Organization","name":"s-serenity","logo":{"@type":"ImageObject","url":"http://yoursite.com/img/logo.svg"}},"description":"Recording and sharing my learning process."}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.1.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="s-serenity" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-11-04T12:59:32.000Z" title="2024-11-4 8:59:32 ├F10: PM┤">2024-11-04</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-11-04T13:19:26.900Z" title="2024-11-4 9:19:26 ├F10: PM┤">2024-11-04</time></span><span class="level-item"><a class="link-muted" href="/categories/computer-science/">computer science</a></span><span class="level-item">3 minutes read (About 394 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/11/04/cn0/">计算机网络(1) - IP地址</a></p><div class="content"><h2><span id="ip地址">IP地址</span></h2><p>IP地址（Internet Protocol Address）是用于标识互联网上每台设备的唯一地址。它允许设备之间进行通信和数据传输。IP地址由32位（IPv4）或128位（IPv6）的二进制数构成，通常以点分十进制（IPv4）或冒号分隔的十六进制（IPv6）的形式表示。</p>
<h3><span id="ipv4地址">IPv4地址</span></h3><p>IPv4地址由四组数字组成，每组数字的范围是0到255，例如：192.168.1.1。这四组数字分别代表了IP地址的四个八位段（octet），每八位段用一个点（.）分隔。</p>
<p>IP地址根据其结构被分为A、B、C、D和E五类，其中A、B、C类是最常见的用于公共网络的地址类别。D类用于多播，E类用于实验和未来使用。</p>
<p>A类地址：范围从0.0.0.0到127.255.255.255，默认子网掩码为255.0.0.0（/8）。<br>B类地址：范围从128.0.0.0到191.255.255.255，默认子网掩码为255.255.0.0（/16）。<br>C类地址：范围从192.0.0.0到223.255.255.255，默认子网掩码为255.255.255.0（/24）。<br>D类地址：范围从224.0.0.0到239.255.255.255，用于多播。<br>E类地址：范围从240.0.0.0到255.255.255.255，保留用于实验和未来使用。</p>
<h3><span id="ipv6地址">IPv6地址</span></h3><p>IPv6地址是为了解决IPv4地址耗尽问题而设计的下一代IP协议。IPv6地址由八组四个十六进制数构成，例如：2001:0db8:85a3:0000:0000:8a2e:0370:7334。这些十六进制数用冒号（:）分隔。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-11-02T10:11:19.000Z" title="2024-11-2 6:11:19 ├F10: PM┤">2024-11-02</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-11-03T02:08:29.719Z" title="2024-11-3 10:08:29 ├F10: AM┤">2024-11-03</time></span><span class="level-item">2 minutes read (About 373 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/11/02/algo4/">算法（5）- 比特运算</a></p><div class="content"><h2><span id="比特运算">比特运算</span></h2><p>比特运算（Bitwise operations）是直接对整数的二进制位进行操作的运算。在Python中，比特运算符可以用来执行按位与（AND）、按位或（OR）、按位异或（XOR）、按位非（NOT）、左移（左移位）和右移（右移位）等操作。</p>
<h3><span id="brian-kernighan算法">Brian Kernighan算法</span></h3><p>Brian Kernighan算法是一种用于高效计算一个整数中二进制表示下1的个数的算法。这个算法的核心思想是利用位运算来快速减少计数的过程。算法基于这样一个观察：对于任意一个非零整数n，n和n-1进行按位与操作（n &amp; (n - 1)）的结果会将n的二进制表示中最右边的一个1变为0。</p>
<h3><span id="位运算的技巧">位运算的技巧</span></h3><p>通过与1进行按位与操作可以取得最右位并且判断一个数是奇数还是偶数。通过与(n-1)进行按位与操作可以清零最低位的1。对于十进制整数 x，我们可以用 x &amp; (1 &lt;&lt; k) 来判断 x 二进制表示的第 k 位（最低位为第 0 位）是否为 1。</p>
<h2><span id="参考文献">参考文献</span></h2><p><a target="_blank" rel="noopener" href="https://leetcode.cn/problems/counting-bits/solutions/627418/bi-te-wei-ji-shu-by-leetcode-solution-0t1i/?envType=study-plan-v2&amp;envId=leetcode-75">https://leetcode.cn/problems/counting-bits/solutions/627418/bi-te-wei-ji-shu-by-leetcode-solution-0t1i/?envType=study-plan-v2&amp;envId=leetcode-75</a></p>
<p><a target="_blank" rel="noopener" href="https://leetcode.cn/problems/minimum-flips-to-make-a-or-b-equal-to-c/solutions/101777/huo-yun-suan-de-zui-xiao-fan-zhuan-ci-shu-by-lee-2/?envType=study-plan-v2&amp;envId=leetcode-75">https://leetcode.cn/problems/minimum-flips-to-make-a-or-b-equal-to-c/solutions/101777/huo-yun-suan-de-zui-xiao-fan-zhuan-ci-shu-by-lee-2/?envType=study-plan-v2&amp;envId=leetcode-75</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-11-02T06:44:41.000Z" title="2024-11-2 2:44:41 ├F10: PM┤">2024-11-02</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-11-02T07:34:55.353Z" title="2024-11-2 3:34:55 ├F10: PM┤">2024-11-02</time></span><span class="level-item">3 minutes read (About 433 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/11/02/algo3/">算法（4）- 前缀树</a></p><div class="content"><h2><span id="前缀树">前缀树</span></h2><p>前缀树（Trie树），也称为字典树或单词查找树，是一种树形数据结构，专门用于高效存储和检索字符串集合中的词项。</p>
<p>前缀树是一种多叉树结构，其中每个节点代表一个字符串前缀，从根节点到任一节点的路径上的字符序列构成该节点对应的前缀。</p>
<h3><span id="前缀树的应用">前缀树的应用</span></h3><p>搜索引擎使用前缀树来快速检索和匹配用户输入的查询词。当用户输入一个查询词时，搜索引擎可以从前缀树的根节点开始，根据查询词中的字符逐个向下遍历节点，直到找到匹配的单词或其前缀。</p>
<p>搜索引擎可以利用前缀树提供自动补全建议和拼写检查。由于前缀树存储了大量的单词和它们的前缀，搜索引擎可以快速地根据用户已经输入的部分单词（前缀）给出完整的单词建议，或者指出拼写错误并提供正确的拼写。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">class Trie:</span><br><span class="line">    def __init__(self):</span><br><span class="line">        self.children = [None] * 26</span><br><span class="line">        self.isEnd = False</span><br><span class="line">    </span><br><span class="line">    def searchPrefix(self, prefix: str) -&gt; &quot;Trie&quot;:</span><br><span class="line">        node = self</span><br><span class="line">        for ch in prefix:</span><br><span class="line">            ch = ord(ch) - ord(&quot;a&quot;)</span><br><span class="line">            if not node.children[ch]:</span><br><span class="line">                return None</span><br><span class="line">            node = node.children[ch]</span><br><span class="line">        return node</span><br><span class="line"></span><br><span class="line">    def insert(self, word: str) -&gt; None:</span><br><span class="line">        node = self</span><br><span class="line">        for ch in word:</span><br><span class="line">            ch = ord(ch) - ord(&quot;a&quot;)</span><br><span class="line">            if not node.children[ch]:</span><br><span class="line">                node.children[ch] = Trie()</span><br><span class="line">            node = node.children[ch]</span><br><span class="line">        node.isEnd = True</span><br><span class="line"></span><br><span class="line">    def search(self, word: str) -&gt; bool:</span><br><span class="line">        node = self.searchPrefix(word)</span><br><span class="line">        return node is not None and node.isEnd</span><br><span class="line"></span><br><span class="line">    def startsWith(self, prefix: str) -&gt; bool:</span><br><span class="line">        return self.searchPrefix(prefix) is not None</span><br></pre></td></tr></table></figure>
<h2><span id="参考文献">参考文献</span></h2><p>链接：<a target="_blank" rel="noopener" href="https://leetcode.cn/problems/implement-trie-prefix-tree/solutions/1/shi-xian-trie-qian-zhui-shu-by-leetcode-ti500/">https://leetcode.cn/problems/implement-trie-prefix-tree/solutions/1/shi-xian-trie-qian-zhui-shu-by-leetcode-ti500/</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-10-27T01:50:02.000Z" title="2024-10-27 9:50:02 ├F10: AM┤">2024-10-27</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-11-03T02:10:40.102Z" title="2024-11-3 10:10:40 ├F10: AM┤">2024-11-03</time></span><span class="level-item">a minute read (About 197 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/10/27/algo2/">算法（3）- 堆/优先级队列</a></p><div class="content"><h2><span id="堆">堆</span></h2><p>堆是一种特殊的树形数据结构，通常使用数组来实现。堆具有以下特性：堆是一棵完全二叉树（Complete Binary Tree），即除了最后一层外，每一层都被填满，最后一层的节点都靠左排列。</p>
<p>最大堆（Max Heap）：父节点的值总是大于或等于其子节点的值。</p>
<p>最小堆（Min Heap）：父节点的值总是小于或等于其子节点的值。</p>
<h2><span id="优先级队列">优先级队列</span></h2><p>优先级队列（Priority Queue）是一种特殊的队列，其中每个元素都有一个优先级。元素的出队顺序取决于其优先级，而不是进入队列的先后顺序。</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-10-26T09:11:10.000Z" title="2024-10-26 5:11:10 ├F10: PM┤">2024-10-26</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-10-26T11:30:42.910Z" title="2024-10-26 7:30:42 ├F10: PM┤">2024-10-26</time></span><span class="level-item"><a class="link-muted" href="/categories/engineering/">engineering</a></span><span class="level-item">8 minutes read (About 1126 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/10/26/DL2/">深度学习(3) - 计算框架PyTorch</a></p><div class="content"><h2><span id="pytorch">PyTorch</span></h2><h3><span id="张量">张量</span></h3><p>张量是一种专门的数据结构，与数组和矩阵非常相似。张量类似于 NumPy 的 ndarrays，但张量可以在 GPU 或其他硬件加速器上运行，张量也针对自动微分进行了优化。</p>
<p>默认情况下，张量是在 CPU 上创建的。我们需要使用 .to 方法将张量显式地移动到 GPU（在检查 GPU 是否可用后）。请记住，跨设备复制大型张量在时间和内存方面可能代价高昂！</p>
<p>将结果存储到操作数中的操作称为就地操作。它们用 <em> 后缀表示。例如：x.copy</em>(y)、x.t_() 将会改变 x。</p>
<h3><span id="数据加载">数据加载</span></h3><p>PyTorch 提供了两个数据基本类型：torch.utils.data.DataLoader 和 torch.utils.data.Dataset，它们使您可以使用预加载的数据集以及您自己的数据。</p>
<h3><span id="神经网络">神经网络</span></h3><p> torch.nn 命名空间提供了构建您自己的神经网络所需的所有构建块。PyTorch 中的每个模块都是 nn.Module 的子类。</p>
<h3><span id="自动微分">自动微分</span></h3><p> 在训练神经网络时，最常用的算法是 <strong>反向传播</strong>。在此算法中，参数（模型权重）会根据损失函数相对于给定参数的 <strong>梯度</strong> 进行调整。</p>
<p>为了计算这些梯度，PyTorch 有一个内置的微分引擎，称为 torch.autograd。它支持任何计算图的梯度自动计算。当我们需要计算损失函数相对于这些变量的梯度，我们设置了这些张量的requires<em>grad 属性。可以在创建张量时设置requires_grad的值，或者之后使用x.requires_grad</em>(True)方法。要计算这些导数，我们调用loss.backward()，然后从w.grad和b.grad中检索值。</p>
<p>要阻止一个张量被跟踪历史，可以调用.detach()方法将其与计算历史分离，并阻止它未来的计算记录被跟踪。为了防止跟踪历史记录(和使用内存），可以将代码块包装在 with torch.no_grad(): 中。</p>
<h3><span id="优化">优化</span></h3><p>在训练循环中，优化分三个步骤进行：<br>调用optimizer.zero_grad()重置模型参数的梯度。默认情况下，梯度会累加；为了防止重复计数，我们在每次迭代中显式地将其归零。</p>
<p>使用loss.backward()反向传播预测损失。PyTorch 将损失相对于每个参数的梯度存储起来。</p>
<p>在获得梯度后，我们调用optimizer.step()根据反向传播中收集的梯度调整参数。<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">def train_loop(dataloader, model, loss_fn, optimizer):</span><br><span class="line">    size = len(dataloader.dataset)</span><br><span class="line">    # Set the model to training mode - important for batch normalization and dropout layers</span><br><span class="line">    # Unnecessary in this situation but added for best practices</span><br><span class="line">    model.train()</span><br><span class="line">    for batch, (X, y) in enumerate(dataloader):</span><br><span class="line">        # Compute prediction and loss</span><br><span class="line">        pred = model(X)</span><br><span class="line">        loss = loss_fn(pred, y)</span><br><span class="line"></span><br><span class="line">        # Backpropagation</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        if batch % 100 == 0:</span><br><span class="line">            loss, current = loss.item(), batch * batch_size + len(X)</span><br><span class="line">            print(f&quot;loss: &#123;loss:&gt;7f&#125;  [&#123;current:&gt;5d&#125;/&#123;size:&gt;5d&#125;]&quot;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def test_loop(dataloader, model, loss_fn):</span><br><span class="line">    # Set the model to evaluation mode - important for batch normalization and dropout layers</span><br><span class="line">    # Unnecessary in this situation but added for best practices</span><br><span class="line">    model.eval()</span><br><span class="line">    size = len(dataloader.dataset)</span><br><span class="line">    num_batches = len(dataloader)</span><br><span class="line">    test_loss, correct = 0, 0</span><br><span class="line"></span><br><span class="line">    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode</span><br><span class="line">    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True</span><br><span class="line">    with torch.no_grad():</span><br><span class="line">        for X, y in dataloader:</span><br><span class="line">            pred = model(X)</span><br><span class="line">            test_loss += loss_fn(pred, y).item()</span><br><span class="line">            correct += (pred.argmax(1) == y).type(torch.float).sum().item()</span><br><span class="line"></span><br><span class="line">    test_loss /= num_batches</span><br><span class="line">    correct /= size</span><br><span class="line">    print(f&quot;Test Error: \n Accuracy: &#123;(100*correct):&gt;0.1f&#125;%, Avg loss: &#123;test_loss:&gt;8f&#125; \n&quot;)</span><br></pre></td></tr></table></figure></p>
<h3><span id="保存和加载模型">保存和加载模型</span></h3><h4><span id="保存和加载模型权重">保存和加载模型权重</span></h4><p>PyTorch 模型将学习到的参数存储在内部状态字典中，称为 state_dict。这些可以通过 torch.save 方法持久化。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model = models.vgg16(weights=&#x27;IMAGENET1K_V1&#x27;)</span><br><span class="line">torch.save(model.state_dict(), &#x27;model_weights.pth&#x27;)</span><br></pre></td></tr></table></figure>
<p>要加载模型权重，您需要首先创建一个相同模型的实例，然后使用 load_state_dict() 方法加载参数。<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = models.vgg16() # we do not specify ``weights``, i.e. create untrained model</span><br><span class="line">model.load_state_dict(torch.load(&#x27;model_weights.pth&#x27;, weights_only=True))</span><br><span class="line">model.eval()</span><br></pre></td></tr></table></figure><br>注意请务必在推理之前调用 model.eval() 方法，以将 dropout 和批归一化层设置为评估模式。如果不这样做，将产生不一致的推理结果。</p>
<h4><span id="保存和加载具有形状的模型">保存和加载具有形状的模型</span></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.save(model, &#x27;model.pth&#x27;)</span><br><span class="line">model = torch.load(&#x27;model.pth&#x27;, weights_only=False),</span><br></pre></td></tr></table></figure>
<h2><span id="参考文献">参考文献</span></h2><p><a target="_blank" rel="noopener" href="https://pytorch.ac.cn/tutorials/beginner/basics/tensor_tutorial.html">https://pytorch.ac.cn/tutorials/beginner/basics/tensor_tutorial.html</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-10-25T08:28:05.000Z" title="2024-10-25 4:28:05 ├F10: PM┤">2024-10-25</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-11-13T01:47:57.962Z" title="2024-11-13 9:47:57 ├F10: AM┤">2024-11-13</time></span><span class="level-item">7 minutes read (About 1035 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/10/25/RS0/">Recommender Systems</a></p><div class="content"><h2><span id="recommender-systems">Recommender Systems</span></h2><h3><span id="collaborative-filtering">Collaborative Filtering</span></h3><p> In a broad sense, it is the process of filtering for information or patterns using techniques involving collaboration among multiple users, agents, and data sources. Overall, CF techniques can be categorized into: memory-based CF, model-based CF, and their hybrid.</p>
<h3><span id="matrix-factorization">Matrix Factorization</span></h3><p> Matrix factorization is a class of collaborative filtering models. Specifically, the model factorizes the user-item interaction matrix (e.g., rating matrix) into the product of two lower-rank matrices, capturing the low-rank structure of the user-item interactions.</p>
<p>Let $\mathbf{R} \in \mathbb{R}^{m \times n}$ denote the interaction matrix with m users and n items and the values of $\mathbf{R}$<br> represent explicit ratings. The user-item interaction will be factorized into a user latent matrix $\mathbf{P} \in \mathbb{R}^{m \times k}$ and  an item latent matrix $\mathbf{Q} \in \mathbb{R}^{n \times k}$. For a given item<br>i, the elements of $\mathbf{q}_i$<br> measure the extent to which the item possesses those characteristics such as the genres and languages of a movie. For a given user u<br>, the elements of $\mathbf{p}_u$<br> measure the extent of interest the user has in items’ corresponding characteristics.<br> The predicted ratings can be estimated by </p>
<script type="math/tex; mode=display">
 \hat{\mathbf{R}} = \mathbf{PQ}^\top</script><p> One major problem of this prediction rule is that users/items biases can not be modeled, so </p>
<script type="math/tex; mode=display">
\hat{\mathbf{R}}_{ui} = \mathbf{p}_u\mathbf{q}^\top_i + b_u + b_i</script><p> Then, we train the matrix factorization model by minimizing the mean squared error between predicted rating scores and real rating scores.</p>
<script type="math/tex; mode=display">
 \underset{\mathbf{P}, \mathbf{Q}, b}{\mathrm{argmin}} \sum_{(u, i) \in \mathcal{K}} \| \mathbf{R}_{ui} -
\hat{\mathbf{R}}_{ui} \|^2 + \lambda (\| \mathbf{P} \|^2_F + \| \mathbf{Q}
\|^2_F + b_u^2 + b_i^2 )</script><p>where $\lambda$ denotes the regularization rate.</p>
<h3><span id="autorec">AutoRec</span></h3><p>Although the matrix factorization model achieves decent performance on the rating prediction task, it is essentially a linear model. Thus, such models are not capable of capturing complex nonlinear and intricate relationships that may be predictive of users’ preferences.<br>AutoRec identifies collaborative filtering (CF) with an autoencoder architecture and aims to integrate nonlinear transformations into CF on the basis of explicit feedback. On the other hand, AutoRec differs from a traditional autoencoder: rather than learning the hidden representations, AutoRec focuses on learning/reconstructing the output layer. It uses a partially observed interaction matrix as input, aiming to reconstruct a completed rating matrix.</p>
<p>Let $\mathbf{R}_{*i}$<br> denote the $i^\textrm{th}$<br> column of the rating matrix, where unknown ratings are set to zeros by default.<br>The neural architecture is defined as:</p>
<script type="math/tex; mode=display">
h(\mathbf{R}_{*i}) = f(\mathbf{W} \cdot g(\mathbf{V} \mathbf{R}_{*i} + \mu) + b)</script><p>The following objective function aims to minimize the reconstruction error:</p>
<script type="math/tex; mode=display">
\underset{\mathbf{W},\mathbf{V},\mu, b}{\mathrm{argmin}} \sum_{i=1}^M{\parallel \mathbf{R}_{*i} - h(\mathbf{R}_{*i})\parallel_{\mathcal{O}}^2} +\lambda(\| \mathbf{W} \|_F^2 + \| \mathbf{V}\|_F^2)</script><p>where $| \cdot |_{\mathcal{O}}$<br> means only the contribution of observed ratings are considered, that is, only weights that are associated with observed inputs are updated during back-propagation.</p>
<h3><span id="personalized-ranking">Personalized Ranking</span></h3><p> In general, personalized ranking models can be optimized with pointwise, pairwise or listwise approaches. Pointwise approaches considers a single interaction at a time and train a classifier or a regressor to predict individual preferences. Matrix factorization and AutoRec are optimized with pointwise objectives. Pairwise approaches consider a pair of items for each user and aim to approximate the optimal ordering for that pair. Listwise approaches approximate the ordering of the entire list of items, for example, direct optimizing the ranking measures such as Normalized Discounted Cumulative Gain (NDCG).</p>
<h4><span id="bayesian-personalized-ranking-loss">Bayesian Personalized Ranking Loss</span></h4><p> Bayesian personalized ranking (BPR) is a pairwise personalized ranking loss that is derived from the maximum posterior estimator. The training data of BPR consists of both positive and negative pairs (missing values). It assumes that the user prefers the positive item over all other non-observed items. We can formulate the maximum posterior estimator to derive the generic optimization criterion for the personalized ranking task. </p>
<script type="math/tex; mode=display">
 \begin{split}\begin{aligned}
\textrm{BPR-OPT} : &= \ln p(\Theta \mid >_u) \\
         & \propto \ln p(>_u \mid \Theta) p(\Theta) \\
         &= \ln \prod_{(u, i, j \in D)} \sigma(\hat{y}_{ui} - \hat{y}_{uj}) p(\Theta) \\
         &= \sum_{(u, i, j \in D)} \ln \sigma(\hat{y}_{ui} - \hat{y}_{uj}) + \ln p(\Theta) \\
         &= \sum_{(u, i, j \in D)} \ln \sigma(\hat{y}_{ui} - \hat{y}_{uj}) - \lambda_\Theta \|\Theta \|^2
\end{aligned}\end{split}</script><p> Where $\Theta$<br> represents the parameters of an arbitrary recommendation model,<br>$&gt;<em>u$ represents the desired personalized total ranking of all items for user.<br>$\hat{y}</em>{ui}$ and $\hat{y}_{uj}$<br> are the predicted scores of the user u to item i and j,respectively. </p>
<h4><span id="hinge-loss">Hinge Loss</span></h4><script type="math/tex; mode=display">
 \sum_{(u, i, j \in D)} \max( m - \hat{y}_{ui} + \hat{y}_{uj}, 0)</script><p> where m<br> is the safety margin size.</p>
<h3><span id="neumf">NeuMF</span></h3><p> This model leverages the flexibility and non-linearity of neural networks to replace dot products of matrix factorization, aiming at enhancing the model expressiveness. In specific, this model is structured with two subnetworks including generalized matrix factorization (GMF) and MLP and models the interactions from two pathways instead of simple dot products. The outputs of these two networks are concatenated for the final prediction scores calculation. </p>
<h3><span id="sequence-aware-recommender-systems">Sequence-Aware Recommender Systems</span></h3><p> Caser, short for convolutional sequence embedding recommendation model, adopts convolutional neural networks capture the dynamic pattern influences of users’ recent activities.   The main component of Caser consists of a horizontal convolutional network and a vertical convolutional network, aiming to uncover the union-level and point-level sequence patterns, respectively.  The goal of Caser is to recommend item by considering user general tastes as well as short-term intention.</p>
<h3><span id="factorization-machines">Factorization Machines</span></h3><p> The strengths of factorization machines over the linear regression and matrix factorization are: (1) it can model<br>$\chi$-way variable interactions, where $\chi$<br> is the number of polynomial order and is usually set to two. (2) A fast optimization algorithm associated with factorization machines can reduce the polynomial computation time to linear complexity, making it extremely efficient especially for high dimensional sparse inputs. </p>
<h2><span id="references">references</span></h2><p><a target="_blank" rel="noopener" href="https://d2l.ai/chapter_recommender-systems/index.html">https://d2l.ai/chapter_recommender-systems/index.html</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-10-25T02:36:13.000Z" title="2024-10-25 10:36:13 ├F10: AM┤">2024-10-25</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-10-25T02:36:13.629Z" title="2024-10-25 10:36:13 ├F10: AM┤">2024-10-25</time></span><span class="level-item">a few seconds read (About 0 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/10/25/RL1/">RL1</a></p><div class="content"></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-10-25T02:12:01.000Z" title="2024-10-25 10:12:01 ├F10: AM┤">2024-10-25</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-10-25T10:10:27.410Z" title="2024-10-25 6:10:27 ├F10: PM┤">2024-10-25</time></span><span class="level-item">5 minutes read (About 711 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/10/25/algo1/">算法(2)--二叉树</a></p><div class="content"><h2><span id="二叉树">二叉树</span></h2><p>二叉树（Binary Tree）是一种常用的数据结构，在计算机科学中有广泛的应用。它是一种非线性数据结构，每个节点最多有两个子节点，通常这两个子节点被称为左子节点（left child）和右子节点（right child）。</p>
<p>有一些特别的二叉树，满二叉树就是每一层节点都是满的，除了叶子节点外，每个节点都有两个子节点。假设深度为 h，那么总节点数就是$2^h - 1$。 完全二叉树是指，二叉树的每一层的节点都紧凑靠左排列，且除了最后一层，其他每层都必须是满的，完全二叉树的特点：由于它的节点紧凑排列，如果从左到右从上到下对它的每个节点编号，那么父子节点的索引存在明显的规律。平衡二叉树中任何两个叶子节点的高度差不大于 1，二叉搜索树（Binary Search Tree，简称 BST）是一种很常见的二叉树，它的定义是：对于树中的每个节点，其左子树的每个节点的值都要小于这个节点的值，右子树的每个节点的值都要大于这个节点的值。</p>
<h2><span id="二叉树的递归层序遍历">二叉树的递归/层序遍历</span></h2><h3><span id="递归遍历">递归遍历</span></h3><p>二叉树的递归遍历是一种常用的遍历方式，它利用递归来访问树中的所有节点。二叉树的递归遍历主要有三种方式：前序遍历（Preorder）、中序遍历（Inorder）和后序遍历（Postorder）。前序遍历的顺序是：先访问根节点，然后递归地遍历左子树，最后递归地遍历右子树。中序遍历的顺序是：递归地遍历左子树，然后访问根节点，最后递归地遍历右子树。后序遍历的顺序是：递归地遍历左子树，然后递归地遍历右子树，最后访问根节点。<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">class TreeNode:</span><br><span class="line">    def __init__(self, value=0, left=None, right=None):</span><br><span class="line">        self.value = value</span><br><span class="line">        self.left = left</span><br><span class="line">        self.right = right</span><br><span class="line"></span><br><span class="line">def preorder_traversal(root):</span><br><span class="line">    if root is None:</span><br><span class="line">        return []</span><br><span class="line">    result = []</span><br><span class="line">    _preorder_helper(root, result)</span><br><span class="line">    return result</span><br><span class="line"></span><br><span class="line">def _preorder_helper(node, result):</span><br><span class="line">    if node:</span><br><span class="line">        result.append(node.value)  # 访问节点</span><br><span class="line">        _preorder_helper(node.left, result)  # 遍历左子树</span><br><span class="line">        _preorder_helper(node.right, result)  # 遍历右子树</span><br></pre></td></tr></table></figure></p>
<h3><span id="层序遍历">层序遍历</span></h3><p>二叉树的层序遍历，顾名思义，就是一层一层地遍历二叉树。这个遍历方式需要借助队列来实现。</p>
<h2><span id="参考文献">参考文献</span></h2><p><a target="_blank" rel="noopener" href="https://labuladong.online/algo/data-structure-basic/binary-tree-basic/#%E5%87%A0%E7%A7%8D%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BA%8C%E5%8F%89%E6%A0%91">https://labuladong.online/algo/data-structure-basic/binary-tree-basic/#%E5%87%A0%E7%A7%8D%E5%B8%B8%E8%A7%81%E7%9A%84%E4%BA%8C%E5%8F%89%E6%A0%91</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-10-24T02:36:56.000Z" title="2024-10-24 10:36:56 ├F10: AM┤">2024-10-24</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-10-25T02:04:20.761Z" title="2024-10-25 10:04:20 ├F10: AM┤">2024-10-25</time></span><span class="level-item"><a class="link-muted" href="/categories/theory/">theory</a></span><span class="level-item">12 minutes read (About 1726 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/10/24/nlp0/">词嵌入</a></p><div class="content"><h2><span id="word2vec">word2vec</span></h2><p>word2vec工具包含两个模型，即跳元模型（skip-gram）和连续词袋（CBOW （Continuous Bag-of-Words）。Skip-gram是给定一个中心词，预测其周围的上下文词。CBOW是给定一段文本中的一个中心词周围的上下文词，预测中心词。</p>
<h3><span id="负采样">负采样</span></h3><p>在标准的 softmax 方法中，我们有</p>
<script type="math/tex; mode=display">
P(w_o \mid w_c) = \frac{\exp(\mathbf{u}_o^\top \mathbf{v}_c)}{ \sum_{i \in \mathcal{V}} \exp(\mathbf{u}_i^\top \mathbf{v}_c)},</script><p>这涉及到计算所有词汇表中词汇的条件概率, 在词汇表非常大的时候是非常耗时的。</p>
<p>负采样的核心思想是通过随机选择一部分非上下文词（负样本）来近似这个条件概率分布。具体来说，对于每个中心词和对应的上下文词，我们不仅更新这两个词的向量表示，而且还随机选择若干个不在上下文中的词作为“负样本”，并更新它们的向量表示。在负采样中，我们采用一个简化的逻辑回归目标函数来替代标准的 softmax 函数。对于每个训练样本,我们希望模型能够正确区分真正的上下文词和负样本词。因此，我们可以定义一个目标函数，它希望对于上下文词，有$\sigma(\u_o\v_i+b)=1$而对于每个负样本，有$\sigma(\u_o\v_i+b)=0$。 通过这种方式，负采样实际上是在模拟原始 softmax 的行为，但它只考虑了一小部分词汇，即一个正样本加上几个负样本。这意味着负采样通过一系列独立的二元分类任务来近似这个条件概率。</p>
<h3><span id="层次softmax">层次softmax</span></h3><p>层次softmax 的核心思想是构建一个词汇的层次结构，通常是一棵哈夫曼树（Huffman Tree），并将分类问题转化为沿着树的路径进行的一系列二元分类问题。这样可以将原始的 softmax 层的计算复杂度从线性减少到对数级别。层次softmax 的目标是通过计算从根节点到词汇的叶子节点的路径概率来近似原条件概率。</p>
<h2><span id="glove">GloVe</span></h2><p>GloVe模型的主要特点是它试图捕捉单词之间的共现频率信息，即一个单词出现时另一个单词出现的概率。通过这种方式，模型可以学习到词汇语义以及词汇间的关系，比如同义词、反义词、以及类比关系等。</p>
<p>训练GloVe模型涉及到最小化词对共现概率预测值与实际共现概率之间的损失函数。</p>
<h2><span id="子词嵌入">子词嵌入</span></h2><h3><span id="fasttext">fastText</span></h3><p>在跳元模型和连续词袋模型中，同一词的不同变形形式直接由不同的向量表示，不需要共享参数。为了使用形态信息，fastText模型提出了一种子词嵌入方法，其中子词是一个字符n-gram。fastText可以被认为是子词级跳元模型，而非学习词级向量表示，其中每个中心词由其子词级向量之和表示。</p>
<h3><span id="字节对编码byte-pair-encoding">字节对编码（Byte Pair Encoding）</span></h3><p>字节对编码（Byte Pair Encoding，简称 BPE）是一种用于词汇归一化和文本压缩的技术，近年来，BPE 被重新引入到自然语言处理领域，特别是在机器翻译和语言建模中，作为一种生成子词单位的有效方法。BPE 的基本思想是不断地合并最常出现的相邻字符对，直到达到预定的词汇表大小。</p>
<h2><span id="elmo">ELMo</span></h2><p>word2vec和GloVe都将相同的预训练向量分配给同一个词，而不考虑词的上下文，考虑到自然语言中丰富的多义现象和复杂的语义，上下文无关表示具有明显的局限性，同一个词可以根据上下文被赋予不同的表示。</p>
<p>ELMo（Embeddings from Language Models）是一种上下文敏感的词嵌入方法，使用双向 LSTM（长短期记忆网络）构建深度语言模型，这种模型可以捕获来自句子左右两边的信息。通过训练这种模型，可以得到一个对上下文敏感的词嵌入。</p>
<h2><span id="gpt">GPT</span></h2><p>初代 GPT 基于 Transformer 架构，使用的是单向的 Transformer，这意味着它在生成文本时只能访问之前的位置信息，而不能访问当前位置之后的信息。通过自回归方式训练，即模型学习给定前面的文字后预测下一个文字。</p>
<h2><span id="bert">BERT</span></h2><p>ELMo对上下文进行双向编码，但使用特定于任务的架构；而GPT是任务无关的，但是从左到右编码上下文，BERT结合了这两个方面的优点。</p>
<p>BERT输入序列明确地表示单个文本和文本对。当输入为单个文本时，BERT输入序列是特殊类别词元“\<cls\>”、文本序列的标记、以及特殊分隔词元“\<sep\>”的连结。当输入为文本对时，BERT输入序列是“\<cls\>”、第一个文本序列的标记、“\<sep\>”、第二个文本序列标记、以及“\<sep\>”的连结。</sep\></sep\></cls\></sep\></cls\></p>
<p>为了双向编码上下文以表示每个词元，BERT随机掩蔽词元并使用来自双向上下文的词元以自监督的方式预测掩蔽词元。此任务称为掩蔽语言模型。</p>
<p>尽管掩蔽语言建模能够编码双向上下文来表示单词，但它不能显式地建模文本对之间的逻辑关系。为了帮助理解两个文本序列之间的关系，BERT在预训练中考虑了一个二元分类任务——下一句预测。在为预训练生成句子对时，有一半的时间它们确实是标签为“真”的连续句子；在另一半的时间里，第二个句子是从语料库中随机抽取的，标记为“假”。</p>
<p>在预训练BERT时，最终的损失函数是掩蔽语言模型损失函数和下一句预测损失函数的线性组合。</p>
<h2><span id="参考文献">参考文献</span></h2><p><a target="_blank" rel="noopener" href="https://zh.d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html">https://zh.d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html</a></p>
<p><a target="_blank" rel="noopener" href="https://zh.d2l.ai/chapter_natural-language-processing-pretraining/subword-embedding.html">https://zh.d2l.ai/chapter_natural-language-processing-pretraining/subword-embedding.html</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-10-23T10:35:00.000Z" title="2024-10-23 6:35:00 ├F10: PM┤">2024-10-23</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-10-26T11:46:14.521Z" title="2024-10-26 7:46:14 ├F10: PM┤">2024-10-26</time></span><span class="level-item"><a class="link-muted" href="/categories/engineering/">engineering</a></span><span class="level-item">7 minutes read (About 1118 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/10/23/DL1/">深度学习(2)-计算性能</a></p><div class="content"><h2><span id="命令式编程和符号式编程">命令式编程和符号式编程</span></h2><p>命令式编程（Imperative Programming）和符号式编程（Symbolic Programming）是两种不同的编程范式。命令式编程描述了计算机应该执行的具体步骤来达到预期的结果。这种编程方式侧重于描述“怎么做”，而不是“做什么”。符号式编程侧重于表达计算的本质，而不是具体的执行步骤。程序表达的是问题的解决方案，而不是具体的操作细节，代码通常只在完全定义了过程之后才执行计算。</p>
<h2><span id="异步计算">异步计算</span></h2><p>异步计算（Asynchronous Computing）是一种编程和计算模式，它允许程序在等待某些操作完成的同时继续执行其他任务。与同步计算不同，异步计算可以提高程序的响应性和性能，尤其是在处理 I/O 操作、网络请求、长时间运行的任务等情况时。</p>
<h3><span id="gputpunpu">GPU,TPU,NPU</span></h3><p>GPU即Graphics Processing Unit，中文名为图形处理单元。需要注意的是，GPU没有独立工作的能力，必须由CPU进行控制调用才能工作，且GPU的功耗一般比较高。TPU即Tensor Processing Unit，中文名为张量处理器,是谷歌在2015年6月的IO开发者大会上推出了为优化自身的TensorFlow框架而设计打造的一款计算神经网络专用芯片。NPU即Neural-network Processing Unit，中文名为神经网络处理器，它采用“数据驱动并行计算”的架构，特别擅长处理视频、图像类的海量多媒体数据。</p>
<h2><span id="并行计算">并行计算</span></h2><p>CUDA是NVIDIA提供的一种GPU并行计算框架。对于GPU本身的编程，使用的是CUDA语言来实现的。在编写程序中，当我们使用了 .cuda() 时，其功能是让我们的模型或者数据从CPU迁移到GPU上（默认是0号GPU）当中，通过GPU开始计算。</p>
<p>当我们的服务器上有多个GPU，我们应该指明我们使用的GPU是哪一块，如果我们不设置的话，tensor.cuda()方法会默认将tensor保存到第一块GPU上，等价于tensor.cuda(0)。</p>
<p>PyTorch提供了两种多卡训练的方式，分别为DataParallel和DistributedDataParallel（以下我们分别简称为DP和DDP）。这两种方法中官方更推荐我们使用DDP，因为它的性能更好。DP只能在单机上使用，且DP是单进程多线程的实现方式，比DDP多进程多线程的方式会效率低一些。</p>
<p>首先我们来看单机多卡DP，通常使用一种叫做数据并行 (Data parallelism) 的策略，即将计算任务划分成多个子任务并在多个GPU卡上同时执行这些子任务。主要使用到了nn.DataParallel函数。<br><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model = Net()</span><br><span class="line">model.cuda() # 模型显示转移到CUDA上</span><br><span class="line"></span><br><span class="line">if torch.cuda.device_count() &gt; 1: # 含有多张GPU的卡</span><br><span class="line">	model = nn.DataParallel(model, device_ids=[0,1]) # 单机多卡DP训练</span><br></pre></td></tr></table></figure><br>不过通过DP进行分布式多卡训练的方式容易造成负载不均衡，有可能第一块GPU显存占用更多，因为输出默认都会被gather到第一块GPU上。为此Pytorch也提供了torch.nn.parallel.DistributedDataParallel（DDP）方法来解决这个问题。</p>
<p>针对每个GPU，启动一个进程，然后这些进程在最开始的时候会保持一致（模型的初始化参数也一致，每个进程拥有自己的优化器），同时在更新模型的时候，梯度传播也是完全一致的，这样就可以保证任何一个GPU上面的模型参数就是完全一致的，所以这样就不会出现DataParallel那样显存不均衡的问题。</p>
<h2><span id="参考文献">参考文献</span></h2><p><a target="_blank" rel="noopener" href="https://zh.d2l.ai/chapter_computational-performance/index.html">https://zh.d2l.ai/chapter_computational-performance/index.html</a></p>
<p><a target="_blank" rel="noopener" href="https://datawhalechina.github.io/thorough-pytorch/%E7%AC%AC%E4%BA%8C%E7%AB%A0/2.3%20%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E7%AE%80%E4%BB%8B.html">https://datawhalechina.github.io/thorough-pytorch/%E7%AC%AC%E4%BA%8C%E7%AB%A0/2.3%20%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E7%AE%80%E4%BB%8B.html</a></p>
<p><a target="_blank" rel="noopener" href="https://datawhalechina.github.io/thorough-pytorch/%E7%AC%AC%E4%BA%8C%E7%AB%A0/2.4%20AI%E7%A1%AC%E4%BB%B6%E5%8A%A0%E9%80%9F%E8%AE%BE%E5%A4%87.html">https://datawhalechina.github.io/thorough-pytorch/%E7%AC%AC%E4%BA%8C%E7%AB%A0/2.4%20AI%E7%A1%AC%E4%BB%B6%E5%8A%A0%E9%80%9F%E8%AE%BE%E5%A4%87.html</a></p>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/">Previous</a></div><div class="pagination-next"><a href="/page/3/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/">1</a></li><li><a class="pagination-link is-current" href="/page/2/">2</a></li><li><a class="pagination-link" href="/page/3/">3</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/11/">11</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/tx.PNG" alt="s-serenity"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">s-serenity</p><p class="is-size-6 is-block">student</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Shanghai, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">104</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">12</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">28</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/s-serenity" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/s-serenity"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://github.com/s-serenity" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/LLM/"><span class="level-start"><span class="level-item">LLM</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/algorithm/"><span class="level-start"><span class="level-item">algorithm</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/computer-science/"><span class="level-start"><span class="level-item">computer science</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/computer-system/"><span class="level-start"><span class="level-item">computer system</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/database/"><span class="level-start"><span class="level-item">database</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/categories/development-tools/"><span class="level-start"><span class="level-item">development tools</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/engineering/"><span class="level-start"><span class="level-item">engineering</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/others/"><span class="level-start"><span class="level-item">others</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/practice/"><span class="level-start"><span class="level-item">practice</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/programming-language/"><span class="level-start"><span class="level-item">programming language</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/theory/"><span class="level-start"><span class="level-item">theory</span></span><span class="level-end"><span class="level-item tag">28</span></span></a></li><li><a class="level is-mobile" href="/categories/web-development/"><span class="level-start"><span class="level-item">web development</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-11-14T02:39:10.000Z">2024-11-14</time></p><p class="title"><a href="/2024/11/14/python-5/">python-5</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-11-07T09:03:42.000Z">2024-11-07</time></p><p class="title"><a href="/2024/11/07/SQL1/">SQL(2) - 开窗函数</a></p><p class="categories"><a href="/categories/database/">database</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-11-06T03:42:14.000Z">2024-11-06</time></p><p class="title"><a href="/2024/11/06/ir0/">信息检索（1）- BM25</a></p><p class="categories"><a href="/categories/algorithm/">algorithm</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-11-05T09:20:14.000Z">2024-11-05</time></p><p class="title"><a href="/2024/11/05/web0/">Web(1) - Cookie 和 Session</a></p><p class="categories"><a href="/categories/web-development/">web development</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-11-05T09:16:04.000Z">2024-11-05</time></p><p class="title"><a href="/2024/11/05/cpp1/">C++（2）- 静态成员函数</a></p><p class="categories"><a href="/categories/programming-language/">programming language</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2024/"><span class="level-start"><span class="level-item">2024</span></span><span class="level-end"><span class="level-item tag">33</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/"><span class="level-start"><span class="level-item">2023</span></span><span class="level-end"><span class="level-item tag">43</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/"><span class="level-start"><span class="level-item">2022</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/"><span class="level-start"><span class="level-item">2021</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/"><span class="level-start"><span class="level-item">2020</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/"><span class="level-start"><span class="level-item">2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/"><span class="level-start"><span class="level-item">2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Algorithm/"><span class="tag">Algorithm</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C++</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Elasticsearch/"><span class="tag">Elasticsearch</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LLM/"><span class="tag">LLM</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux/"><span class="tag">Linux</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Markdown/"><span class="tag">Markdown</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Math/"><span class="tag">Math</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">12</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Recommender-System/"><span class="tag">Recommender System</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SQL/"><span class="tag">SQL</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/algorithm/"><span class="tag">algorithm</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/big-data/"><span class="tag">big data</span><span class="tag">13</span></a></div><div class="control"><a class="tags has-addons" href="/tags/computer-network/"><span class="tag">computer network</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/computer-vision/"><span class="tag">computer vision</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/deep-learning/"><span class="tag">deep learning</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/git/"><span class="tag">git</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/graph/"><span class="tag">graph</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/hexo/"><span class="tag">hexo</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/information-retrieval/"><span class="tag">information retrieval</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/java/"><span class="tag">java</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/machine-learning/"><span class="tag">machine learning</span><span class="tag">18</span></a></div><div class="control"><a class="tags has-addons" href="/tags/operating-system/"><span class="tag">operating system</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python/"><span class="tag">python</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/reinforcement-learning/"><span class="tag">reinforcement learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/speech/"><span class="tag">speech</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/tools/"><span class="tag">tools</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/web/"><span class="tag">web</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="s-serenity" height="28"></a><p class="is-size-7"><span>&copy; 2024 s-serenity</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2019</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>