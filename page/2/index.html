<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>s-serenity</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="s-serenity"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="s-serenity"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Recording and sharing my learning process."><meta property="og:type" content="blog"><meta property="og:title" content="s-serenity"><meta property="og:url" content="http://yoursite.com/"><meta property="og:site_name" content="s-serenity"><meta property="og:description" content="Recording and sharing my learning process."><meta property="og:locale" content="en_US"><meta property="og:image" content="http://yoursite.com/img/og_image.png"><meta property="article:author" content="s-serenity"><meta property="article:tag" content="Algorithm study development"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://yoursite.com/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://yoursite.com"},"headline":"s-serenity","image":["http://yoursite.com/img/og_image.png"],"author":{"@type":"Person","name":"s-serenity"},"publisher":{"@type":"Organization","name":"s-serenity","logo":{"@type":"ImageObject","url":"http://yoursite.com/img/logo.svg"}},"description":"Recording and sharing my learning process."}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.1.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="s-serenity" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-11-28T13:19:33.000Z" title="2023-11-28 2:19:33 ├F10: PM┤">2023-11-28</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-12-12T15:35:34.301Z" title="2023-12-12 4:35:34 ├F10: PM┤">2023-12-12</time></span><span class="level-item"><a class="link-muted" href="/categories/database/">database</a></span><span class="level-item">17 minutes read (About 2537 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/11/28/bigdata9/">bigdata - MongoDB</a></p><div class="content"><h1><span id="document-stores">Document stores</span></h1><p>Can we rebuild a similar system for collections of trees, in the sense that we drop all three constraints: relational integrity, domain integrity, and atomic integrity? Document stores bring us one step in this direction.</p>
<h2><span id="challenges">Challenges</span></h2><h3><span id="schema-on-read">Schema on read</span></h3><p>In a relational database management system, it is not possible to populate a table without having defined its schema first. However, when encountering such denormalized data, in the real world, there is often no schema. In fact, one of the important features of a system that deals with denormalized data is the ability to discover a schema.</p>
<h3><span id="making-trees-fit-in-tables">Making trees fit in tables</span></h3><p>Several XML elements (or, likewise, several JSON objects) can be naturally mapped to a relational table with several rows if the collection is flat and homogeneous, but semi-structured data can generally be nested and heterogeneous. if we map nested and heterogeneous into a table,such mapping will at best have to be done for every single dataset, and requires in most cases a schema, whereas we are looking for a generic solution for semistructured data with no a-priori schema information.</p>
<h2><span id="document-stores">Document stores</span></h2><p>Document stores provide a native database management system for semi-structured data. Document stores work on collections of records, generalizing the way that relational tables can be seen as collections of rows. It is important to understand that document stores are optimized for the typical use cases of many records of small to medium sizes. Typically, a collection can have millions or billions of documents, while each single document weighs no more than 16 MB (or a size in a similar magnitude). Finally, a collection of documents need not have a schema: it is possible to insert random documents that have dissimilar structures with no problem at all. Most document stores, however, do provide the ability to add a schema. Document stores can generally do selection, projection, aggregation and sorting quite well, but many of them are typically not (yet) optimized for joining collections. In fact, often, their language or API does not offer any joining functionality at all, which pushes the burden to reimplement joins in a host language to the users. This is a serious breach of data independence.</p>
<h2><span id="implementations">Implementations</span></h2><p>There is a very large number of products in the document store space for both JSON and XML, let us mention for example MongoDB, CouchDB, ElasticSearch, Cloudant, existDB, ArangoDB, BaseX, MarkLogic, DocumentDB, CosmosDB, and so on. We will focus, as an example, on MongoDB.</p>
<h2><span id="physical-storage">Physical storage</span></h2><p>Just like the storage format is optimized for tabular data in a relational database management system, it is optimized for tree-like data in a document store. In MongoDB, the format is a binary version of JSON called BSON. BSON is basically based on a sequence of tokens that efficiently encode the JSON constructs found in a document. The immediate benefit of BSON is that it takes less space in storage than JSON stored as a text file: for example, null, true and false literals need four or five bytes in text format at best, while they can be efficiently encoded as single bytes in BSON. Furthermore, BSON supports additional types that JSON does not have, such as dates. </p>
<h2><span id="querying-paradigm-crud">Querying paradigm (CRUD)</span></h2><p>The API of MongoDB, like many document stores, is based on the CRUD paradigm. CRUD means Create, Read, Update, Delete and corresponds to low-level primitives similar to those for HBase. MongoDB supports several host languages to query collections via an API. This includes in particular JavaScript and Python, but many other languages are supported via drivers. We will use JavaScript here because this is the native host language of MongoDB. It is important to note that these APIs are not query languages. MongoDB also provides access to the data via a shall called mongo or, newly, mongosh. This is a simple JavaScript interface wrapped around the MongoDB’s node.js driver. </p>
<h3><span id="populating-a-collection">Populating a collection</span></h3><p>To create a collection, one can simply insert a document in it, and it will be automatically created if it does not exist.</p>
<p>MongoDB automatically adds to every inserted document a special field called “ id” and associated with a value called an Object ID and with a type of its own.Object IDs are convenient for deleting or updating a specific document with no ambiguity.</p>
<h3><span id="querying-a-collection">Querying a collection</span></h3><h4><span id="scan-a-collection">Scan a collection</span></h4><p>Asking for the contents of an entire collection is done with a simple find() call on the previous object:db.collection.find().This function does not in fact return the entire collection; rather, it returns some pointer, called a cursor, to the collection; the user can then iterate on the cursor in an imperative fashion in the host language.</p>
<h4><span id="selection">Selection</span></h4><p>It is possible to perform a selection on a collection by passing a parameter to find() that is a JSON object:<br>db.collection.find({ “Theory” : “Relativity” }). </p>
<p>A disjunction (OR) uses a special MongoDB keyword, prefixed with a dollar sign:<br>db.collection.find( { “$or” : [ { “Theory”:”Relativity” }, { “Last”:”Einstein” } ] } ).</p>
<p>MongoDB offers many other keywords, for example for comparison other than equality:<br>db.collection.find( { “Publications” : { “$gte” : 100 } } )</p>
<h4><span id="projection">Projection</span></h4><p>Projections are made with the second parameter of this same find() method. This is done in form of a JSON object associating all the desired keys in the projection with the value 1. db.scientists.find( { “Theory” : “Relativity” }, { “First” : 1, “Last”: 1 } ).</p>
<p>It is also possible to project fields away in the same way with 0s, however 1s and 0s cannot be mixed in the projection parameter, except in the specific above case of projecting away the object ID</p>
<h4><span id="counting">Counting</span></h4><p>Counting can be done by chaining a count() method call:<br>db.scientists.find( { “Theory” : “Relativity” } ).count().</p>
<h4><span id="sorting">Sorting</span></h4><p>Sorting can be done by chaining a sort() method call.<br>db.scientists.find( { “Theory” : “Relativity” }, { “First” : 1, “Last” : 1 } ).sort( { “First” : 1, “Name” : -1 } )</p>
<p>1 is for ascending order and -1 for descending order.It is also possible to add limits and offsets to paginate results also by chaining more method calls:<br>db.scientists.find( { “Theory” : “Relativity” }, { “First” : 1, “Last” : 1 } ).sort( { “First” : 1, “Name” : -1 } ).skip(30).limit(10).</p>
<p>Note that, contrary to intuition, the order of the calls does not matter, as this is really just the creation of a query plan by providing parameters (in any order).</p>
<h4><span id="duplicate-elimination">Duplicate elimination</span></h4><p>It is possible to obtain all the distinct values for one field with a distinct() call: db.scientists.distinct(“Theory”)</p>
<h3><span id="querying-for-heterogeneity">Querying for heterogeneity</span></h3><h4><span id="absent-fields">Absent fields</span></h4><p>Absent fields can be filtered with: db.scientists.find( { “Theory” : null } ). </p>
<h4><span id="filtering-for-values-across-types">Filtering for values across types</span></h4><p>db.collection.find( { “$or” : [ { “Theory”: “Relativity” }, { “Theory”: 42 }, { “Theory”: null } ] } )</p>
<p>db.scientists.find( { “Theory” : { “$in” : [“Relativity”, 42, null ] } } ).</p>
<p>MongoDB is also able to sort on fields that have heterogeneous data types. It does so by first order by type in some (arbitrary, but documented) order, and then within each type.</p>
<h3><span id="querying-for-nestedness">Querying for nestedness</span></h3><p>Nestedness in MongoDB is handled in several ad-hoc ways for specific use cases.</p>
<h4><span id="values-in-nested-objects">Values in nested objects</span></h4><p>We saw how to select documents based on values associated with a top-level keys. What about values that are not at the top-level, but in nested objects?<br>The first solution that might come to mind is something like this: db.scientists.find({ “Name” : { “First” : “Albert” } }) However, this query will not have the behavior many would have expected: instead of finding documents that have a value “Albert” associated with the key “First” in an object itself associated with the top-level key ”Name”, this query looks for an exact match of the entire object.<br>In order to include documents such as above, MongoDB uses a dot syntax: db.scientists.find({ “Name.First” : “Albert” }).</p>
<h4><span id="values-in-nested-arrays">Values in nested arrays</span></h4><p>MongoDB allows to filter documents based on whether a nested array contains a specific value, like so: db.scientists.find({ “Theories” : “Special relativity” }). </p>
<h4><span id="deleting-objects-from-a-collection">Deleting objects from a collection</span></h4><p>Objects can be deleted from a collection either one at a time with deleteOne(), or several at a time with deleteMany(): db.scientists.deleteMany( { “century” : “15” } ). </p>
<h4><span id="updating-objects-in-a-collection">Updating objects in a collection</span></h4><p>Documents can be updated with updateOne() and updateMany() by providing both a filtering criterion (with the same syntax as the first parameter of find()) and an update to apply. The command looks like so: db.scientists.updateMany( { “Last” : “Einstein” }, { $set : { “Century” : “20” } } ).</p>
<p>The granularity of updates is per document, that is, a single document can be updated by at most one query at the same time.However, within the same collection, several different documents can be modified concurrently by different queries in parallel.</p>
<h4><span id="complex-pipelines">Complex pipelines</span></h4><p>For grouping and such more complex queries, MongoDB provides an API in the form of aggregation pipelines.<br>db.scientists.aggregate( { $match : { “Century” : 20 }}, { $group : { “Year” : “$year”, “Count” : { “$sum” : 1 } } }, { $sort : { “Count” : -1 } }, { $limit : 5 } ).</p>
<h3><span id="limitations-of-a-document-store-querying-api">Limitations of a document store querying API</span></h3><p>Simple use cases are straightforward to handle, however more complex use cases require a lot of additional code in the host language, be it JavaScript or Python. An example is that joins must be taken care of by the end user in the host language: the burden of implementing more complex use cases is pushed to the end user.</p>
<h2><span id="architecture">Architecture</span></h2><p>The architecture of MongoDB follows similar principles to what we covered before: scaling out the hardware to multiple machine, and sharding as well as replicating the data.</p>
<h3><span id="sharding-collections">Sharding collections</span></h3><p>Collections in MongoDB can be sharded. Shards are determined by selecting one or several fields. (Lexicographically-ordered) intervals over these fields then determine the shards. The fields used to shard must be organized in a tree index structure.</p>
<h3><span id="replica-sets">Replica sets</span></h3><p>A replica set is a set of several nodes running the MongoDB server process. The nodes within the same replica set all have a copy of the same data.</p>
<p>Each shard of each collection is assigned to exactly one replica set. Note that this architecture is not the same as that of HDFS, in which the replicas are spread over the entire cluster with no notion of “walls” between replica sets and no two DataNodes having the exact same block replicas.</p>
<h3><span id="write-concerns">Write concerns</span></h3><p>When writing (be it delete, update or insert) to a collection, more exactly, to a specific shard of a collection, MongoDB checks that a specific minimum number of nodes (within the replica set that is responsible for the shard) have successfully performed the update.</p>
<h3><span id="motivation">Motivation</span></h3><p>A document store, unlike a data lake, manages the physical data layout. This has a cost: the need to import (ETL) data before it is possible to query it, but this cost comes with a nice benefit: index support, just like relational database management systems.</p>
<h3><span id="hash-indices">Hash indices</span></h3><p>Hash indices are used to optimize point queries and more generally query that select on a specific value of a field. The general idea is that all the values that a field takes in a specific collection can be hashed to an integer. The value, together with pointers to the corresponding documents, is then placed in a physical array in memory, at the position corresponding to this integer.</p>
<h3><span id="tree-indices">Tree indices</span></h3><p>Hash indices are great and fast, but have limitations: first, they consume space. The more one wants to avoid hash collisions, the larger the array needs to be. But more importantly, hash indices cannot support range queries. This is because hashes do not preserve the order of the values and distribute them “randomly” in the index array structure.</p>
<p>Range queries are supported with tree indices. Instead of an array, tree indices use some sort of tree structure in which they arrange the possible values of the indexed field, such that the values are ordered when traversing the tree in a depth-first-search manner. More precisely, the structure is called a B+-tree. Unlike a simple binary tree, nodes have a large number of children.</p>
<h3><span id="secondary-indices">Secondary indices</span></h3><p>By default, MongoDB always builds a tree index for the id field. Users can request to build hash and tree indices for more fields. These indices are called secondary indices.</p>
<p>The command for building a hash index looks like so: db.scientists.createIndex({ “Name.Last” : “hash” })</p>
<p>And for a tree index (1 means in ascending order, -1 would be descending): db.scientists.createIndex({ “Name.Last” : 1 }).</p>
<h3><span id="when-are-indices-useful">When are indices useful</span></h3><p>When building indices, it is important to get a feeling for whether a query will be faster or not with this index.</p>
<h3><span id="index-types">index types</span></h3><h4><span id="single-field-indexes">Single Field Indexes</span></h4><p>By default, all collections have an index on the _id field. You can add additional indexes to speed up important queries and operations. You can create a single-field index on any field in a document, including:Top-level document fields, Embedded documents ,Fields within embedded documents. When you create an index on an embedded document, only queries that specify the entire embedded document use the index. Queries on a specific field within the document do not use the index. In order for a dot notation query to use an index, you must create an index on the specific embedded field you are querying, not the entire embedded object. </p>
<h4><span id="compound-indexes">Compound Indexes</span></h4><p>Compound indexes collect and sort data from two or more fields in each document in a collection. Data is grouped by the first field in the index and then by each subsequent field.</p>
<p>The order of the indexed fields impacts the effectiveness of a compound index. Compound indexes contain references to documents according to the order of the fields in the index. To create efficient compound indexes, follow the ESR (Equality, Sort, Range) rule. The ESR (Equality, Sort, Range) Rule is to place fields that require exact matches first in your index. Sort follows equality matches because the equality matches reduce the number of documents that need to be sorted. Sorting after the equality matches also allows MongoDB to do a non-blocking sort. “Range” filters scan fields. The scan doesn’t require an exact match, which means range filters are loosely bound to index keys. To improve query efficiency, make the range bounds as tight as possible and use equality matches to limit the number of documents that must be scanned. MongoDB cannot do an index sort on the results of a range filter. Place the range filter after the sort predicate so MongoDB can use a non-blocking index sort.  </p>
<p>Compound indexes cannot support queries where the sort order does not match the index or the reverse of the index. </p>
<p>Index prefixes are the beginning subsets of indexed fields. Compound indexes support queries on all fields included in the index prefix. Index fields are parsed in order; if a query omits an index prefix, it is unable to use any index fields that follow that prefix.</p>
<h4><span id="multikey-indexes">Multikey Indexes</span></h4><p>Multikey indexes collect and sort data from fields containing array values. Multikey indexes improve performance for queries on array fields.</p>
<p>In a compound multikey index, each indexed document can have at most one indexed field whose value is an array. </p>
<h2><span id="exercise">exercise</span></h2><h3><span id="data-model">data model</span></h3><h4><span id="embedded-data-models">Embedded Data Models</span></h4><p>In general, embedding provides better performance for read operations, as well as the ability to request and retrieve related data in a single database operation. Embedded data models make it possible to update related data in a single atomic write operation.</p>
<h4><span id="normalized-data-models">Normalized Data Models</span></h4><p>Normalized data models describe relationships using references between documents.</p>
<h2><span id="references">references</span></h2><p><a target="_blank" rel="noopener" href="https://www.mongodb.com/docs/manual/core/data-model-design/">https://www.mongodb.com/docs/manual/core/data-model-design/</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-11-17T15:56:37.000Z" title="2023-11-17 4:56:37 ├F10: PM┤">2023-11-17</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-02-21T11:20:42.482Z" title="2024-2-21 12:20:42 ├F10: PM┤">2024-02-21</time></span><span class="level-item"><a class="link-muted" href="/categories/theory/">theory</a></span><span class="level-item">8 minutes read (About 1205 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/11/17/pai8/">pai - Tabular Reinforcement Learning</a></p><div class="content"><h1><span id="tabular-reinforcement-learning">Tabular Reinforcement Learning</span></h1><h2><span id="the-reinforcement-learning-problem">The Reinforcement Learning Problem</span></h2><p>Reinforcement learning is concerned with probabilistic planning in unknown environments. In this chapter, we will begin by considering reinforcement learning with small state and action spaces. This setting is often called the tabular setting, as the value functions can be computed exhaustively for all states and stored in a table.</p>
<p>Clearly, the agent needs to trade exploring and learning about the environment with exploiting its knowledge to maximize rewards. In fact, Bayesian optimization can be viewed as reinforcement learning with a fixed state and a continuous action space: In each round, the agent plays an action, aiming to find the action that maximizes the reward.Another key challenge of reinforcement learning is that the observed data is dependent on the played actions.</p>
<h3><span id="trajectories">Trajectories</span></h3><p><img src="/2023/11/17/pai8/image-64.png" alt="Alt text"></p>
<p>Crucially, the newly observed states xt+1 and the rewards rt (across multiple transitions) are conditionally independent given the previous states xt and actions at. This independence property is crucial for being able to learn about the underlying Markov decision process. Notably, this implies that we can apply the law of large numbers (1.83) and Hoeffding’s inequality (1.87) to our estimators of both quantities.</p>
<p>The collection of data is commonly classified into two settings. In the episodic setting, the agent performs a sequence of “training” rounds (called episodes). In the beginning of each round, the agent is reset to some initial state. In contrast, in the continuous setting (or non-episodic,or online setting), the agent learns online. </p>
<h3><span id="control">control</span></h3><p>Another important distinction in how data is collected, is the distinction between on-policy and off-policy control. As the names suggest, on-policy methods are used when the agent has control over its own actions, in other words, the agent can freely choose to follow any policy. In contrast, off-policy methods can be used even when the agent cannot freely choose its actions. Off-policy methods are therefore able to make use of observational data.Off-policy methods are therefore more sample-efficient than on-policy methods. This is crucial, especially in settings where conducting experiments (i.e., collecting new data) is expensive.</p>
<p>On-Policy learning algorithms are the algorithms that evaluate and improve the same policy which is being used to select actions. Off-Policy learning algorithms evaluate and improve a policy that is different from Policy that is used for action selection.</p>
<p>To understand the difference between on-policy learning and off-policy learning one must first understand the difference between the behavior policy (i.e., sampling policy) and the update policy. The behavior policy is the policy an agent follows when choosing which action to take in the environment at each time step. The update policy is how the agent updates the Q-function. On-policy algorithms attempt to improve upon the current behavior policy that is used to make decisions and therefore these algorithms learn the value of the policy carried out by the agent, Off-policy algorithms learn the value of the optimal policy and can improve upon a policy that is different from the behavior policy. Determining if the update and behavior policy are the same or different can give us insight into whether or not the algorithm is on-policy or off-policy.</p>
<h2><span id="model-based-approaches">Model-based Approaches</span></h2><p>Approaches to reinforcement learning are largely categorized into two classes. Model-based approaches aim to learn the underlying Markov decision process. In contrast, model-free approaches learn the value function directly.</p>
<h3><span id="learning-the-underlying-markov-decision-process">Learning the Underlying Markov Decision Process</span></h3><p>A natural first idea is to use maximum likelihood estimation to approximate transition and reward function.</p>
<p><img src="/2023/11/17/pai8/image-65.png" alt="Alt text"></p>
<h3><span id="ε-greedy-algorithm">ε-greedy Algorithm</span></h3><p><img src="/2023/11/17/pai8/image-66.png" alt="Alt text"></p>
<p>The key problem of ε-greedy is that it explores the state space in an uninformed manner. In other words, it explores ignoring all past experience. It thus does not eliminate clearly suboptimal actions.</p>
<h3><span id="rmax-algorithm">Rmax Algorithm</span></h3><p>A key principle in effectively trading exploration and exploitation is “optimism in the face of uncertainty”. Let us apply this principle to the reinforcement learning setting. The key idea is to assume that the dynamics and rewards model “work in our favor” until we have learned “good estimates” of the true dynamics and rewards. </p>
<p><img src="/2023/11/17/pai8/image-67.png" alt="Alt text"></p>
<p>How many transitions are “enough”? We can use Hoeffding’s inequality to get a rough idea!</p>
<p><img src="/2023/11/17/pai8/image-68.png" alt="Alt text"></p>
<h3><span id="challenges">challenges</span></h3><h2><span id="model-free-approaches">Model-free Approaches</span></h2><p>A significant benefit to model-based reinforcement learning is that it is inherently off-policy. That is, any trajectory regardless of the policy used to obtain it can be used to improve the model of the underlying Markov decision process. In the model-free setting, this not necessarily true.</p>
<h3><span id="on-policy-value-estimation">On-policy Value Estimation</span></h3><p><img src="/2023/11/17/pai8/image-71.png" alt="Alt text"></p>
<p>Note that to estimate this expectation we use a single(!) sample.However, there is one significant problem in this approximation. Our approximation of vπ does in turn depend on the (unknown) true value of vπ. The key idea is to use a bootstrapping estimate of the value function instead. That is, in place of the true value function vπ, we will use a “running estimate” Vπ. In other words, whenever observing a new transition, we use our previous best estimate of vπ to obtain a new estimate Vπ.</p>
<p>Crucially, using a bootstrapping estimate generally results in biased estimates of the value function. Moreover, due to relying on a single sample, the estimates tend to have very large variance. </p>
<h4><span id="td-learning">TD-learning</span></h4><p>The variance of the estimate is typically reduced by mixing new estimates of the value function with previous estimates using a learning rate αt. This yields the temporal-difference learning algorithm.</p>
<p><img src="/2023/11/17/pai8/image-70.png" alt="Alt text"></p>
<p>TD-learning is a fundamentally on-policy method. That is, for the estimates Vπ to converge to the true value function vπ, the transitions that are used for the estimation must follow policy π. </p>
<h4><span id="sarsa">SARSA</span></h4><p><img src="/2023/11/17/pai8/image-69.png" alt="Alt text"></p>
<h3><span id="off-policy-value-estimation">Off-policy Value Estimation</span></h3><p><img src="/2023/11/17/pai8/image-72.png" alt="Alt text"><br>This adapted update rule explicitly chooses the subsequent action a′ according to policy π whereas SARSA absorbs this choice into the Monte Carlo approximation. The algorithm has analogous convergence guarantees to those of SARSA. Crucially, this algorithm is off-policy. As noted, the key difference to the on-policy TD-learning and SARSA is that our estimate of the Qfunction explicitly keeps track of the next-performed action. It does so for any action in any state.</p>
<h3><span id="q-learning">Q-learning</span></h3><p><img src="/2023/11/17/pai8/image-73.png" alt="Alt text"><br>Crucially, the Monte Carlo approximation of eq. (11.21) does not depend on the policy. Thus, Q-learning is an off-policy method.<br><img src="/2023/11/17/pai8/image-74.png" alt="Alt text"></p>
<h3><span id="optimistic-q-learning">Optimistic Q-learning</span></h3><p><img src="/2023/11/17/pai8/image-76.png" alt="Alt text"></p>
<h3><span id="challenges">Challenges</span></h3><p>We have seen that both the model-based Rmax algorithm and the modelfree Q-learning take time polynomial in the number of states |X| and the number of actions |A| to converge. While this is acceptable in small grid worlds, this is completely unacceptable for large state and action spaces.</p>
<h2><span id="references">references</span></h2><p><a target="_blank" rel="noopener" href="https://core-robotics.gatech.edu/2022/02/28/bootcamp-summer-2020-week-4-on-policy-vs-off-policy-reinforcement-learning/">https://core-robotics.gatech.edu/2022/02/28/bootcamp-summer-2020-week-4-on-policy-vs-off-policy-reinforcement-learning/</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-11-17T15:56:14.000Z" title="2023-11-17 4:56:14 ├F10: PM┤">2023-11-17</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-02-21T11:20:45.095Z" title="2024-2-21 12:20:45 ├F10: PM┤">2024-02-21</time></span><span class="level-item"><a class="link-muted" href="/categories/theory/">theory</a></span><span class="level-item">4 minutes read (About 567 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/11/17/pai7/">pai - Markov Decision Processes</a></p><div class="content"><h1><span id="markov-decision-processes">Markov Decision Processes</span></h1><p>Planning deals with the problem of deciding which action an agent should play in a (stochastic) environment(An environment is stochastic as opposed to deterministic, when the outcome of actions is random.). A key formalism for probabilistic plan ning in known environments are so-called Markov decision processes.<br><img src="/2023/11/17/pai7/image-50.png" alt="Alt text"></p>
<p>Our fundamental objective is to learn how the agent should behave to optimize its reward. In other words, given its current state, the agent should decide (optimally) on the action to play. Such a decision map — whether optimal or not — is called a policy.</p>
<p><img src="/2023/11/17/pai7/image-51.png" alt="Alt text"></p>
<p><img src="/2023/11/17/pai7/image-52.png" alt="Alt text"></p>
<p>For the purpose of our discussion of Markov decision processes and reinforcement learning, we will focus on a very common reward called discounted payoff.</p>
<p><img src="/2023/11/17/pai7/image-53.png" alt="Alt text"></p>
<p><img src="/2023/11/17/pai7/image-54.png" alt="Alt text"></p>
<p>Because we assumed stationary dynamics, rewards, and policies, the discounted payoff starting from a given state x will be independent of the start time t.</p>
<h2><span id="bellman-expectation-equation">Bellman Expectation Equation</span></h2><p>Let us now see how we can compute the value function：</p>
<p><img src="/2023/11/17/pai7/image-55.png" alt="Alt text"></p>
<p>This equation is known as the Bellman expectation equation, and it shows a recursive dependence of the value function on itself. The intuition is clear: the value of the current state corresponds to the reward from the next action plus the discounted sum of all future rewards obtained from the subsequent states.</p>
<h2><span id="policy-evaluation">Policy Evaluation</span></h2><p>Bellman’s expectation equation tells us how we can find the value function vπ of a fixed policy π using a system of linear equations.</p>
<p><img src="/2023/11/17/pai7/image-56.png" alt="Alt text"></p>
<h3><span id="fixed-point-iteration">Fixed-point Iteration</span></h3><p><img src="/2023/11/17/pai7/image-57.png" alt="Alt text"></p>
<h2><span id="policy-optimization">Policy Optimization</span></h2><h3><span id="greedy-policies">Greedy Policies</span></h3><p><img src="/2023/11/17/pai7/image-58.png" alt="Alt text"></p>
<h3><span id="bellman-optimality-equation">Bellman Optimality Equation</span></h3><p><img src="/2023/11/17/pai7/image-59.png" alt="Alt text"></p>
<p><img src="/2023/11/17/pai7/image-60.png" alt="Alt text"><br>These equations are also called the Bellman optimality equations. Intuitively, the Bellman optimality equations express that the value of a state under an optimal policy must equal the expected return for the best action from that state. Bellman’s theorem is also known as Bellman’s optimality principle, which is a more general concept.</p>
<h3><span id="policy-iteration">Policy Iteration</span></h3><p><img src="/2023/11/17/pai7/image-61.png" alt="Alt text"></p>
<p>It can be shown that policy iteration converges to an exact solution in a polynomial number of iterations.Each iteration of policy iteration requires computing the value function, which we have seen to be of cubic complexity in the number of states. </p>
<h3><span id="value-iteration">Value Iteration</span></h3><p><img src="/2023/11/17/pai7/image-62.png" alt="Alt text"><br>Value iteration converges to an ε-optimal solution in a polynomial number of iterations. Unlike policy iteration, value iteration does not converge to an exact solution in general.an iteration of 7 Sparsity refers to the interconnectivity of the state space. When only few states are reachable from any state, we call an MDP sparse. value iteration can be performed in (virtually) constant time in sparse Markov decision processes.</p>
<h2><span id="partial-observability">Partial Observability</span></h2><p>In this section, we consider how Markov decision processes can be extended to a partially observable setting where the agent can only access noisy observations Yt of its state Xt.</p>
<p><img src="/2023/11/17/pai7/image-63.png" alt="Alt text"></p>
<p>Observe that a Kalman filter can be viewed as a hidden Markov model with conditional linear Gaussian motion and sensor models and a Gaussian prior on the initial state.</p>
<p>POMDPs can be reduced to a Markov decision process with an enlarged state space.</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-11-17T15:41:00.000Z" title="2023-11-17 4:41:00 ├F10: PM┤">2023-11-17</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-02-21T11:17:40.535Z" title="2024-2-21 12:17:40 ├F10: PM┤">2024-02-21</time></span><span class="level-item"><a class="link-muted" href="/categories/theory/">theory</a></span><span class="level-item">8 minutes read (About 1149 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/11/17/pai6/">pai - Bayesian Optimization</a></p><div class="content"><h1><span id="bayesian-optimization">Bayesian Optimization</span></h1><p><img src="/2023/11/17/pai6/image-39.png" alt="Alt text"></p>
<h2><span id="exploration-exploitation-dilemma">Exploration-Exploitation Dilemma</span></h2><p>In Bayesian optimization, we want to learn a model of f ⋆ and use this model to optimize f ⋆ simultaneously. These goals are somewhat contrary. Learning a model of f ⋆ requires us to explore the input space while using the model to optimize f ⋆ requires us to focus on the most promising well-explored areas. This trade-off is commonly known as the exploration-exploitation dilemma.</p>
<p>It is common to use a so-called acquisition function to greedily pick the next point to sample based on the current model.</p>
<h2><span id="online-learning-and-bandits">Online Learning and Bandits</span></h2><p><img src="/2023/11/17/pai6/image-40.png" alt="Alt text"></p>
<h3><span id="multi-armed-banditsmab">Multi-Armed Bandits(MAB)</span></h3><p>The “multi-armed bandits” (MAB) problem is a classical, canonical formalization of the exploration-exploitation dilemma. In the MAB problem, we are provided with k possible actions (arms) and want to maximize our reward online within the time horizon T. We do not know the reward distributions of the actions in advance, however, so we need to trade learning the reward distribution with following the most promising action.</p>
<p>Bayesian optimization can be interpreted as a variant of the MAB problem where there can be a potentially infinite number of actions (arms), but their rewards are correlated (because of the smoothness of the Gaussian process prior). Smoothness means that nearby points in the input space are likely to have similar function values. Because of the smoothness property inherent in the Gaussian process prior, Bayesian optimization can make informed decisions about where to explore next in the input space. The model can leverage information from previously evaluated points to predict the behavior of the objective function at unexplored points more effectively.</p>
<h3><span id="regret">Regret</span></h3><p>The key performance metric in online learning is the regret.</p>
<p><img src="/2023/11/17/pai6/image-41.png" alt="Alt text"></p>
<p>Achieving sublinear regret requires balancing exploration with exploitation. Typically, online learning (and Bayesian optimization) consider stationary environments, hence the comparison to the static optimum.</p>
<h3><span id="acquisition-functions">Acquisition Functions</span></h3><p>Throughout our description of acquisition functions, we will focus on a setting where we model $f^⋆$ using a Gaussian process which we denote by f. The methods generalize to other means of learning $f^⋆$ such as Bayesian neural networks.</p>
<p><img src="/2023/11/17/pai6/image-42.png" alt="Alt text"></p>
<p>One possible acquisition function is uncertainty sampling. However, this acquisition function does not at all take into account the objective of maximizing $f^⋆$ and focuses solely on exploration. </p>
<p>Suppose that our model f of $f^⋆$  is well-calibrated, in the sense that the true function lies within its confidence bounds. Consider the best lower bound, that is, the maximum of the lower confidence bound. Now, if the true function is really contained in the confidence bounds, it must hold that the optimum is somewhere above this best lower bound.</p>
<p>Therefore, we only really care how the function looks like in the regions where the upper confidence bound is larger than the best lower bound. The key idea behind the methods that we will explore is to focus exploration on these plausible maximizers.</p>
<h3><span id="upper-confidence-bound">Upper Confidence Bound</span></h3><p><img src="/2023/11/17/pai6/image-43.png" alt="Alt text"><br>This acquisition function naturally trades exploitation by preferring a large posterior mean with exploration by preferring a large posterior variance.<br>This optimization problem is non-convex in general. However, we can use approximate global optimization techniques like Lipschitz optimization (in low dimensions) and gradient ascent with random initialization (in high dimensions). Another widely used option is to sample some random points from the domain, score them according to this criterion, and simply take the best one.</p>
<p><img src="/2023/11/17/pai6/image-44.png" alt="Alt text"><br>Observe that if the information gain is sublinear in T then we achieve sublinear regret and, in particular, converge to the true optimum. The term “sublinear” refers to a growth rate that is slower than linear. </p>
<p><img src="/2023/11/17/pai6/image-45.png" alt="Alt text"><br>Intuitively, to work even if the unknown function $f^⋆$  is not contained in the confidence bounds, we use βt to re-scale the confidence bounds to enclose $f^⋆$.</p>
<h3><span id="probability-of-improvement">Probability of Improvement</span></h3><p><img src="/2023/11/17/pai6/image-46.png" alt="Alt text"></p>
<p>Probability of improvement tends to be biased in favor of exploitation, as it prefers points with large posterior mean and small posterior variance.</p>
<h3><span id="expected-improvement">Expected Improvement</span></h3><p><img src="/2023/11/17/pai6/image-47.png" alt="Alt text"></p>
<p>Intuitively, expected improvement seeks a large expected improvement (exploitation) while also preferring states with a large variance (exploration).</p>
<h3><span id="thompson-sampling">Thompson Sampling</span></h3><p><img src="/2023/11/17/pai6/image-48.png" alt="Alt text"></p>
<p>Probability matching is exploratory as it prefers points with larger variance (as they automatically have a larger chance of being optimal), but at the same time exploitative as it effectively discards points with low posterior mean and low posterior variance. Unfortunately, it is generally difficult to compute π analytically given a posterior. Instead, it is common to use a sampling-based approximation of π.</p>
<p><img src="/2023/11/17/pai6/image-49.png" alt="Alt text"></p>
<p>In many cases, the randomness in the realizations of  ̃ ft+1 is already sufficient to effectively trade exploration and exploitation.</p>
<h2><span id="model-selection">Model Selection</span></h2><p>Selecting a model of f ⋆ is much harder than in the i.i.d. data setting of supervised learning. There are mainly the two following dangers, • the data sets collected in active learning and Bayesian optimization are small; and • the data points are selected dependently on prior observations. This leads to a specific danger of overfitting. In particular, due to feedback loops between the model and the acquisition function, one may end up sampling the same point repeatedly.</p>
<p>Another approach that often works fairly well is to occasionally (according to some schedule) select points uniformly at random instead of using the acquisition function. This tends to prevent getting stuck in suboptimal parts of the state space.</p>
<h2><span id="difference-betwwen-active-learning-and-bayesian-optimization">difference betwwen active learning and Bayesian Optimization</span></h2><p>Problem Setting:<br>Active Learning: Active learning typically deals with supervised learning tasks where there is a large pool of unlabeled instances, and the algorithm decides which instances to query for labels to improve the model.<br>Bayesian Optimization: Bayesian optimization deals with optimization problems where the objective function is unknown, expensive to evaluate, and possibly noisy. It aims to find the global optimum with as few evaluations as possible.</p>
<p>Nature of Queries:<br>Active Learning: In active learning, the queries are often in the form of “Which instance should be labeled next?” The goal is to select instances that will most benefit the model’s learning process.<br>Bayesian Optimization: In Bayesian optimization, the queries are in the form of “What point should be evaluated next in the input space to maximize/minimize the objective function?” The goal is to efficiently explore the input space and find the optimal configuration.</p>
<p>Algorithmic Approaches:<br>Active Learning: Active learning involves various strategies such as uncertainty sampling, query-by-committee, and diversity sampling to select informative instances for labeling.<br>Bayesian Optimization: Bayesian optimization employs probabilistic surrogate models (often Gaussian processes) to model the unknown objective function. Acquisition functions guide the search to balance exploration and exploitation efficiently.</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-11-16T12:02:24.000Z" title="2023-11-16 1:02:24 ├F10: PM┤">2023-11-16</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-02-21T11:20:47.323Z" title="2024-2-21 12:20:47 ├F10: PM┤">2024-02-21</time></span><span class="level-item"><a class="link-muted" href="/categories/theory/">theory</a></span><span class="level-item">4 minutes read (About 606 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/11/16/pai5/">pai - active learning</a></p><div class="content"><h1><span id="active-learning">Active learning</span></h1><p>Active learning is a machine learning paradigm in which a model is trained on a dataset that is not entirely labeled. Instead of relying solely on a fully labeled dataset for training, active learning involves an iterative process where the model actively selects the most informative instances from an unlabeled pool, queries an oracle (typically a human annotator), and adds the newly labeled instances to the training set. The model is then retrained on the expanded labeled dataset.</p>
<p>The key idea behind active learning is to strategically choose the most valuable instances for labeling, with the goal of improving the model’s performance while minimizing the number of labeled instances needed. This process is especially beneficial when obtaining labeled data is expensive or time-consuming.</p>
<p>Active learning strategies vary in how they select instances for labeling. Common strategies include uncertainty sampling (select instances where the model is uncertain), query-by-committee (select instances where different model hypotheses disagree), and diversity sampling (select instances to ensure diverse coverage of the input space).</p>
<h2><span id="conditional-entropy">Conditional Entropy</span></h2><p><img src="/2023/11/16/pai5/image-29.png" alt="Alt text"></p>
<p>Intuitively, the conditional entropy of X given Y describes our average surprise about realizations of X given a particular realization of Y, averaged over all such possible realizations of Y. In other words, conditional entropy corresponds to the expected remaining uncertainty in X after we observe Y.</p>
<p><img src="/2023/11/16/pai5/image-30.png" alt="Alt text"><br>That is, the joint entropy of X and Y is given by the uncertainty about X and the additional uncertainty about Y given X.</p>
<h2><span id="mutual-information">Mutual Information</span></h2><p><img src="/2023/11/16/pai5/image-31.png" alt="Alt text"></p>
<p>In words, we subtract the uncertainty left about X after observing Y from our initial uncertainty about X. This measures the reduction in our uncertainty in X (as measured by entropy) upon observing Y.</p>
<p><img src="/2023/11/16/pai5/image-32.png" alt="Alt text"></p>
<p>Thus, the mutual information between X and Y can be understood as the approximation error (or information loss) when assuming that X and Y are independent.</p>
<p><img src="/2023/11/16/pai5/image-33.png" alt="Alt text"></p>
<p>Thus, the conditional mutual information corresponds to the reduction of uncertainty in X when observing Y, given we already observed Z.</p>
<p>Following our introduction of mutual information, it is natural to answer the question “where should I collect data?” by saying “wherever mutual information is maximized”.</p>
<h2><span id="submodularity-of-mutual-information">Submodularity of Mutual Information</span></h2><p><img src="/2023/11/16/pai5/image-34.png" alt="Alt text"></p>
<p><img src="/2023/11/16/pai5/image-35.png" alt="Alt text"></p>
<p>That is, “adding” x to the smaller set A yields more marginal gain than adding x to the larger set B. In other words, the function F has “diminishing returns”. In this way, submodularity can be interpreted as a notion of “concavity” for discrete functions.</p>
<p><img src="/2023/11/16/pai5/image-36.png" alt="Alt text"></p>
<h2><span id="maximizing-mutual-information">Maximizing Mutual Information</span></h2><h3><span id="uncertainty-sampling">Uncertainty Sampling</span></h3><p><img src="/2023/11/16/pai5/image-37.png" alt="Alt text"></p>
<p>Therefore, if f is modeled by a Gaussian and we assume homoscedastic noise, greedily maximizing mutual information corresponds to simply picking the point x with the largest variance. This strategy is also called uncertainty sampling.</p>
<h3><span id="heteroscedastic-noise">Heteroscedastic Noise</span></h3><p>Uncertainty sampling is clearly problematic if the noise is heteroscedastic. If there are a particular set of inputs with a large aleatoric uncertainty dominating the epistemic uncertainty, uncertainty sampling will continuously choose those points even though the epistemic uncertainty will not be reduced substantially.<br><img src="/2023/11/16/pai5/image-38.png" alt="Alt text"></p>
<p>Thus, we choose locations that trade large epistemic uncertainty with large aleatoric uncertainty. Ideally, we find a location where the epistemic uncertainty is large, and the aleatoric uncertainty is low, which promises a significant reduction of uncertainty around this location.</p>
<h3><span id="classification">Classification</span></h3><p>Here, uncertainty sampling corresponds to selecting samples that maximize the entropy of the predicted label yx.</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-11-14T12:11:13.000Z" title="2023-11-14 1:11:13 ├F10: PM┤">2023-11-14</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-02-21T11:23:43.615Z" title="2024-2-21 12:23:43 ├F10: PM┤">2024-02-21</time></span><span class="level-item"><a class="link-muted" href="/categories/database/">database</a></span><span class="level-item">11 minutes read (About 1721 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/11/14/bigdata8/">bigdata - spark</a></p><div class="content"><h1><span id="generic-dataflow-management">Generic Dataflow Management</span></h1><p>MapReduce is very simple and generic, but many more complex uses involve not just one, but a sequence of several MapReduce jobs. Furthermore, the MapReduce API is low-level, and most users need higherlevel interfaces, either in the form of APIs or query languages. This is why, after MapReduce, another generation of distributed processing technologies were invented. The most popular one is the open source Apache Spark.</p>
<h2><span id="a-more-general-dataflow-model">A more general dataflow model</span></h2><p>MapReduce consists of a map phase, followed by shuffling, followed by a reduce phase. In fact, the map phase and the reduce phase are not so different: both involve the computation of tasks in parallel on slots.</p>
<h2><span id="resilient-distributed-datasets">Resilient distributed datasets</span></h2><p>The first idea behind generic dataflow processing is to allow the dataflow to be arranged in any distributed acyclic graph (DAG).</p>
<p><img src="/2023/11/14/bigdata8/image-28.png" alt="Alt text"></p>
<p>All the rectangular nodes in the above graph correspond to intermediate data. They are called resilient distributed datasets, or in short, RDDs.</p>
<p>A major difference with MapReduce, though, is that RDDs need not be collections of pairs. In fact, RDDs can be (ordered) collections of just about anything: strings, dates, integers, objects, arrays, arbitrary JSON values, XML nodes, etc.The only constraint is that the values within the same RDD share the same static type, which does not exclude the use of polymorphism.</p>
<h2><span id="the-rdd-lifecycle">The RDD lifecycle</span></h2><h3><span id="creation">Creation</span></h3><p>RDDs undergo a lifecycle. First, they get created. RDDs can be created by reading a dataset from the local disk, or from cloud storage, or from a distributed file system, or from a database source, or directly on the fly from a list of values residing in the memory of the client using Apache Spark</p>
<h3><span id="transformation">Transformation</span></h3><p>Then, RDDs can be transformed into other RDDs. Mapping or reducing, in this model, become two very specific cases of transformations. However, Spark allows for many, many more kinds of transformations. This also includes transformations with several RDDs as input.</p>
<h3><span id="action">Action</span></h3><p>RDDs can also undergo a final action leading to making an output persistent. This can be by outputting the contents of an RDD to the local disk, to cloud storage, to a distributed file system, to a database system, or directly to the screen of the user.</p>
<h3><span id="lazy-evaluation">Lazy evaluation</span></h3><p>Another important aspect of the RDD lifecycle is that the evaluation is lazy: in fact, creations and transformations on their own do nothing. It is only with an action that the entire computation pipeline is put into motion, leading to the computation of all the necessary intermediate RDDs all the way down to the final output corresponding to the action.</p>
<h2><span id="transformations">Transformations</span></h2><h3><span id="unary-transformations">Unary transformations</span></h3><p>Let us start with unary transformations, i.e., those that take a single RDD as their input.</p>
<h3><span id="binary-transformations">Binary transformations</span></h3><p>There are also transformations that take two RDDs as input.</p>
<h3><span id="pair-transformations">Pair transformations</span></h3><p>Spark has transformations specifically tailored for RDDs of key-value pairs.</p>
<h2><span id="actions">Actions</span></h2><h3><span id="gathering-output-locally">Gathering output locally</span></h3><p>The collect action downloads all values of an RDD on the client machine and outputs them as a (local) list. It is important to only call this action on an RDD that is known to be small enough (e.g., after filtering) to avoid a memory overflow.</p>
<p>The count action computes (in parallel) the total number of values in the input RDD. This one is safe even for RDDs with billions of values, as it returns a simple integer to the client.</p>
<h3><span id="writing-to-sharded-datasets">Writing to sharded datasets</span></h3><p>There is also an action called saveAsTextFile that can save the entire RDD to a sharded dataset on Cloud storage (S3, Azure blob storage) or a distributed file system (HDFS).<br>Binary outputs can be saved with saveAsObjectFile.</p>
<h3><span id="actions-on-pair-rdds">Actions on Pair RDDs</span></h3><h2><span id="physical-architecture">Physical architecture</span></h2><h3><span id="narrow-dependency-transformations">Narrow-dependency transformations</span></h3><p>In a narrow-dependency transformation, the computation of each output value involves a single input value. In a narrow-dependency transformation, the computation of each output value involves a single input value.</p>
<p>By default, if the transformation is applied to an RDD freshly created from reading a dataset from HDFS, each partition will correspond to an HDFS block. Thus, the computation of the narrow-dependency transformation mostly involves local reads by short-circuiting HDFS.</p>
<p>In fact, the way this works is quite similar to MapReduce: the sequential calls of the transformation function on each input value within a single partition is called a task. And just like MapReduce, the tasks are assigned to slots. These slots correspond to cores within YARN containers. YARN containers used by Spark are called executors. The processing of the tasks is sequential within each executor, and tasks are executed in parallel across executors. And like in MapReduce, a queue of unprocessed tasks is maintained, and everytime a slot is done, it gets a new task. When all tasks have been assigned, the slots who are done become idle and wait for all others to complete.</p>
<h3><span id="chains-of-narrow-dependency-transformations">Chains of narrow-dependency transformations</span></h3><p>In fact, on the physical level, the physical calls of the underlying map/filter/etc functions are directly chained on each input value to directly produce the corresponding final, output value, meaning that the intermediate RDDs are not even materialized anywhere and exist purely logically.</p>
<p>Such a chain of narrow-dependency transformations executed efficiently as a single set of tasks is called a stage, which would correspond to what is called a phase in MapReduce.</p>
<h3><span id="physical-parameters">Physical parameters</span></h3><p>Users can parameterize how many executors there are, how many cores there are per executor and how much memory per executor (remember that these then correspond to YARN container requests).</p>
<h3><span id="shuffling">Shuffling</span></h3><p>What about wide-dependency transformations? They involve a shuffling of the data over the network, so that the data necessary to compute each partition of the output RDD is physically at the same location.Thus, on the high-level of a job, the physical execution consists of a sequence of stages, with shuffling happening everytime a new stage begins.</p>
<h2><span id="optimizations">Optimizations</span></h2><h3><span id="pinning-rdds">Pinning RDDs</span></h3><p>Everytime an action is triggered, all the computations of the ”reverse transitive closure” (i.e., all the way up the DAG through the reverted edges) are set into motion. In some cases, several actions might share subgraphs of the DAG. It makes sense, then, to “pin” the intermediate RDD by persisting it.</p>
<h3><span id="pre-partitioning">Pre-partitioning</span></h3><p>Shuffle is needed to bring together the data that is needed to jointly contribute to individual output values. If, however, Spark knows that the data is already located where it should be, then shuffling is not needed.</p>
<h2><span id="dataframes-in-spark">DataFrames in Spark</span></h2><h3><span id="data-independence">Data independence</span></h3><p>Unlike a relational database that has everything right off-theshelf, with RDDs, the user has to re-implement all the primitives they need. This is a breach of the data independence principle. The developers behind Spark addressed this issue in a subsequent version of Spark by extending the model with support for DataFrames and Spark SQL, bringing back a widely established and popular declarative, high-level language into the ecosystem.</p>
<h3><span id="a-specific-kind-of-rdd">A specific kind of RDD</span></h3><p>A DataFrame can be seen as a specific kind of RDD: it is an RDD of rows (equivalently: tuples, records) that has relational integrity, domain integrity, but not necessarily atomic integrity.</p>
<h3><span id="performance-impact">Performance impact</span></h3><p>DataFrames are stored column-wise in memory, meaning that the values that belong to the same column are stored together. Furthermore, since there is a known schema, the names of the attributes need not be repeated in every single row, as would be the case with raw RDDs. DataFrames are thus considerably more compact in memory than raw RDDs.</p>
<p>Generally, Spark converts Spark SQL to internal DataFrame transformation and eventually to a physical query plan. An optimizer known as Catalyst is then able to find many ways of making the execution faster.</p>
<h3><span id="input-formats">Input formats</span></h3><p>Note that Spark automatically infers the schema from discovering the JSON Lines file, which adds a static performance overhead that does not exist for raw RDDs. </p>
<h3><span id="dataframe-transformations">DataFrame transformations</span></h3><p>It is also possible, instead of Spark SQL, to use a transformation API similar to (but distinct from) the RDD transformation API.</p>
<p>Unlike the RDD transformation API, there is no guarantee that the execution will happen as written, as the optimizer is free to reorganize the actual computations.</p>
<h3><span id="dataframe-column-types">DataFrame column types</span></h3><p>DataFrames also support the three structured types: arrays, structs, and maps. As a reminder, structs have string keys and arbitrary value types and correspond to generic JSON objects, while in maps, all values have the same type. Thus, structs are more common than maps.</p>
<h3><span id="the-spark-sql-dialect">The Spark SQL dialect</span></h3><p>Note that both GROUP BY and ORDER BY will trigger a shuffle in the system, even though this can be optimized as the grouping key and the sorting key are the same. The SORT BY clause can sort rows within each partition, but not across partitions, i.e., does not induce any shuffling. The DISTRIBUTE BY clause forces a repartition by putting all rows with the same value (for the specified field(s)) into the same new partition.<br>Note that the SORT BY clause is used to return the result rows sorted within each partition in the user specified order. When there is more than one partition SORT BY may return result that is partially ordered. This is different than ORDER BY clause which guarantees a total order of the output.</p>
<p>A word of warning must be given on SORT, DISTRIBUTE and CLUSTER clauses: they are, in fact, a breach of data independence, because they expose partitions. </p>
<p>Spark SQL also comes with language features to deal with nested arrays and objects. First, nested arrays can be navigated with the explode() function. Lateral views are more powerful and generic than just an explode() because they give more control, and they can also be used to go down several levels of nesting. A lateral view can be intuitively described this way: the array mentioned in the LATERAL VIEW clause is turned into a second, virtual table with the rest of the original table is joined. The other clauses can then refer to columns in both the original and second, virtual table.</p>
<h2><span id="exercise">exercise</span></h2><h3><span id="rdd">RDD</span></h3><p>Why RDD should be immutable and lazy: immutable is for lineage.<br>Why need RDD partitioning: parallel computing and reduce shuffling.</p>
<h3><span id="dataframe-api">DataFrame API</span></h3><p>For nested array,use array_contains.</p>
<h3><span id="spark-sql">spark SQL</span></h3><p>In jupyter notebook, we can use “%load_ext sparksql_magic” directly.</p>
<h2><span id="reference">reference</span></h2><p><a target="_blank" rel="noopener" href="https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/">https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-11-09T13:28:54.000Z" title="2023-11-9 2:28:54 ├F10: PM┤">2023-11-09</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-11-09T17:58:09.122Z" title="2023-11-9 6:58:09 ├F10: PM┤">2023-11-09</time></span><span class="level-item"><a class="link-muted" href="/categories/theory/">theory</a></span><span class="level-item">a minute read (About 185 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/11/09/pai4/">Probabilistic Artificial Intelligence - Bayesian Deep Learning</a></p><div class="content"><h2><span id="swagstochastic-weight-averaging-gaussian">SWAG(Stochastic Weight Averaging Gaussian)</span></h2><p>This paper proposes a different approach to Bayesian deep learning: they use the information contained in the SGD trajectory to efficiently approximate the posterior distribution over the weights of the neural network [1].</p>
<h3><span id="swa">SWA</span></h3><p>This paper shows that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training [2].</p>
<h3><span id="cyclical-learning-rate-schedule">cyclical learning rate schedule</span></h3><h2><span id="calibration-of-modern-neural-networks">Calibration of Modern Neural Networks</span></h2><p>Confidence calibration – the problem of predicting probability estimates representative of the true correctness likelihood – is important for classification models in many applications. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration.</p>
<h2><span id="references">references</span></h2><p>[1] Maddox W J, Izmailov P, Garipov T, et al. A simple baseline for bayesian uncertainty in deep learning[J]. Advances in neural information processing systems, 2019, 32.<br>[2] Izmailov P, Podoprikhin D, Garipov T, et al. Averaging weights leads to wider optima and better generalization[J]. arXiv preprint arXiv:1803.05407, 2018.<br>[3] Guo C, Pleiss G, Sun Y, et al. On calibration of modern neural networks[C]//International conference on machine learning. PMLR, 2017: 1321-1330.</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-11-09T13:16:52.000Z" title="2023-11-9 2:16:52 ├F10: PM┤">2023-11-09</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-02-21T11:22:37.593Z" title="2024-2-21 12:22:37 ├F10: PM┤">2024-02-21</time></span><span class="level-item"><a class="link-muted" href="/categories/database/">database</a></span><span class="level-item">9 minutes read (About 1319 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/11/09/bigdata7/">bigdata - YARN</a></p><div class="content"><h1><span id="resource-management">Resource management</span></h1><p>The first version of MapReduce is inefficient in several respects. For this reason, the architecture was fundamentally changed by adding a resource management layer to the stack, adding one more level of decoupling between scheduling and monitoring. A resource management system, here YARN, is a very important building block not only for a better MapReduce, but also for many other technologies running on a cluster.</p>
<h2><span id="limitations-of-mapreduce-in-its-first-version">Limitations of MapReduce in its first version</span></h2><p>The JobTracker has a lot on its shoulders! It has to deal with resource management, scheduling, monitoring, the job lifecycle, and fault tolerance.The first consequence of this is scalability. The second consequence is the bottleneck that this introduces at the JobTracker level, which slows down the entire system. The third issue is that it is difficult to design a system that do many things well. The fourth issue is that resources are statically allocated to the Map or the Reduce phase, meaning that parts of the cluster remain idle during both phases. The fifth issue is the lack of fungibility between the Map phase and the Reduce phase.</p>
<h2><span id="yarn">YARN</span></h2><h3><span id="general-architecture">General architecture</span></h3><p>YARN means Yet Another Resource manager. It was introduced as an additional layer that specifically handles the management of CPU and memory resources in the cluster.</p>
<p>YARN, unsurprisingly, is based on a centralized architecture in which the coordinator node is called the ResourceManager, and the worker nodes are called NodeManagers. NodeManagers furthermore provide slots (equipped with exclusively allocated CPU and memory) known as containers.</p>
<p>When a new application is launched, the ResourceManager assigns one of the container to act as the ApplicationMaster which will take care of running the application. The ApplicationMaster can then communicate with the ResourceManager in order to book and use more containers in order to run jobs. This is a fundamental change from the initial MapReduce architecture, in which the JobTracker was also taking care of running the MapReduce job.</p>
<p><img src="/2023/11/09/bigdata7/image-27.png" alt="Alt text"></p>
<p>Thus, YARN cleanly separates between the general management of resources and bootstrapping new applications, which remains centralized on the coordinator node, and monitoring the job lifecycle, which is now delegated to one or more ApplicationMasters running concurrently. This means, in particular, that several applications can run concurrently in the same cluster. This ability, known as multi-tenancy, is very important for large companies or universities in order to optimize the use of their resources.</p>
<h3><span id="resource-management">Resource management</span></h3><p>In resource management, one abstracts away from hardware by distinguish between four specific resources used in a distributed database system. These four resources are: • Memory • CPU • Disk I/O • Network I/O.<br>ApplicationMasters can request and release containers at any time, dynamically. A container request is typically made by the ApplicationMasters with a specific demand. If the request is granted by the ResourceManager fully or partially, this is done indirectly by signing and issuing a container token to the ApplicationMaster that acts as proof that the resource was granted. The ApplicationMaster can then connect to the allocated NodeManager and send the token. The NodeManager will then check the validity of the token and provide the memory and CPU granted by the ResourceManager. The ApplicationMaster ships the code (e.g., as a jar file) as well as parameters, which then runs as a process with exclusive use of this memory and CPU.</p>
<h3><span id="job-lifecycle-management-and-fault-tolerance">Job lifecycle management and fault tolerance</span></h3><p>Version 2 of MapReduce works on top of YARN by leaving the job lifecycle management to an ApplicationMaster. The ApplicationMaster requests containers for the Map phase, and sets these containers up to execute Map tasks. As soon as a container is done executing a Map task, the ApplicationMaster will assign a new Map task to this container from the remaining queue, until no Map tasks are left.</p>
<p>a container in the Map phase can contain several Map slots. Sharing memory and containers across slots in this way improves the overall efficiency, because setting up a container adds latency. It is thus more efficient to allocate 10 containers of each 4 cores, compared to 40 containers of each 1 core.</p>
<p>In the event of a container crashing during the Map phase, the ApplicationMaster will handle this by re-requesting containers and restarting the failed tasks. In the case that some data is lost in the Reduce phase, it is possible that the entire job must be restarted, because this the only way to recreate the intermediate data is to re-execute the Map tasks.</p>
<h3><span id="scheduling">Scheduling</span></h3><p>The ResourceManager keeps track of the list of available NodeManagers (who can dynamically come and go) and their status. Just like in HDFS, NodeManagers send periodic heartbeats to the ResourceManager to give a sign of life. The ResourceManager also offers an interface so that ApplicationMasters can register and send container requests. ApplicationMasters also send periodic heartbeats to the ResourceManager. It is important to understand that, unlike the JobTracker, the ResourceManager does not monitor tasks, and will not restart slots upon failure. This job is left to the ApplicationMasters.</p>
<h2><span id="scheduling-strategies">Scheduling strategies</span></h2><h3><span id="fifo-scheduling">FIFO scheduling</span></h3><p>In FIFO scheduling, there is one application at a time running on the entire cluster. When it is done, the next application runs again on the entire cluster, and so on.</p>
<h3><span id="capacity-scheduling">Capacity scheduling</span></h3><p>In capacity scheduling, the resources of the cluster are partitioned into several sub-clusters of various sizes. Each one of these sub-clusters has its own queue of applications running in a FIFO fashion within this queue. Capacity scheduling also exists in a more “dynamic flavour” in which, when a sub-cluster is not currently used, its resources can be temporarily lent to the other sub-clusters. This is also in the spirit of usage maximization. </p>
<h3><span id="fair-scheduling">Fair scheduling</span></h3><p>Fair scheduling involves more complex algorithms that attempt to allocate resources in a way fair to all users of the cluster and based on the share they are normally entitled to. fair scheduling consists on making dynamic decisions regarding which requests get granted and which requests have to wait. Fair scheduling combines several ways to compute cluster shares: Steady fair share: this is the share of the cluster officially allocated to each department. Instantaneous fair share: this is the fair share that a department should ideally be allocated (according to economic and game theory considerations) at any point in time. This is a dynamic number that changes constantly, based on departments being idle: if a department is idle, then the instantaneous fair share of others department becomes higher than their steady fair shares. Current share: this is the actual share of the cluster that a department effectively uses at any point in time. This is highly dynamic. The current share does not necessarily match the instantaneous fair share because there is some inertia in the process: a department might be using more resources while another is idle. When the other department later stops being idle, these resources are not immediately withdrawn from the first department; rather, the first department will stop getting more resources, and the second department will gradually recover these resources as they get released by the first department.</p>
<p>The easiest case of fair scheduling is when only one resource is considered: for example, only CPU cores, or only memory. Things become more complicated when several resources are considered, in practice both CPU cores and memory. This problem was solved game-theoretically with the Dominant Resource Fairness algorithm. The two (or more) dimensions are projected again to a single dimension by looking at the dominant resource for each user.</p>
<h2><span id="exercise">exercise</span></h2><h3><span id="motivation">motivation</span></h3><p>improve hadoop 1.x by adding a layer YARN to separate resource management from data processing.</p>
<h3><span id="architecture">architecture</span></h3><p>NodeMananger is generalized taskTracker.<br>On ResourceManager, there is an ApplicationManager rsponsible for admiting new jobs and collecting finished jobs and logs.<br>A scheduler has a global view to assign an ApplicationMaster. The applicationMaster will compute how many resources needed and send a request to the scheduler. </p>
<p>fault tolerance is provided by applicationMaster+NodeManager+HDFS.</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-10-31T15:42:31.000Z" title="2023-10-31 4:42:31 ├F10: PM┤">2023-10-31</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-02-21T11:22:34.940Z" title="2024-2-21 12:22:34 ├F10: PM┤">2024-02-21</time></span><span class="level-item"><a class="link-muted" href="/categories/database/">database</a></span><span class="level-item">11 minutes read (About 1586 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/10/31/bigdata6/">bigdata - Map Reduce</a></p><div class="content"><h2><span id="patterns-of-large-scale-query-processing">Patterns of large-scale query processing</span></h2><h3><span id="textual-input">Textual input</span></h3><p>We saw that the data we want to query can take many forms. First, it can be billions of lines of text. It can also be plenty of CSV lines. Some other formats (e.g., Parquet, …) can be binary. We also encountered HFiles in Chapter 6, which are lists of keyvalue pairs. In fact, Hadoop has another such kind of key-value-based format called Sequence File, which is simply a list of key-values, but not necessarily sorted by key (although ordered) and with keys not necessarily unique.</p>
<h3><span id="shards">Shards</span></h3><p>There are several practical motivations for the many-files pattern even in HDFS.</p>
<h3><span id="querying-pattern">Querying pattern</span></h3><p>This is the motivation behind the standard MapReduce pattern: a map-like phase on the entire input, then a shuffle phase on the intermediate data, then another map-like phase (called reduce) producing the entire output</p>
<h2><span id="mapreduce-model">MapReduce Model</span></h2><p>In MapReduce, the input data, intermediate data, and output data are all made of a large collection of key-value pairs (with the keys not necessarily unique, and not necessarily sorted by key). The types of the keys and values are known at compile-time (statically), and they do not need to be the same across all three collections. </p>
<h2><span id="mapreduce-architecture">MapReduce architecture</span></h2><p>On a cluster, the architecture is centralized, just like for HDFS and HBase. In the original version of MapReduce, the main node is called JobTracker, and the worker nodes are called TaskTrackers.</p>
<p>In fact, the JobTracker typically runs on the same machine as the NameNode (and HMaster) and the TaskTrackers on the same machines as the DataNodes (and RegionServers). This is called “bring the query to the data.”</p>
<p>As the map phase progresses, there is a risk that the memory becomes full. But we have seen this before with HBase: the intermediate pairs on that machine are then sorted by key and flushed to the disk to a Sequence File. And as more flushes happen, these Sequence Files can be compacted to less of them, very similarly to HBase’s Log-Structured Merge Trees. When the map phase is over, each TaskTracker runs an HTTP server listening for connections, so that they can connect to each other and ship the intermediate data over to create the intermediate partitions ensuring that the same keys are on the same machines.This is the phase called shuffling. Then, the reduce phase can start. When the reduce phase is completed, each output partition will be output to a shard (as we saw, a file named incrementally) in the output destination (HDFS, S3, etc) and in the desired format.</p>
<p>Note that shuffling can start before the map phase is over, but the reduce phase can only start after the map phase is over.</p>
<h2><span id="mapreduce-input-and-output-formats">MapReduce input and output formats</span></h2><h3><span id="impedance-mismatch">Impedance mismatch</span></h3><p>MapReduce can read its input from files lying in a data lake as well as directly from a database system such as HBase or a relational database management system. MapReduce only reads and writes lists of key-value pairs, where keys may be duplicates and need not appear in order. However, the inputs we considered are not key-value pairs. So we need an additional mechanism that allows MapReduce to interpret this input as key-value pairs. For tables, whereas relational or in a wide column stores, this is relatively easy: indeed, tables have primary keys, consisting of either a single column or multiple columns. Thus, each tuple can be interpreted as a key-value pair.</p>
<h3><span id="mapping-files-to-pairs">Mapping files to pairs</span></h3><p>How do we read a (possibly huge) text file as a list of key-value pairs? The most natural way to do so is to turn each line of text in a key value pair1: the value is the string corresponding to the entire line, while the key is an integer that expresses the position (as a number of characters), or offset, at which the line starts in the current file being read.</p>
<h2><span id="a-few-examples">A few examples</span></h2><h3><span id="counting-words">Counting words</span></h3><p><img src="/2023/10/31/bigdata6/image-3.png" alt="Alt text"><br><img src="/2023/10/31/bigdata6/image-4.png" alt="Alt text"></p>
<h3><span id="selecting">Selecting</span></h3><p>This is something easily done by having a map function that outputs a subset of its input, based on some predicate provided by the user.<br><img src="/2023/10/31/bigdata6/image-5.png" alt="Alt text"><br>Here we notice that the output of the map phase already gives us the desired result; we still need to provide a reduce function, which is taken trivially as the identity function. This is not unusual (and there are also examples where the map function is trivial, and the reduce function is doing the actual processing).</p>
<h3><span id="projecting">Projecting</span></h3><p>The map function can project this object to an object with less attributes:<br><img src="/2023/10/31/bigdata6/image-6.png" alt="Alt text"></p>
<h3><span id="mapreduce-and-the-relational-algebra">MapReduce and the relational algebra</span></h3><p>As an exercise, try to figure out how to implement a GROUP BY clause and an ORDER BY clause. What about the HAVING clause? Naturally, executing these queries directly in MapReduce is very cumbersome because of the low-level code we need to write. </p>
<h2><span id="combine-functions-and-optimization">Combine functions and optimization</span></h2><p>In addition to the map function and the reduce function, the user can supply a combine function. This combine function can then be called by the system during the map phase as many times as it sees fit to “compress” the intermediate key-value pairs. Strategically, the combine function is likely to be called at every flush of key-value pairs to a Sequence File on disk, and at every compaction of several Sequence Files into one.</p>
<p>However, there is no guarantee that the combine function will be called at all, and there is also no guarantee on how many times it will be called. Thus, if the user provides a combine function, it is important that they think carefully about a combine function that does not affect the correctness of the output data.</p>
<p>In fact, in most of the cases, the combine function will be identical to the reduce function, which is generally possible if the intermediate key-value pairs have the same type as the output key-value pairs, and the reduce function is both associative and commutative.</p>
<h2><span id="mapreduce-programming-api">MapReduce programming API</span></h2><h3><span id="mapper-classes">Mapper classes</span></h3><p>In Java, the user needs to define a so-called Mapper class that contains the map function, and a Reducer class that contains the reduce function. A map function takes in particular a key and a value. Note that it outputs key-value pairs via the call of the write method on the context, rather than with a return statement.</p>
<h3><span id="reducer-classes">Reducer classes</span></h3><p>A reduce function takes in particular a key and a list of values. Note that it outputs key-value pairs via the call of the write method on the context, rather than with a return statement.</p>
<h3><span id="running-the-job">Running the job</span></h3><p>Finally, a MapReduce job can be created and invoked by supplying a Mapper and Reducer instance to the job. A combine function can also be supplied with the setCombinerClass method. It is also possible to use Python rather than Java, via the socalled Streaming API. The Streaming API is the general way to invoke MapReduce jobs in other languages than Java. </p>
<h2><span id="using-correct-terminology">Using correct terminology</span></h2><h3><span id="functions">Functions</span></h3><p>A map function is a mathematical, or programmed, function that takes one input key-value pair and returns zero, one or more intermediate key-value pairs.</p>
<p>A reduce function is a mathematical, or programmed, function that takes one or more intermediate key-value pairs and returns zero, one or more output key-value pairs.</p>
<p>A combine function is a mathematical, or programmed, function that takes one or more intermediate key-value pairs and returns zero, one or more intermediate key-value pairs.</p>
<p>Note that the combine function is an optional local aggregation step that occurs before shuffling and sorting, and its purpose is to reduce the amount of data that needs to be transferred to the reducers. The reduce function, on the other hand, performs the final aggregation and processing based on keys. </p>
<h3><span id="tasks">Tasks</span></h3><p>A map task is an assignment (or “homework”, or “TODO”) that consists in a (sequential) series of calls of the map function on a subset of the input.</p>
<p>A reduce task is an assignment that consists in a (sequential) series of calls of the reduce function on a subset of the intermediate input.</p>
<p>We insist that the calls within a task are sequential, meaning that there is no parallelism at all within a task.</p>
<p>There is no such thing as a combine task. Calls of the combine function are not planned as a task, but is called ad-hoc during flushing and compaction.</p>
<h3><span id="slots">Slots</span></h3><p>The map tasks are processed thanks to compute and memory resources (CPU and RAM). These resources are called map slots. One map slot corresponds to one CPU core and some allocated memory. The number of map slots is limited by the number of available cores. Each map slot then processes one map task at a time, sequentially. The resources used to process reduce tasks are called reduce slots. So, there is no parallelism either within one map slot, or one reduce slot. In fact, parallelism happens across several slots.</p>
<h3><span id="phases">Phases</span></h3><p>The map phase thus consists of several map slots processing map tasks in parallel. </p>
<h3><span id="blocks-vs-splits">blocks vs. splits</span></h3><p>HDFS blocks have a size of (at most) 128 MB. In every file, all blocks but the last one have a size of exactly 128 MB. Splits, however, only contain full records: a key-value pair will only belong to one split (and thus be processed by one map task).</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-10-27T09:00:34.000Z" title="2023-10-27 11:00:34 ├F10: AM┤">2023-10-27</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-02-21T11:20:27.846Z" title="2024-2-21 12:20:27 ├F10: PM┤">2024-02-21</time></span><span class="level-item"><a class="link-muted" href="/categories/theory/">theory</a></span><span class="level-item">3 minutes read (About 426 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/10/27/pai3/">Probabilistic Artificial Intelligence - Markov Chain Monte Carlo Methods</a></p><div class="content"><h1><span id="markov-chain-monte-carlo-methods">Markov Chain Monte Carlo Methods</span></h1><p>The key idea of Markov chain Monte Carlo methods is to construct a Markov chain, which is efficient to simulate and has the stationary distribution p.</p>
<h2><span id="markov-chains">Markov Chains</span></h2><p><img src="/2023/10/27/pai3/image-21.png" alt="Alt text"></p>
<p>Intuitively, the Markov property states that future behavior is independent of past states given the present state.</p>
<p>Markov chains can be classified into different types based on their behavior. These classifications include time-homogeneous and time-inhomogeneous Markov chains, irreducible Markov chains, and periodic and aperiodic Markov chains.</p>
<p>We restrict our attention to time-homogeneous Markov chains. That is, the transition probabilities do not change over time, which can be characterized by a transition function.</p>
<p>Irreducible Markov chains are those in which every state can be reached from any other state, while periodic Markov chains exhibit a repeating pattern in their states. On the other hand, aperiodic Markov chains do not exhibit any regular pattern in their states. If there is no fixed period at which the probabilities return to the starting values, the chain is classified as aperiodic. Aperiodic Markov chains often display a more complex and unpredictable behavior compared to periodic ones.</p>
<h3><span id="stationarity">Stationarity</span></h3><p><img src="/2023/10/27/pai3/image-22.png" alt="Alt text"></p>
<p><img src="/2023/10/27/pai3/image-23.png" alt="Alt text"></p>
<h3><span id="convergence">Convergence</span></h3><p><img src="/2023/10/27/pai3/image-24.png" alt="Alt text"></p>
<p><img src="/2023/10/27/pai3/image-25.png" alt="Alt text"></p>
<p><img src="/2023/10/27/pai3/image-26.png" alt="Alt text"></p>
<p>A Markov Chain is called ergodic, if there exists a finite t such that every state can be reached from every state in exactly t steps.</p>
<h4><span id="fundamental-theorem-of-ergodic-markov-chains">Fundamental theorem of ergodic Markov chains</span></h4><p><img src="/2023/10/27/pai3/image-112.png" alt="Alt text"></p>
<h3><span id="detailed-balance-equation">Detailed Balance Equation</span></h3><p><img src="/2023/10/27/pai3/image-113.png" alt="Alt text"></p>
<h3><span id="ergodic-theorem">Ergodic Theorem</span></h3><p><img src="/2023/10/27/pai3/image-114.png" alt="Alt text"></p>
<p>This result is the fundamental reason for why Markov chain Monte Carlo methods are possible. In practice, one observes that Markov chain Monte Carlo methods have a so-called “burn-in” time during which the distribution of the Markov chain does not yet approximate the posterior distribution well. Typically, the first t0 samples are therefore discarded. It is not clear in general how T and t0 should be chosen such that the estimator is unbiased, rather they have to be tuned. </p>
<h2><span id="elementary-sampling-methods">Elementary Sampling Methods</span></h2><h3><span id="metropolis-hastings-algorithm">Metropolis-Hastings Algorithm</span></h3><p><img src="/2023/10/27/pai3/image-115.png" alt="Alt text"></p>
<h3><span id="gibbs-sampling">Gibbs Sampling</span></h3><p>A popular example of a Metropolis-Hastings algorithm is Gibbs sampling. </p>
<p><img src="/2023/10/27/pai3/image-116.png" alt="Alt text"></p>
<p>Intuitively, by re-sampling single coordinates according to the posterior distribution given the other coordinates, Gibbs sampling finds states that are successively “more” likely.</p>
<h2><span id="langevin-dynamics">Langevin Dynamics</span></h2><h3><span id="gibbs-distributions">Gibbs Distributions</span></h3><h2><span id="references">references</span></h2><p><a target="_blank" rel="noopener" href="https://www.collimator.ai/reference-guides/what-is-a-aperiodic-markov-chain">https://www.collimator.ai/reference-guides/what-is-a-aperiodic-markov-chain</a></p>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/">Previous</a></div><div class="pagination-next"><a href="/page/3/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/">1</a></li><li><a class="pagination-link is-current" href="/page/2/">2</a></li><li><a class="pagination-link" href="/page/3/">3</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/8/">8</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/tx.jpg" alt="s-serenity"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">s-serenity</p><p class="is-size-6 is-block">student</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Shanghai, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">76</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">9</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">21</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/s-serenity" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/s-serenity"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://github.com/s-serenity" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/algorithm/"><span class="level-start"><span class="level-item">algorithm</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/computer-system/"><span class="level-start"><span class="level-item">computer system</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/database/"><span class="level-start"><span class="level-item">database</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/development-tools/"><span class="level-start"><span class="level-item">development tools</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/others/"><span class="level-start"><span class="level-item">others</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/practice/"><span class="level-start"><span class="level-item">practice</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/programming-language/"><span class="level-start"><span class="level-item">programming language</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/theory/"><span class="level-start"><span class="level-item">theory</span></span><span class="level-end"><span class="level-item tag">21</span></span></a></li><li><a class="level is-mobile" href="/categories/web-development/"><span class="level-start"><span class="level-item">web development</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-04-02T11:52:28.000Z">2024-04-02</time></p><p class="title"><a href="/2024/04/02/llm2/">Problems record of using OpenAI&#039;s API</a></p><p class="categories"><a href="/categories/theory/">theory</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-03-29T12:00:15.000Z">2024-03-29</time></p><p class="title"><a href="/2024/03/29/llm1/">Sampling</a></p><p class="categories"><a href="/categories/theory/">theory</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-02-21T11:25:29.000Z">2024-02-21</time></p><p class="title"><a href="/2024/02/21/llm0/">Measuring sentence similarity</a></p><p class="categories"><a href="/categories/theory/">theory</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-01-25T16:14:35.000Z">2024-01-25</time></p><p class="title"><a href="/2024/01/25/pai11/">pai - review notes</a></p><p class="categories"><a href="/categories/theory/">theory</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-01-03T15:06:23.000Z">2024-01-03</time></p><p class="title"><a href="/2024/01/03/bigdata13/">bigdata - review notes</a></p><p class="categories"><a href="/categories/database/">database</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2024/"><span class="level-start"><span class="level-item">2024</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/"><span class="level-start"><span class="level-item">2023</span></span><span class="level-end"><span class="level-item tag">43</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/"><span class="level-start"><span class="level-item">2022</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/"><span class="level-start"><span class="level-item">2021</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/"><span class="level-start"><span class="level-item">2020</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/"><span class="level-start"><span class="level-item">2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/"><span class="level-start"><span class="level-item">2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Algorithm/"><span class="tag">Algorithm</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Elasticsearch/"><span class="tag">Elasticsearch</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux/"><span class="tag">Linux</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Markdown/"><span class="tag">Markdown</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SQL/"><span class="tag">SQL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/algorithm/"><span class="tag">algorithm</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/big-data/"><span class="tag">big data</span><span class="tag">13</span></a></div><div class="control"><a class="tags has-addons" href="/tags/computer-vision/"><span class="tag">computer vision</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/deep-learning/"><span class="tag">deep learning</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/git/"><span class="tag">git</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/graph/"><span class="tag">graph</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/hexo/"><span class="tag">hexo</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/java/"><span class="tag">java</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/machine-learning/"><span class="tag">machine learning</span><span class="tag">18</span></a></div><div class="control"><a class="tags has-addons" href="/tags/probability/"><span class="tag">probability</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python/"><span class="tag">python</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/reinforcement-learning/"><span class="tag">reinforcement learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/speech/"><span class="tag">speech</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/tools/"><span class="tag">tools</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="s-serenity" height="28"></a><p class="is-size-7"><span>&copy; 2024 s-serenity</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2019</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>