<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>s-serenity</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="s-serenity"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="s-serenity"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Recording and sharing my learning process."><meta property="og:type" content="blog"><meta property="og:title" content="s-serenity"><meta property="og:url" content="http://yoursite.com/"><meta property="og:site_name" content="s-serenity"><meta property="og:description" content="Recording and sharing my learning process."><meta property="og:locale" content="en_US"><meta property="og:image" content="http://yoursite.com/img/og_image.png"><meta property="article:author" content="s-serenity"><meta property="article:tag" content="Algorithm study development"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://yoursite.com/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://yoursite.com"},"headline":"s-serenity","image":["http://yoursite.com/img/og_image.png"],"author":{"@type":"Person","name":"s-serenity"},"publisher":{"@type":"Organization","name":"s-serenity","logo":{"@type":"ImageObject","url":"http://yoursite.com/img/logo.svg"}},"description":"Recording and sharing my learning process."}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.1.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="s-serenity" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-05-05T09:19:36.000Z" title="2023-5-5 11:19:36 ├F10: AM┤">2023-05-05</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-07-28T13:56:15.031Z" title="2023-7-28 3:56:15 ├F10: PM┤">2023-07-28</time></span><span class="level-item">34 minutes read (About 5120 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/05/05/object-detection/">object detection</a></p><div class="content"><h1><span id="object-detection">Object detection</span></h1><p>Object detection is the field of computer vision that deals with the localization and classification of objects contained in an image or video.<br>Deep learning-based approaches use neural network architectures like RetinaNet, YOLO (You Only Look Once), CenterNet, SSD (Single Shot Multibox detector), Region proposals (R-CNN, Fast-RCNN, Faster RCNN, Cascade R-CNN) for feature detection of the object, and then identification into labels.The YOLO series current provide the SOTA of object detection in real-time.</p>
<p>Object detection usually consists of the following parts:<br>Input: Refers to the input of the picture<br>Backbone: A skeleton pre-trained on ImageNet<br>Neck: Usually used to extract feature maps of different levels<br>Head: Predict the object category and the detector of bndBox, usually divided into two types: Dense Prediction (one stage), Sparse Prediction (two stage).</p>
<h2><span id="metric">metric</span></h2><h3><span id="map">mAP</span></h3><p>Mean average precision (mAP) is the average value of AP of each category.The AP metric is the area under curve (AUC) of PR curve (Precision-Recall curve).This metric provides a balanced assessment of precision and recall by considering the area under the precision-recall curve. PR curve is a curve drawn with Recall as the X axis and Precision as the Y axis. The higher the Precision and Recall, the better the performance of the model, so the closer to the upper right corner, the better. The AP metric incorporates the Intersection over Union (IoU) measure to assess the quality of the predicted bounding boxes. If the IOU is greater than the threshold (Threshold, usually set to 0.5), and the same Ground Truth can only be calculated once, it will be considered as a TP.</p>
<h3><span id="intersection-over-unioniou">Intersection over Union(IoU)</span></h3><p>IoU is the ratio of the intersection area to the union area of the predicted bounding box and the ground truth bounding box. It measures the overlap between the ground truth and predicted bounding boxes.</p>
<h3><span id="flops-and-fps">Flops and FPS</span></h3><p>FLOPS (Floating-Point Operations Per Second)  is a measure of a computer’s or a processor’s performance in terms of the number of floating-point operations it can perform per second.Higher FLOPS values generally indicate faster computational capabilities.FPS (Frames Per Second) is a measure of how many individual frames (images) a video system can display or process per second.</p>
<h2><span id="non-maximum-suppression-nms">Non-Maximum Suppression (NMS)</span></h2><p>Non-Maximum Suppression (NMS) is a post-processing technique used in object detection algorithms to reduce the number of overlapping bounding boxes and improve the overall detection quality.</p>
<h2><span id="model-history">Model History</span></h2><p>Traditionally, object detection is done by Viola Jones Detector \cite{viola2001rapid}, Histogram of Oriented Gradients (HOG) detector, or Deformable Part-based Model (DPM) before deep learning took off. With deep learning, object detection generally is categorized into 2 categories: one-stage detector and two-stage detector. Two-stage detector is started by Regions with CNN features (RCNN). Spatial Pyramid Pooling Networks (SPPNet), Fast RCNN, Faster RCNN, and Feature Pyramid Networks (FPN) were proposed after it. Limited by the poor speed of the two-stage detector, the one-stage detector came with the first representative You Only Look Once (YOLO). Subsequent versions of YOLO, Single Shot MultiBox Detector (SSD), RetinaNet, CornerNet, CenterNet,DETR were proposed latter. YOLOv7 performs best compared to most detectors.  </p>
<h3><span id="rcnn">RCNN</span></h3><p>The object detection system consists of three modules. The first generates category-independent region proposals. These proposals define the set of candidate detections available to our detector. The second module is a large convolutional neural network that extracts a fixed-length feature vector from each region. The third module is a set of classspecific linear SVMs.</p>
<h3><span id="yolo-series">YOLO series</span></h3><p>The history of YOLO (You Only Look Once) dates back to 2015 when the original YOLO algorithm was introduced in “You Only Look Once: Unified, Real-Time Object Detection,” .The original YOLO architecture used a convolutional neural network (CNN) to process the entire image and output a fixed number of bounding boxes along with their associated class probabilities. It divided the image into a grid and applied convolutional operations to predict bounding boxes within each grid cell, considering multiple scales and aspect ratios.In subsequent years, YOLO underwent several iterations and improvements to enhance its accuracy and speed. YOLOv2 was introduced in 2016, featuring an updated architecture that incorporated anchor boxes and multi-scale predictions. YOLOv3 followed in 2018, introducing further advancements, including feature pyramid networks (FPN) and Darknet-53 as the backbone architecture.</p>
<h4><span id="yolo-you-only-look-once">YOLO (You Only Look Once)</span></h4><p>Network architecture is inspired by the GoogLeNet model for image classification.The network has 24 convolutional layers followed by 2 fully connected layers. They pretrain our convolutional layers on the ImageNet 1000-class competition dataset.For pretraining they use the first 20 convolutional layers followed by a average-pooling layer and a fully connected layer. Then they add four convolutional layers and two fully connected layers with randomly initialized weights. The final layer predicts both class probabilities and bounding box coordinates. They optimize for sum-squared error in the output of the model by increasing the loss from bounding box coordinate predictions and decreasing the loss from confidence predictions for boxes that don’t contain objects and predicting the square root of the bounding box width and height instead of the width and height directly. They design the loss to handle the problem that the sum-squared error weights localization error equally with classification error and also equally weights errors in large boxes and small boxes.</p>
<h4><span id="yolov2">YOLOv2</span></h4><p>The improvements of YOLOv2 in YOLOv1:<br>The author adds a batch normalization layer after each convolutional layer, no longer uses dropout.<br>YOLOv1 uses a 224x224 image classifier. YOLO2 increases the resolution to 448x448.<br>Because YOLOv1 has difficulty learning to adapt to the shape of different objects during training, resulting in poor performance in precise positioning. YOLOv2 also tries to use rectangles of different shapes as anchor boxes (Anchor Box).Unlike YOLOv1, Anchor Box does not directly predict the coordinate value of bndBox, but predicts the offset (offset value of coordinates) and confidence scores (confidence) of Anchor Box.<br>In Faster R-CNN and SSD, the size of the Anchor Box is manually selected.YOLOv2 uses the k-means clustering method to perform cluster analysis on the bndBox of the objects in the training set.<br>YOLOv2 uses a new basic model (feature extractor) Darknet-19, including 19 convolutional layers, 5 maxpooling layers.</p>
<h4><span id="yolo9000">YOLO9000</span></h4><p>YOLO9000 is a model that can detect more than 9,000 categories proposed on the basis of YOLOv2. Its main contribution is to propose a joint training strategy for classification and detection.For the detection data set, it is used to learn the bounding box (bndBox), confidence (confidence) and object classification of the predicted object, while for the classification data set, it is only used to learn classification, but it can greatly expand the capabilities of the model the type of object detected.</p>
<p>The author proposes a hierarchical classification method (Hierarchical classification),which establishes a tree structure WordTree according to the affiliation between categories.When softmax is performed, it is not performed on all categories, but on the categories of the same level.When making predictions, it traverses down from the root node, selects the child node with the highest probability at each level, and calculates the product of all conditional probabilities from the node to the root node. Stop when the product of the conditional probability is less than a certain threshold, and use the current node to represent the predicted category.</p>
<h4><span id="yolov3">YOLOv3</span></h4><p>On the basis of YOLOv2, YOLOv3 improves the network backbone, uses multi-scale feature maps (feature maps) for detection, and uses multiple independent Logistic regression classifiers instead of softmax to predict category classification.YOLOv3 proposes a new backbone: Darknet-53, from layer 0 to layer 74, a total of 53 convolutional layers, and the rest are Resnet layers.Darknet-53 joins Resnet Network (Residual Network) to solve the gradient problem.<br>YOLOv3 draws on the Feature Pyramid Network (FPN) method, uses multi-scale feature maps to detect objects of different sizes, and improves the prediction ability of small objects.The feature map of each scale will predict 3 Anchor priors, and the size of the Anchor priors is clustered using K-means.</p>
<h5><span id="feature-pyramid-networks-fpn">Feature Pyramid Networks (FPN)</span></h5><p>The main idea behind FPNs is to leverage the nature of convolutional layers — which reduce the size of the feature space and increase the coverage of each feature in the initial image — to output predictions at different scales.FPNs provide semantically strong features at multiple scales which make them extremely well suited for object detection.</p>
<h4><span id="yolov4">YOLOv4</span></h4><p>Bag-of-Freebies refers to the techniques used in network training, which does not affect the time of reasoning and prediction, mainly including:<br>Data augmentation: Random erase, CutOut, Hide-and-seek, Grid mask, GAN, MixUp, CutMix;Regularization methods: DropOut, DropConnect;Dealing with data imbalance: focal loss, Online hard example mining, Hard negative example mining;Handle bndBox regression problems: MSE, IOU, GIOU, DIOU/CIOU.</p>
<p>Bag-of-specials refers to the techniques used in network design or post-processing, which slightly increases the time of reasoning and prediction, but can improve the accuracy, mainly including:Receptive field: SPP, ASPP, RFB;Feature Fusion: FPN, PAN;Attention mechanism: attention module;<br>Activation functions: Swish, Mish;NMS: Soft-NMS、DIoU NMS.</p>
<p>The architecture of the YOLOv4 model consists of three parts<br>BackBone: CSPDarknet53; Neck: SPP+PAN; HEAD: YOLO HEAD.</p>
<h5><span id="cross-stage-partial-network-cspnet">Cross Stage Partial Network (CSPNet)</span></h5><p>The main purpose of CSPNet is to enable the network architecture to obtain richer gradient fusion information and reduce the amount of calculation.<br>The method is to first divide the feature map of the Base layer into two parts, and then pass through transition -&gt; concatenation -&gt; transition. parts merged.This approach allows CSPNet to solve three problems:<br>Increase the learning ability of CNN, even if the model is lightweight, it can maintain accuracy;<br>Remove the computing bottleneck structure with high computing power (reduce computing);Reduce memory usage.</p>
<h5><span id="spppan">SPP+PAN</span></h5><p>SPP (Spatial Pyramid Pooling): Concate all feature maps in the last layer of the network, and then continue to connect CNN module.<br>PANet (Path Aggregation Network): Improve on the basis of FPN.</p>
<h5><span id="cutmix">CutMix</span></h5><p>CutMix is ​​a data enhancement method proposed in 2019. The method is to cut off a part of the area but not fill it with 0 pixels, but randomly fill the area pixel values ​​​​of other data in the training set.Mixup: Mix two random samples proportionally, and the classification results are distributed proportionally.utout: Randomly cut out some areas in the sample and fill them with 0 pixel values, and the classification result remains unchanged.</p>
<h5><span id="mosaic-data-augmentation">Mosaic data augmentation</span></h5><p>Whilst common transforms in object detection tend to be augmentations such as flips and rotations, the YOLO authors take a slightly different approach by applying Mosaic augmentation; which was previously used by YOLOv4, YOLOv5 and YOLOX models.The objective of mosaic augmentation is to overcome the observation that object detection models tend to focus on detecting items towards the centre of the image. The key idea is that, if we stitch multiple images together, the objects are likely to be in positions and contexts that are not normally observed in images seen in the dataset; which should force the features learned by the model to be more position invariant. It uses random scaling and cropping to mix and stitch 4 kinds of pictures for training. When using Mosaic training, the data of 4 pictures can be directly calculated, so that the size of the Mini-batch does not need to be large.</p>
<h5><span id="post-mosaic-affine-transforms">Post-mosaic affine transforms</span></h5><p>As we noted earlier, the mosaics that we are creating are significantly bigger than the image sizes we will use to train our model, so we will need to do some sort of resizing here. Whilst this would work, this is likely to result in some very small objects, as we are essentially resizing four images to the size of one - which is likely to become a problem where the domain already contains very small bounding boxes. Additionally, each of our mosaics are structurally quite similar, with an image in each quadrant. Recalling that our aim was to make the model more robust to position changes, this may not actually help that much; as the model is likely just to start looking in the middle of each quadrant.To overcome this, one approach that we can take is to simply take a random crop from our mosaic. This will still provide the variability in positioning whilst preserving the size and aspect ratio of the target objects. At this point, it may also be a good opportunity to add in some other transforms such as scaling and rotation to add even more variability.</p>
<h5><span id="dropblock-regularization">DropBlock regularization</span></h5><p>Dropout, which randomly deletes the number of neurons, but the network can still learn the same information from adjacent activation units.<br>DropBlock randomly deletes the entire local area, and the network will focus on learning certain features to achieve correct classification and get better generalization effects.</p>
<h5><span id="class-label-smoothing">Class label smoothing</span></h5><p>In multi-classification tasks, the output is usually normalized with softmax, and then one-hot label is used to calculate the cross-entropy loss function to train the model. However, the use of one-hot vector representation can easily lead to the problem of network overfitting, so Label Smoothing is to make the one-hot label softer, so that the phenomenon of overfitting can be effectively suppressed when calculating the loss, and the generalization ability of the model can be improved.</p>
<h5><span id="mish-activation">Mish activation</span></h5><p>Mish is a continuously differentiable non-monotonic activation function. Compared with ReLU, Mish’s gradient is smoother, and it allows a smaller negative gradient when it is negative, which can stabilize the network gradient flow and has better generalization ability.<br>$f(x) = xtanh(ln(1+e^x))$.</p>
<h5><span id="multiinput-weighted-residual-connections-miwrc">Multiinput weighted residual connections (MiWRC)</span></h5><p>YOLOv4 refers to the architecture and method of EfficientDet , and uses the multi-input weighted residual connection (MiWRC).The backbone of EfficientDet uses EfficientNet, Neck is BiFPN.EfficientNet-B0 is constructed by multiple MBConv Blocks. MBConv Block refers to the Inverted Residual Block of MobileNet V2.The design of MBConv is to first increase the dimension and then reduce the dimension, which is different from the operation of the residual block to first reduce the dimension and then increase the dimension. This design allows MobileNetV2 to better use the residual connection to improve Accuracy.The idea of ​​MiWRC is derived from BiFPN. In FPN, the features obtained by each layer are regarded as equal, while MiWRC believes that the features of different layers should have different importance, and different weight ratios should be given to the features of different scales.</p>
<h5><span id="loss">loss</span></h5><p>2 problems with using IOU loss:When the predict box (predict bndBox) and the target box (ground truth) do not intersect, the IOU is 0, which cannot reflect the distance between the two boxes. At this time, the loss function is not derivable, that is to say, the gradient cannot be calculated, so it cannot Optimizing the case where two boxes do not intersect;The IOU cannot reflect the coincidence size of the prediction frame and the target frame.<br>Subsequent GIoU, DIoU, CIoU are based on IOU loss to add a penalty item:<br> GIOU loss (Generalized IOU loss):C is the minimum bounding box of the target box Ground Truth and the prediction box Predict.<br> $L_{GIOU}=1-IOU+\frac{|C-B\cupB^{gt}|}{|C|}$.<br> DIOU loss (Distance IOU loss) considers the overlapping area and the center point distance, and adds a penalty term to minimize the center point distance between the two boxes.CIOU loss (Complete IOU loss) adds a penalty item based on DIOU, taking into account the factor of aspect ratio.</p>
<h5><span id="cmbn-cross-mini-batch-normalization">CmBN (Cross mini-Batch Normalization)</span></h5><p> BN is to normalize the current mini-batch, but often the batch size is very small, and uneven sampling may occur, which may cause problems in normalization. Therefore, there are many Batch Normalization methods for small batch sizes.The idea of ​​CBN is to calculate the previous mini-batch together, but not keep too many mini-batches. The method is to normalize the results of the current and the current 3 mini-batches.The CmBN newly created by YOLOv4 is based on CBN for modification, and does not update calculations between mini-batches, but updates network parameters after a batch is completed.</p>
<h5><span id="self-adversarial-training-sat">Self-Adversarial Training (SAT)</span></h5><p> SAT is a data enhancement method innovated by the author, which is completed in two stages:First, forward-propagate the training samples, and then modify the image pixels (without modifying the network weights) during back-propagation to reduce the performance of model detection. In this way, the neural network can perform adversarial attacks on itself. Creates the illusion that there is no detected object in the picture. This first stage is actually increasing the difficulty of training samples.The second stage is to use the modified pictures to train the model.</p>
<h5><span id="eliminate-grid-sensitivity">Eliminate grid sensitivity</span></h5><p> The author observed a video of object detection and found that because the center point of the detected object is mostly located close to the center of the Grid, it is difficult to detect when it is on the edge of the Grid. The author believes that the problem that the center point of the detected object is mostly located close to the center point of the Grid is because of the gradient of the Sigmoid function. Therefore, the author made some changes in the Sigmoid function, multiplying Sigmoid by a value greater than 1, and taking into account the sensitivity of different Grid sizes to boundary effects, using (1+x)<em>Sigmoid — (0.5</em>x), where When the Grid resolution is higher, the x will be higher.</p>
<h5><span id="cosine-annealing-scheduler">Cosine annealing scheduler</span></h5><p> Cosine annealing is to use the cosine function to adjust the learning rate. At the beginning, the learning rate will be slowly reduced, then accelerated halfway, and finally slowed down again.</p>
<h5><span id="optimal-hyperparameters">Optimal hyperparameters</span></h5><p> Use Genetic Algorithms (Evolutionary Algorithms) to select hyperparameters. The method is to randomly combine hyperparameters for training, then select the best 10% hyperparameters and then randomly combine and train them, and finally select the best model.</p>
<h5><span id="sam-block-spatial-attention-module">SAM-block (Spatial Attention Module)</span></h5><p> SAM is derived from the CBAM (Convolutional Block Attention Module) paper, which provides two attention mechanism techniques.</p>
<h5><span id="diou-nms">DIoU-NMS</span></h5><p> In the classic NMS, the detection frame with the highest confidence and other detection frames will calculate the corresponding IOU value one by one, and the detection frame whose value exceeds the threshold is filtered out. But in the actual situation, when two different objects are very close, due to the relatively large IOU value, after the NMS algorithm, there is often only one detection frame left, which may cause missed detection.DIoU-NMS considers not only the IOU value, but also the distance between the center points of two boxes. If the IOU between the two frames is relatively large, but the distance between them is relatively far, it will be considered as the detection frame of different objects and will not be filtered out.</p>
<h4><span id="yolov7">YOLOv7</span></h4><h5><span id="anchor-boxes">Anchor boxes</span></h5><p>YOLOv7 family is an anchor-based model.In these models, the general philosophy is to first create lots of potential bounding boxes, then select the most promising options to match to our target objects; slightly moving and resizing them as necessary to obtain the best possible fit.The basic idea is that we draw a grid on top of each image and, at each grid intersection (anchor point), generate candidate boxes (anchor boxes) based on a number of anchor sizes. That is, the same set of boxes is repeated at each anchor point. However, one issue with this approach is that our target, ground truth, boxes can range in size — from tiny to huge! Therefore, it is usually not possible to define a single set of anchor sizes that can be matched to all targets. For this reason, anchor-based model architectures usually employ a Feature-Pyramid-Network (FPN) to assist with this.</p>
<h5><span id="center-priors">Center Priors</span></h5><p>If we put 3 anchor boxes in each anchor point of each of the grids, we end up with a lot of boxes.The issue is that most of these predictions are not going to contain an object, which we classify as ‘background’.To make the problem cheaper computationally, the YOLOv7 loss finds first the anchor boxes that are likely to match each target box and treats them differently — these are known as the center prior anchor boxes. This process is applied at each FPN head, for each target box, across all images in batch at once.</p>
<h5><span id="model-reparameterization">model reparameterization</span></h5><p>Model re-parametrization techniques merge multiple computational modules into one at inference stage. The model re-parameterization technique can be regarded as an ensemble technique, and we can divide it into two categories, i.e., module-level ensemble and model-level ensemble.</p>
<h5><span id="model-scaling">Model scaling</span></h5><p>Model scaling is a way to scale up or down an already designed model and make it fit in different computing devices.Network architecture search (NAS) is one of the commonly used model scaling methods.</p>
<h5><span id="efficient-layer-aggregation-networkselan">efficient layer aggregation networks(ELAN)</span></h5><h6><span id="vovnetosanet">VovNet/OSANet</span></h6><p>VovNet, short for “Variance-based Overparameterized Convolutional Networks,” is a convolutional neural network (CNN) architecture proposed by Lee et al. in their paper “Variance-based Overparameterization for Robustness” in 2019. VovNet is designed to improve the robustness of deep neural networks, particularly in the context of image classification tasks.The key idea behind VovNet is to introduce variance-based overparameterization to enhance the representation power of CNNs. Overparameterization involves increasing the number of parameters in a neural network, which can improve the model’s ability to learn complex patterns and features.<br>VovNet achieves variance-based overparameterization by introducing multiple “VovNet blocks.” Each VovNet block is designed to capture different levels of granularity within the input data. Instead of using a single set of convolutional filters for all spatial dimensions, VovNet employs different filters for each spatial dimension. This allows the network to capture variations in features at different scales, leading to more robust representations.</p>
<p>One-shot aggregation (OSA) module is designed which is more efficient than Dense Block in DenseNet.By cascading OSA module, an efficient object detection network VoVNet is formed.One-shot aggregation (OSA) module is designed to aggregate its feature in the last layer at once.It has much less Memory access cost (MAC)  than that with dense block.Also, OSA improves GPU computation efficiency. The input sizes of intermediate layers of OSA module are constant. Hence, it is unnecessary to adopt additional 1×1 conv bottleneck to reduce dimension. The means it consists of fewer layers.</p>
<h6><span id="cspvovnet">CSPVOVNet</span></h6><p>It combines CSPNet and VoVNet and considers the gradient path for improvement, so that the weights of different layers can learn more diverse features to improve accuracy.</p>
<h5><span id="deep-supervision">Deep supervision</span></h5><p>When training deep networks, auxiliary head and auxiliary classifiers are often added to the middle layer of the neural network to improve stability, convergence speed, and avoid gradient disappearance problems, that is, to use auxiliary loss for shallow layers. Network weights for training, this technique is called Deep Supervision.</p>
<h5><span id="dynamic-label-assignment">dynamic label assignment</span></h5><p>Label Assigner is a mechanism that considers the network prediction results together with the ground truth and then assigns soft labels. In the past, the definition of the target label was usually to use a hard label that follows the ground truth. In recent years, it has also been used to perform some optimization operations on the prediction results of the model and the ground truth to obtain a soft label. This mechanism is called label assigner in this paper.The author discusses three methods of assigning soft labels on the auxiliary head and lead head: Independent, Lead head guided label assigner,Coarse-to-fine lead head guided label assigner.Independent:Auxiliary head and lead head perform label assignment with ground truth respectively, which is the most used method at present.Lead head guided label assigner:Since the lead head has a stronger learning ability than the auxiliary head, the soft label obtained by optimizing the prediction result of the lead head and the ground truth can better express the distribution and correlation between the data and the ground truth.Then use the soft label as the target of the auxiliary head and lead head for training, so that the shallower auxiliary head can directly learn the information that the lead head has learned, while the lead head pays more attention to the unlearned residual information. Coarse-to-fine lead head guided label assigner:This part is also the soft label obtained by optimizing the prediction result of the lead head and the ground truth. The difference is that two different soft labels will be generated: coarse label and fine label, where the fine label is the same as the soft label of the lead head , coarse label is used for auxiliary head.</p>
<h6><span id="optimal-transport-assignment">Optimal Transport Assignment</span></h6><p>The simplest approach is to define an Intersection over Union (IoU) threshold and decide based on that. While this generally works, it becomes problematic when there are occlusions, ambiguity or when multiple objects are very close together. Optimal Transport Assignment (OTA) aims to solve some of these problems by considering label assignment as a global optimization problem for each image.YOLOv7 implements simOTA (introduced in the YOLOX paper), a simplified version of the OTA problem. </p>
<h5><span id="model-ema">Model EMA</span></h5><p>When training a model, it can be beneficial to set the values for the model weights by taking a moving average of the parameters that were observed across the entire training run, as opposed to using the parameters obtained after the last incremental update. This is often done by maintaining an exponentially weighted average (EMA) of the model parameters, in practice, this usually means maintaining another copy of the model to store these averaged weights. This technique has been employed in several training schemes for popular models such as training MNASNet, MobileNet-V3 and EfficientNet.</p>
<p>The approach to EMA taken by the YOLOv7 authors is slightly different to other implementations as, instead of using a fixed decay, the amount of decay changes based on the number of updates that have been made.</p>
<h5><span id="loss-algorithm">Loss algorithm</span></h5><p>we can break down the algorithm used in the YOLOv7 loss calculation into the following steps:</p>
<ol>
<li>For each FPN head (or each FPN head and Aux FPN head pair if Aux heads used):<br>Find the Center Prior anchor boxes.<br>Refine the candidate selection through the simOTA algorithm. Always use lead FPN heads for this.<br>Obtain the objectness loss score using Binary Cross Entropy Loss between the predicted objectness probability and the Complete Intersection over Union (CIoU) with the matched target as ground truth. If there are no matches, this is 0.<br>If there are any selected anchor box candidates, also calculate (otherwise they are just 0):</li>
</ol>
<ul>
<li>The box (or regression) loss, defined as the mean(1 - CIoU) between all candidate anchor boxes and their matched target.</li>
<li>The classification loss, using Binary Cross Entropy Loss between the predicted class probabilities for each anchor box and a one-hot encoded vector of the true class of the matched target.<br>If model uses auxiliary heads, add each component obtained from the aux head to the corresponding main loss component (i.e., x = x + aux_wt*aux_x). The contribution weight (aux_wt) is defined by a predefined hyperparameter.<br>Multiply the objectness loss by the corresponding FPN head weight (predefined hyperparameter).</li>
</ul>
<ol>
<li><p>Multiply each loss component (objectness, classification, regression) by their contribution weight (predefined hyperparameter).</p>
</li>
<li><p>Sum the already weighted loss components.</p>
</li>
<li><p>Multiply the final loss value by the batch size.</p>
</li>
</ol>
<h3><span id="using-yolov7">using yolov7</span></h3><p>github address: <a target="_blank" rel="noopener" href="https://github.com/WongKinYiu/yolov7">https://github.com/WongKinYiu/yolov7</a>;<br>Format converter:<a target="_blank" rel="noopener" href="https://github.com/wy17646051/UA-DETRAC-Format-Converter">https://github.com/wy17646051/UA-DETRAC-Format-Converter</a></p>
<h2><span id="potential-ideas">potential ideas</span></h2><h3><span id="efficiency">efficiency</span></h3><p>In order to enhance the real-time detection of the network, researchers generally analyze the number of parameters, calculation amount and calculation density from the aspects of model parameters, calculation amount, memory access times, input-output channel ratio, element-wise operation, etc. In fact, these research methods are similar to ShuffleNetV2 at that time.</p>
<h3><span id="nasneural-architecture-search">NAS(Neural Architecture Search)</span></h3><p>NAS was an inspiring work out of Google that lead to several follow up works such as ENAS, PNAS, and DARTS. It involves training a recurrent neural network (RNN) controller using reinforcement learning (RL) to automatically generate architectures.</p>
<h3><span id="vision-transformer">Vision Transformer</span></h3><p>The core conclusion in the original ViT paper is that when there is enough data for pre-training, ViT’s performance will exceed CNN, breaking through the limitation of transformer lack of inductive bias, you can use it in Better transfer results in downstream tasks. However, when the training data set is not large enough, the performance of ViT is usually worse than that of ResNets of the same size, because Transformer lacks inductive bias compared with CNN, that is, a priori knowledge, a good assumption in advance.</p>
<h3><span id="improve-choosing-anchor-box">improve choosing anchor box</span></h3><h2><span id="datasets">datasets</span></h2><p>PASCAL VOC 2007, VOC 2012, Microsoft COCO (Common Objects in Context).<br>UA-DETRAC: <a target="_blank" rel="noopener" href="https://detrac-db.rit.albany.edu/">https://detrac-db.rit.albany.edu/</a>  <a target="_blank" rel="noopener" href="https://www.kaggle.com/datasets/patrikskalos/ua-detrac-fix-masks-two-wheelers?resource=download">https://www.kaggle.com/datasets/patrikskalos/ua-detrac-fix-masks-two-wheelers?resource=download</a> <a target="_blank" rel="noopener" href="https://colab.research.google.com/github/hardik0/Multi-Object-Tracking-Google-Colab/blob/main/Towards-Realtime-MOT-Vehicle-">https://colab.research.google.com/github/hardik0/Multi-Object-Tracking-Google-Colab/blob/main/Towards-Realtime-MOT-Vehicle-</a><br><a target="_blank" rel="noopener" href="https://github.com/hardik0/Towards-Realtime-MOT/tree/master">https://github.com/hardik0/Towards-Realtime-MOT/tree/master</a><br>Tracking.ipynb#scrollTo=y6KZeLt9ViDe<br><a target="_blank" rel="noopener" href="https://github.com/wy17646051/UA-DETRAC-Format-Converter/tree/main">https://github.com/wy17646051/UA-DETRAC-Format-Converter/tree/main</a><br>MIO-TCD:<a target="_blank" rel="noopener" href="https://tcd.miovision.com/">https://tcd.miovision.com/</a><br>KITTI:<a target="_blank" rel="noopener" href="https://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark">https://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark</a><br>TRANCOS: <a target="_blank" rel="noopener" href="https://gram.web.uah.es/data/datasets/trancos/index.html">https://gram.web.uah.es/data/datasets/trancos/index.html</a><br>STREETS:<a target="_blank" rel="noopener" href="https://www.kaggle.com/datasets/ryankraus/traffic-camera-object-detection">https://www.kaggle.com/datasets/ryankraus/traffic-camera-object-detection</a>: single class<br>VERI-Wild: <a target="_blank" rel="noopener" href="https://github.com/PKU-IMRE/VERI-Wild">https://github.com/PKU-IMRE/VERI-Wild</a></p>
<p><a target="_blank" rel="noopener" href="https://universe.roboflow.com/7-class/11-11-2021-09.41">https://universe.roboflow.com/7-class/11-11-2021-09.41</a><br><a target="_blank" rel="noopener" href="https://universe.roboflow.com/szabo/densitytrafficcontroller-1axlm">https://universe.roboflow.com/szabo/densitytrafficcontroller-1axlm</a><br><a target="_blank" rel="noopener" href="https://universe.roboflow.com/future-institute-of-technology-1wuwl/indian-vehicle-set-1">https://universe.roboflow.com/future-institute-of-technology-1wuwl/indian-vehicle-set-1</a><br><a target="_blank" rel="noopener" href="https://universe.roboflow.com/cv-2022-kyjj6/tesi">https://universe.roboflow.com/cv-2022-kyjj6/tesi</a><br><a target="_blank" rel="noopener" href="https://universe.roboflow.com/vehicleclassification-kxtkb/vehicle_classification-fvssn">https://universe.roboflow.com/vehicleclassification-kxtkb/vehicle_classification-fvssn</a><br><a target="_blank" rel="noopener" href="https://universe.roboflow.com/urban-data/urban-data">https://universe.roboflow.com/urban-data/urban-data</a><br><a target="_blank" rel="noopener" href="https://www.kaggle.com/datasets/ashfakyeafi/road-vehicle-images-dataset">https://www.kaggle.com/datasets/ashfakyeafi/road-vehicle-images-dataset</a><br><a target="_blank" rel="noopener" href="https://github.com/MaryamBoneh/Vehicle-Detection">https://github.com/MaryamBoneh/Vehicle-Detection</a></p>
<h2><span id="references">References</span></h2><p><a target="_blank" rel="noopener" href="https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173">https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173</a><br><a target="_blank" rel="noopener" href="https://medium.com/ching-i/yolo%E6%BC%94%E9%80%B2-1-33220ebc1d09">https://medium.com/ching-i/yolo%E6%BC%94%E9%80%B2-1-33220ebc1d09</a><br><a target="_blank" rel="noopener" href="https://medium.com/ching-i/yolo%E6%BC%94%E9%80%B2-2-85ee99d114a1">https://medium.com/ching-i/yolo%E6%BC%94%E9%80%B2-2-85ee99d114a1</a><br><a target="_blank" rel="noopener" href="https://medium.com/@chingi071/yolo%E6%BC%94%E9%80%B2-3-yolov4%E8%A9%B3%E7%B4%B0%E4%BB%8B%E7%B4%B9-5ab2490754ef">https://medium.com/@chingi071/yolo%E6%BC%94%E9%80%B2-3-yolov4%E8%A9%B3%E7%B4%B0%E4%BB%8B%E7%B4%B9-5ab2490754ef</a><br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/183261974">https://zhuanlan.zhihu.com/p/183261974</a><br><a target="_blank" rel="noopener" href="https://sh-tsang.medium.com/review-vovnet-osanet-an-energy-and-gpu-computation-efficient-backbone-network-for-real-time-3b26cd035887">https://sh-tsang.medium.com/review-vovnet-osanet-an-energy-and-gpu-computation-efficient-backbone-network-for-real-time-3b26cd035887</a><br><a target="_blank" rel="noopener" href="https://medium.com/ching-i/yolo%E6%BC%94%E9%80%B2-yolov7-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80-97b0e914bdbe">https://medium.com/ching-i/yolo%E6%BC%94%E9%80%B2-yolov7-%E8%AB%96%E6%96%87%E9%96%B1%E8%AE%80-97b0e914bdbe</a><br><a target="_blank" rel="noopener" href="https://towardsdatascience.com/yolov7-a-deep-dive-into-the-current-state-of-the-art-for-object-detection-ce3ffedeeaeb">https://towardsdatascience.com/yolov7-a-deep-dive-into-the-current-state-of-the-art-for-object-detection-ce3ffedeeaeb</a><br><a target="_blank" rel="noopener" href="https://towardsdatascience.com/neural-architecture-search-limitations-and-extensions-8141bec7681f">https://towardsdatascience.com/neural-architecture-search-limitations-and-extensions-8141bec7681f</a><br><a target="_blank" rel="noopener" href="https://learnopencv.com/fine-tuning-yolov7-on-custom-dataset/#The-Training-Experiments-that-We-Will-Carry-Out">https://learnopencv.com/fine-tuning-yolov7-on-custom-dataset/#The-Training-Experiments-that-We-Will-Carry-Out</a><br><a target="_blank" rel="noopener" href="https://learnopencv.com/yolov7-object-detection-paper-explanation-and-inference/">https://learnopencv.com/yolov7-object-detection-paper-explanation-and-inference/</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-05-05T09:15:45.000Z" title="2023-5-5 11:15:45 ├F10: AM┤">2023-05-05</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-05-23T13:59:20.937Z" title="2023-5-23 3:59:20 ├F10: PM┤">2023-05-23</time></span><span class="level-item">a minute read (About 154 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/05/05/speech-embedding/">speech embedding</a></p><div class="content"><h2><span id="contrastive-predictive-coding">Contrastive Predictive Coding</span></h2><p>Contrastive Predictive Coding (CPC) learns self-supervised representations by predicting the future in latent space by using powerful autoregressive models. The model uses a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It describes a form of unidirectional modeling in the feature space,<br>where the model learns to predict the near future frames in<br>an acoustic sequence while contrasting with frames from other<br>sequences or frames from a more distant time.</p>
<h2><span id="autoregressive-predictive-coding">Autoregressive Predictive Coding</span></h2><p>The APC approach uses an autoregressive model to encode<br>temporal information of past acoustic sequence; the model then<br>predicts future frames like a recurrent-based LM while<br>conditioning on past frames.</p>
<h2><span id="tera">TERA</span></h2><p>TERA, which stands for Transformer Encoder Representations from Alteration, is a self-supervised speech pre-training<br>method. </p>
<h2><span id="experiment-design">experiment design</span></h2><p>Amount of labeled data needed to perform well.<br>with pre-trained and without pre-trained.</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-04-25T09:59:30.000Z" title="2023-4-25 11:59:30 ├F10: AM┤">2023-04-25</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-05-21T20:01:11.333Z" title="2023-5-21 10:01:11 ├F10: PM┤">2023-05-21</time></span><span class="level-item">4 minutes read (About 646 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/04/25/Turing-Machines/">Turing Machines and Undecidability</a></p><div class="content"><h2><span id="mathematical-model-of-computation">mathematical model of computation</span></h2><h3><span id="turing-machines">Turing Machines</span></h3><p>Turing machines are primarily a mathematical construction. A Turing machine (TM) consists of:A finite set of states: initial state(when the TM starts, it is in this state) and accepting states(if the TM ends in one of these states, it says yes); Transition rules that determine what to do based on current state and content of memory tape at current position; An alphabet, the set of symbols that can be stored on the memory tape.</p>
<h4><span id="the-church-turing-hypothesis">The Church-Turing Hypothesis</span></h4><p>Anything which can be computed by any automatic method, can also be computed by a Turing machine.</p>
<p>If a model can also do everything that a Turing machine can, then it is called a Turing-complete model of computation.</p>
<h3><span id="the-ram-model">The RAM Model</span></h3><p>Models the way modern computers operate, with registers and memory cells that can be immediately accessed.</p>
<h3><span id="comparison-based-sorting">Comparison-Based Sorting</span></h3><p>We will prove that any such algorithm must make Ω(n log n)<br>comparisons (and thus take Ω(n log n) time)</p>
<h2><span id="undecidability">Undecidability</span></h2><h3><span id="decision-problems">decision problems</span></h3><p>The answer is just a single bit of information,“yes” or “no”.A decision problem is decidable if there exists an algorithm for solving the problem without any efficiency considerations.</p>
<h3><span id="the-halting-problem">The Halting Problem</span></h3><p>Turing machine M, and input x to M. Does M halt when run on the input x?<br>The Halting Problem is undecidable.</p>
<h3><span id="halting-on-empty-input">Halting On Empty Input</span></h3><p>It is undecidable.</p>
<h3><span id="halting-on-all-inputs">Halting on All Inputs</span></h3><p>It is undecidable.</p>
<h3><span id="recursively-enumerable-problems">Recursively Enumerable Problems</span></h3><p>There exists an “algorithm” which terminates whenever the answer is<br>“yes” but does not terminate on “no” instances.Problems which have algorithms like this are called recursively enumerable.</p>
<h2><span id="turing-reduction">Turing reduction</span></h2><p>A Turing reduction (also called Cook reduction) from a problem<br>X to a problem Y is an efficient algorithm for problem X which is<br>allowed to assume an algorithm for Y as a black box.</p>
<h3><span id="vertex-cover">Vertex Cover</span></h3><h3><span id="independent-set">Independent Set</span></h3><h3><span id="set-cover">Set Cover</span></h3><h3><span id="3-sat">3-Sat</span></h3><h2><span id="karp-reductions">Karp Reductions</span></h2><p>X ≤p Y if given any instance A of problem X, we can in<br>polynomial time construct an instance f (A) of problem Y such that<br>the answer to A is the same as the answer to f (A).</p>
<p>Turing reduction is a type of reduction that is stronger than Karp reduction. A problem A is Turing-reducible to problem B if there exists a Turing machine that can solve problem A by making a polynomial number of calls to a black-box subroutine for problem B. In other words, if we can use an algorithm for problem B to solve problem A, then A is Turing-reducible to B. Turing reduction preserves the complexity class of a problem, meaning that if A is Turing-reducible to B and B is in a certain complexity class, then A is also in that complexity class.</p>
<p>Karp reduction, also known as polynomial-time reduction, is a weaker form of reduction than Turing reduction. A problem A is Karp-reducible to problem B if there exists a polynomial-time algorithm that can transform any instance of problem A into an instance of problem B such that the answer to the transformed instance is the same as the answer to the original instance of problem A. In other words, if we can use an algorithm for problem B to solve a transformed version of problem A in polynomial time, then A is Karp-reducible to B. Karp reduction preserves the complexity class up to polynomial factors, meaning that if A is Karp-reducible to B and B is in a certain complexity class, then A is also in that complexity class up to polynomial factors.</p>
<h2><span id="good-reference">Good reference</span></h2><p><a target="_blank" rel="noopener" href="https://www.cs.rochester.edu/u/nelson/courses/csc_173/computability/undecidable.html">https://www.cs.rochester.edu/u/nelson/courses/csc_173/computability/undecidable.html</a><br><a target="_blank" rel="noopener" href="http://www2.lawrence.edu/fast/GREGGJ/CMSC515/chapt05/Reducibility.html">http://www2.lawrence.edu/fast/GREGGJ/CMSC515/chapt05/Reducibility.html</a><br><a target="_blank" rel="noopener" href="https://www.cs.princeton.edu/courses/archive/spring05/cos423/lectures.php">https://www.cs.princeton.edu/courses/archive/spring05/cos423/lectures.php</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-04-25T08:33:57.000Z" title="2023-4-25 10:33:57 ├F10: AM┤">2023-04-25</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-05-20T17:42:31.874Z" title="2023-5-20 7:42:31 ├F10: PM┤">2023-05-20</time></span><span class="level-item">4 minutes read (About 573 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/04/25/Randomized-Algorithms/">Randomized Algorithms</a></p><div class="content"><h2><span id="randomized-algorithms-and-approximation-algorithm">Randomized Algorithms and  approximation algorithm</span></h2><p>A randomized algorithm is an algorithm that uses randomness to<br>make some of its choices.</p>
<h3><span id="las-vegas-algorithms">Las Vegas Algorithms</span></h3><p>A Las Vegas algorithm is a randomized algorithm that always finds<br>the correct answer, but the running time of the algorithm might<br>vary significantly depending on the random choices made.</p>
<h3><span id="monte-carlo-algorithms">Monte Carlo Algorithms</span></h3><p>A Monte Carlo algorithm is a randomized algorithm where the<br>output of the algorithm may be incorrect, but we have a guarantee<br>that this only happens with small probability.</p>
<h3><span id="random-number-generator">Random Number Generator</span></h3><p> True RNG: gets randomness from measuring physical phenomena<br>(e.g. background radiation) that we believe is sufficiently random<br>Pseudo-RNG: starting from a small random seed, new numbers are<br>generated in a completely deterministic way.</p>
<h3><span id="randomized-min-cut-algorithm">Randomized Min-Cut Algorithm</span></h3><p>while G has more than 2 vertices:Pick a uniformly random edge of G and contract it.<br>the total runtime is $O(n^2m)$. However this algorithm can be refined and running time improved to $O(n^2log(n))$ (relatively simple algorithm) or<br>$O(m log^3(n))$ (more complicated algorithm).</p>
<h2><span id="approximation-algorithm">approximation algorithm</span></h2><h3><span id="the-maximum-cut-problem">The Maximum Cut Problem</span></h3><p>Partition of vertices of G into two non-empty sets A and B such that number of edges between A and B is maximized. It is a NP-hard problem. Let us lower the ambition and try to find a fast algorithm that finds a “reasonably good” cut: Random algorithm:Put each vertex in either A or B independently with probability 1/2.The random assignment algorithm cuts (in expectation) at least half of all the edges in the graph.This is an example of an approximation algorithm.</p>
<h3><span id="finding-good-but-maybe-not-optimal-solutions">finding “good but maybe not optimal” solutions</span></h3><h4><span id="heuristic-algorithms">Heuristic Algorithms</span></h4><p>Work well in some or even many cases, but with no guarantees<br>about how well they perform.</p>
<h4><span id="approximation-algorithms">Approximation Algorithms</span></h4><p>Algorithms with provable guarantee that the solution found is relatively good compared to the optimum solution, for all instances.<br>For a minimization problem, an algorithm has approximation ratio $\alpha ≥ 1$ if for every instance it holds that $ Alg ≤ \alpha Opt$.<br>For a maximization problem, the inequality goes the other way: we have approximation ratio $\alpha ≤ 1$ if $ Alg ≥ \alpha Opt$.</p>
<h3><span id="alpha-approximation-algorithm">alpha-approximation algorithm</span></h3><p>approximation ratio: $\alpha$.</p>
<h3><span id="minimum-load-balancing">Minimum Load Balancing</span></h3><p>NP-hard problem.<br> Given Lengths t1; : : : ; tn of n jobs to be run on m machines, to find Minimum possible makespan of a schedule for the n jobs.<br>Approximation Algorithm for Load Balancing:Assign job i to the machine j with the smallest load. </p>
<h3><span id="how-to-prove-it">how to prove it</span></h3><p>The main difficulty in analyzing approximation<br>algorithms is to get some handle on the Opt value. We need to find Opt, but finding Opt is NP-hard, so we find lower bounds on Opt. </p>
<h3><span id="minimum-vertex-cover">Minimum Vertex Cover</span></h3><p>A vertex cover in a graph G is a set of vertices that “touches” every edge of G.What is the size of a minimum vertex cover of G? Approximation Algorithm for Minimum Vertex Cover:while there exists an edge e = (u; v) such that u /∈ S and v /∈ S, add (u,v) into S. The algorithm is a 2-approximation algorithm.<br>“Unique Games Conjecture”: it is known that Vertex Cover cannot be approximated better than<br>within a factor 2.</p>
<h3><span id="minimum-set-cover">Minimum Set Cover</span></h3><p>A collection of sets S1; : : : ; Sm ⊆ U over some universe U, What is minimum number of Si’s whose union equals U?</p>
<h3><span id="analysis-of-greedy-set-cover-finale">Analysis of Greedy Set Cover Finale?</span></h3></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-04-02T14:17:48.000Z" title="2023-4-2 4:17:48 ├F10: PM┤">2023-04-02</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-10-12T17:48:43.583Z" title="2023-10-12 7:48:43 ├F10: PM┤">2023-10-12</time></span><span class="level-item"><a class="link-muted" href="/categories/database/">database</a></span><span class="level-item">a minute read (About 185 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/04/02/Elasticsearch/">Elasticsearch</a></p><div class="content"><h2><span id="elk-stack">ELK Stack</span></h2><p>Elasticsearch, Logstash and Kibana</p>
<h2><span id="elasticsearch">Elasticsearch</span></h2><p>Elasticsearch is a NoSQL database.When you feed data into Elasticsearch, the data is placed into Apache Lucene indexes.</p>
<h3><span id="apache-lucene">Apache Lucene</span></h3><p>Apache Lucene™ is a high-performance, full-featured search engine library written entirely in Java.</p>
<h3><span id="api">API</span></h3><h2><span id="logstash">Logstash</span></h2><p>Using more than 50 input plugins for different platforms, databases and applications, Logstash can be defined to collect and process data from these sources and send them to other systems for storage and analysis.</p>
<h2><span id="project">project</span></h2><p><a target="_blank" rel="noopener" href="https://trecpodcasts.github.io/">https://trecpodcasts.github.io/</a><br><a target="_blank" rel="noopener" href="https://doc.yonyoucloud.com/doc/mastering-elasticsearch/chapter-2/21_README.html">https://doc.yonyoucloud.com/doc/mastering-elasticsearch/chapter-2/21_README.html</a><br><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1600163">https://cloud.tencent.com/developer/article/1600163</a><br><a target="_blank" rel="noopener" href="https://www.elastic.co/cn/blog/how-to-improve-elasticsearch-search-relevance-with-boolean-queries">https://www.elastic.co/cn/blog/how-to-improve-elasticsearch-search-relevance-with-boolean-queries</a><br><a target="_blank" rel="noopener" href="https://www.elastic.co/guide/en/app-search/current/relevance-tuning-guide.html">https://www.elastic.co/guide/en/app-search/current/relevance-tuning-guide.html</a><br><a target="_blank" rel="noopener" href="https://medium.com/mlearning-ai/enhancing-information-retrieval-via-semantic-and-relevance-matching-64973ff81818">https://medium.com/mlearning-ai/enhancing-information-retrieval-via-semantic-and-relevance-matching-64973ff81818</a><br><a target="_blank" rel="noopener" href="https://www.elastic.co/cn/blog/how-to-improve-elasticsearch-search-relevance-with-boolean-queries">https://www.elastic.co/cn/blog/how-to-improve-elasticsearch-search-relevance-with-boolean-queries</a><br><a target="_blank" rel="noopener" href="https://bigdataboutique.com/blog/optimizing-elasticsearch-relevance-a-detailed-guide-c9efd3">https://bigdataboutique.com/blog/optimizing-elasticsearch-relevance-a-detailed-guide-c9efd3</a><br>NDCG：<br><a target="_blank" rel="noopener" href="https://www.javatips.net/api/MyMediaLiteJava-master/src/org/mymedialite/eval/measures/NDCG.java">https://www.javatips.net/api/MyMediaLiteJava-master/src/org/mymedialite/eval/measures/NDCG.java</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-03-31T15:53:12.000Z" title="2023-3-31 5:53:12 ├F10: PM┤">2023-03-31</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-10-12T17:27:05.212Z" title="2023-10-12 7:27:05 ├F10: PM┤">2023-10-12</time></span><span class="level-item"><a class="link-muted" href="/categories/theory/">theory</a></span><span class="level-item">a minute read (About 204 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/03/31/speech/">speech</a></p><div class="content"><h2><span id="signal">signal</span></h2><p>spectrogram: A spectrogram of a time signal is a special two-dimensional representation that displays time in its horizontal axis and frequency in its vertical axis.</p>
<h2><span id="short-time-fourier-analysis">short-time Fourier analysis</span></h2><p>Why use it?<br>Some regions of speech signals shorter than 100 milliseconds often appear to be periodic, so that we can use the exact definition of Fourier<br>transform.</p>
<h3><span id="spectral-leakage">spectral leakage</span></h3><p>This phenomenon is called spectral leakage because the amplitude of one harmonic leaks over the rest and masks its value.</p>
<h2><span id="feature-extraction">feature extraction</span></h2><p>Representation of speech signals in the frequency domain is especially useful because the frequency structure of a phoneme is generally unique.</p>
<p>Sinusoids are important because speech signals can be decomposed as sums of sinusoids.</p>
<p>For voiced sounds there is typically more energy at low frequencies<br>than at high frequencies, also called roll-off. To make the spectrograms easier to read, sometimes the signal is first preemphasized (typically with a first-order difference FIR filter) to boost the high frequencies<br>to counter the roll-off of natural speech.</p>
<h2><span id="digital-systems">Digital Systems</span></h2><p>Linear Time-Invariant Systems and Linear Time-Varying Systems.</p>
<h2><span id="the-fourier-transform">The Fourier Transform</span></h2><h2><span id="z-transform">Z-Transform</span></h2><h2><span id="digital-filter">digital filter</span></h2><h2><span id="filterbank">filterbank</span></h2><p>A filterbank is a collection of filters that span the whole frequency spectrum.</p>
<h2><span id="short-time-analysis">short-time analysis</span></h2></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-03-28T09:46:34.000Z" title="2023-3-28 11:46:34 ├F10: AM┤">2023-03-28</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-05-21T21:24:08.965Z" title="2023-5-21 11:24:08 ├F10: PM┤">2023-05-21</time></span><span class="level-item">4 minutes read (About 611 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/03/28/NP/">NP and reduction</a></p><div class="content"><h2><span id="p">P</span></h2><p>P is the set of all decision problems that can be solved in<br>Polynomial time.</p>
<h2><span id="extended-church-turing-thesis">Extended Church-Turing Thesis</span></h2><p>Any realistic model of computation can be efficiently<br>simulated by a Turing machine.<br>(The ECT is probably false!Probable counter-example: Quantum computing,However, the ECT is true for the classic models of computation<br>underlying our laptops)</p>
<h2><span id="np-problem">NP problem</span></h2><p>We do not know of polynomial-time algorithms for these problems, and we cannot prove that no<br>polynomial-time algorithm exists. </p>
<p>NP stands for Nondeterministic Polynomial time.</p>
<p>NP is a set of problems that there is a polynomial-time algorithm that verify if a solution to the problem is correct.<br>Note that showing that a problem is in NP does not mean that the problem can be solved in polynomial time. It only means that a proposed solution can be verified in polynomial time.</p>
<p>All problems in P are also in NP, we do not know if P=NP.</p>
<h2><span id="np-hardness">NP-hardness</span></h2><p>We say that a problem Y is NP-hard if every problem in NP can be Karp-reduced to Y. To prove NP-hardness for Y we only have to find<br>one other NP-hard problem X and reduce X to Y.</p>
<h2><span id="np-complete-problem">NP-complete problem</span></h2><p>A large class of problems in this “gray area” has been characterized,<br>and it has been proved that they are equivalent in the following sense: a polynomial-time algorithm for any one of them would imply the existence of a polynomial-time algorithm for all of them. These are the NP-complete problems.</p>
<p>Every problem in NP can be reduced to X. We will call such an X an NP-complete problem.</p>
<p>To prove a problem is np-complete, we need to prove it lies in Np and it is np-hard. In NP is proved by verifying the solution in polynomial time. Np-hard is proved by using Karp-reduction from known NP-hard problem.</p>
<p>the NP-complete problems are the hardest problems in NP</p>
<h2><span id="the-cook-levin-theorem">The Cook-Levin Theorem</span></h2><p>The Sat problem is NP-hard.</p>
<h3><span id="hamiltonian-cycle">Hamiltonian Cycle</span></h3><p>Reduction from Sat to Hamiltonian Cycle.</p>
<h3><span id="travelling-salesman">Travelling Salesman</span></h3><p>Reduction from Hamiltonian Cycle</p>
<h3><span id="graph-coloring">Graph Coloring</span></h3><p>Graph Coloring is NP-hard.<br>2-coloring is not NP-hard.</p>
<h3><span id="vertex-cover">Vertex Cover</span></h3><p>we say that a set of nodes S is a vertex cover if every edge e has at least one end in S.</p>
<h3><span id="set-cover">Set Cover</span></h3><p>Given a set U of n elements, a collection S1,…, Sm of subsets of U, and<br>a number k, does there exist a collection of at most k of these sets whose<br>union is equal to all of U?</p>
<h2><span id="conp">CoNP</span></h2><p>The complexity class CoNP consists of all problems where a “no” answer can be efficiently verified.<br>For every NP-complete problem there is a<br>corresponding CoNP-complete problem.</p>
<p>We do not know if NP=CoNP.</p>
<h2><span id="pspace">PSPACE</span></h2><p>The complexity class PSPACE consists of all problems that can<br>be solved by an algorithm using at most a polynomial amount of<br>space.<br>It is strongly believed that NP != PSPACE, but we do not even<br>know with certainty whether P != PSPACE or not!</p>
<p>For many 2-player games like Geography, deciding if there is a<br>winning strategy from a given position is a PSPACE problem</p>
<h2><span id="bpp-and-zpp">BPP and ZPP</span></h2><h3><span id="bpp">BPP</span></h3><p>BPP (Bounded-error Probabilistic Polynomial-time)<br>consists of all decision problems for which there is a<br>polynomial-time randomized algorithms that is correct with<br>probability at least 2=3 on all instances.</p>
<h3><span id="zpp">ZPP</span></h3><p>ZPP (Zero-error Probabilistic Polynomial Time)<br>consists of all decision problems for which there is a Las Vegas<br>algorithm running in expected polynomial time.<br>It is widely believed that P = ZPP = BPP but all three could be<br>different.</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-01-03T12:49:28.000Z" title="2023-1-3 1:49:28 ├F10: PM┤">2023-01-03</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-10-12T17:25:11.914Z" title="2023-10-12 7:25:11 ├F10: PM┤">2023-10-12</time></span><span class="level-item"><a class="link-muted" href="/categories/development-tools/">development tools</a></span><span class="level-item">2 minutes read (About 232 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/01/03/toolnotes/">toolnotes</a></p><div class="content"><h2><span id="背景">背景</span></h2><p>记录平时学习和开发工程中使用工具的一些备忘点。</p>
<h2><span id="正文">正文</span></h2><p>用vim时，鼠标右键不能粘贴而是进入了visual模式，解决方法：：set mouse-=a</p>
<p>远程jupyter配置<br><a target="_blank" rel="noopener" href="https://juejin.cn/post/7026371559971520525">https://juejin.cn/post/7026371559971520525</a></p>
<p>For Debian / Ubuntu: .deb packages installed by apt and dpkg<br>For Rocky / Fedora / RHEL: .rpm packages installed by yum<br>For FreeBSD: .txz packages installed by pkg</p>
<p>ssh-keygen -t rsa -b 4096 -C “your_email@example.com”</p>
<p>Kill PyTorch Distributed Training Processes:<br>kill $(ps aux | grep YOUR_TRAINING_SCRIPT.py | grep -v grep | awk ‘{print $2}’)</p>
<p>git reset —soft HEAD^ 撤销commit<br>git reset —hard HEAD^ 撤销add</p>
<p> tokenizing in Unix: “tr” command<br> 分词文件排序和统计<br> tr -sc ’A-Za-z’ ’\n’ &lt; $file_name| sort | uniq -c</p>
<p> conda:<br> conda create -n python=3.7 yourenv pip</p>
<p> git find . -name “*.py”|xargs git add —</p>
<p>导出项目依赖：<br>pip install pipreqs<br>pipreqs ./ —encoding=utf-8 —force</p>
<h3><span id="plotneuralnet">PlotNeuralNet</span></h3><p> <a target="_blank" rel="noopener" href="https://pub.towardsai.net/creating-stunning-neural-network-visualizations-with-chatgpt-and-plotneuralnet-adab37589e5">https://pub.towardsai.net/creating-stunning-neural-network-visualizations-with-chatgpt-and-plotneuralnet-adab37589e5</a></p>
<h3><span id="remote-develop">remote develop</span></h3><p><a target="_blank" rel="noopener" href="https://devblogs.microsoft.com/python/remote-python-development-in-visual-studio-code/">https://devblogs.microsoft.com/python/remote-python-development-in-visual-studio-code/</a></p>
<h2><span id="references">references”</span></h2><p><a target="_blank" rel="noopener" href="https://leimao.github.io/blog/Kill-PyTorch-Distributed-Training-Processes/">https://leimao.github.io/blog/Kill-PyTorch-Distributed-Training-Processes/</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-12-29T20:59:33.000Z" title="2022-12-29 9:59:33 ├F10: PM┤">2022-12-29</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-01-13T16:25:27.335Z" title="2023-1-13 5:25:27 ├F10: PM┤">2023-01-13</time></span><span class="level-item">8 minutes read (About 1239 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/12/29/PCA/">PCA</a></p><div class="content"><h2><span id="关于pca为什么要中心化">关于PCA为什么要中心化</span></h2><p>因为不做zero mean，根本做不了PCA。从线性变换的本质来说，PCA就是在线性空间做一个旋转（数据矩阵右乘协方差矩阵的特征向量矩阵），然后取低维子空间（实际上就是前n<em>components个特征向量张成的子空间）上的投影点来代替原本的点，以达到降维的目的，注意我说的，只做了旋转，没有平移，所以首先你要保证原本空间里的点是以原点为中心分布的，这就是zero mean的目的。另外如果自己手撸过PCA的算法就知道了，explained_variance</em>和explained<em>variance_ratio</em>是怎么实现的？explained<em>variance就是协方差矩阵的每个特征值除以sample数，而explained_variance_ratio</em>是每个特征值除以所有特征值之和。为什么这么简单呢？这也和zero mean有关，如果你用最大投影长度的证法去证明PCA就会在过程中很自然的发现这一点，在这里我就不展开了。</p>
<p>链接：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/40956812/answer/848527057">https://www.zhihu.com/question/40956812/answer/848527057</a></p>
<p>PCA 去中心化不一定是必需的，这一结论成立的前提是严格使用协方差矩阵的定义式 S=XXT−nμμT，而不是用 XXT 来当作协方差矩阵。</p>
<p> Centering is an important pre-processing step because it ensures that the resulting components are only looking at the variance within the dataset, and not capturing the overall mean of the dataset as an important variable (dimension). Without mean-centering, the first principal component found by PCA might correspond with the mean of the data instead of the direction of maximum variance.<br> <a target="_blank" rel="noopener" href="https://towardsdatascience.com/tidying-up-with-pca-an-introduction-to-principal-components-analysis-f876599af383">https://towardsdatascience.com/tidying-up-with-pca-an-introduction-to-principal-components-analysis-f876599af383</a></p>
<p> Why is minimizing squared residuals equivalent to maximizing variance?<br> Consider a datapoint  (row  of ). Then the contribution of that datapoint to the variance is , or equivalently the squared Euclidean length . Applying the Pythagorean theorem shows that this total variance equals the sum of variance lost (the squared residual) and variance remaining. Thus, it is equivalent to either maximize remaining variance or minimize lost variance to find the principal components. </p>
<p> <a target="_blank" rel="noopener" href="http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/">http://alexhwilliams.info/itsneuronalblog/2016/03/27/pca/</a></p>
<p> PCA can only be interpreted as the singular value decomposition of a data matrix when the columns have first been centered by their means.<br> <a target="_blank" rel="noopener" href="https://stats.stackexchange.com/questions/29781/when-conducting-multiple-regression-when-should-you-center-your-predictor-varia">https://stats.stackexchange.com/questions/29781/when-conducting-multiple-regression-when-should-you-center-your-predictor-varia</a></p>
<p> PCA focuses on “explaining” the data matrix using the sample means plus the eigencomponents.  When the column mean is far from the origin, the first right singular value is usually quite highly correlated with column mean - thus using PCA concentrates on the second, third and sometimes higher order singular vectors.  This is a loss of information when the mean is informative for the process under study.  On the other hand, when the scatterplot of the data is roughly elliptical, the PCs typically align with the major axes of the ellipse.  Due to the uncorrelatedness constraint, if the mean is far from the origin, the first singular vector will be close to the mean and the others will be tilted away form the major axes of the ellipse.  Thus the first singular vector will not be informative about the spread of the data, and the second and third singular values will not be in the most informative directions.  Generally, PCA will be more informative, particularly as a method for plotting the data, than uncentered SVD.<br> <a target="_blank" rel="noopener" href="https://online.stat.psu.edu/stat555/node/94/">https://online.stat.psu.edu/stat555/node/94/</a></p>
<p> Since X is zero centered we can think of them as capturing the spread of the data around the mean in a sense reminiscent of PCA.<br> <a target="_blank" rel="noopener" href="https://intoli.com/blog/pca-and-svd/">https://intoli.com/blog/pca-and-svd/</a></p>
<p> that reconstruction error is minimized by taking as columns of W some k orthonormal vectors maximizing the total variance of the projection.<br> <a target="_blank" rel="noopener" href="https://stats.stackexchange.com/questions/130721/what-norm-of-the-reconstruction-error-is-minimized-by-the-low-rank-approximation">https://stats.stackexchange.com/questions/130721/what-norm-of-the-reconstruction-error-is-minimized-by-the-low-rank-approximation</a></p>
<p> PCA is a regressional model without intercept1. Thus, principal components inevitably come through the origin. If you forget to center your data, the 1st principal component may pierce the cloud not along the main direction of the cloud, and will be (for statistics purposes) misleading.<br><a target="_blank" rel="noopener" href="https://stats.stackexchange.com/questions/22329/how-does-centering-the-data-get-rid-of-the-intercept-in-regression-and-pca">https://stats.stackexchange.com/questions/22329/how-does-centering-the-data-get-rid-of-the-intercept-in-regression-and-pca</a></p>
<p>Centering brings in a big difference. PCA with centering maximizes SS deviations from the mean (i.e. variance); PCA on raw data maximizes SS deviations from the zero point.<br><a target="_blank" rel="noopener" href="https://stats.stackexchange.com/questions/489037/principal-components-with-and-without-centering?noredirect=1&amp;lq=1">https://stats.stackexchange.com/questions/489037/principal-components-with-and-without-centering?noredirect=1&amp;lq=1</a></p>
<h2><span id="svd-and-pca">SVD and PCA</span></h2><p>  <a target="_blank" rel="noopener" href="https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca">https://stats.stackexchange.com/questions/134282/relationship-between-svd-and-pca-how-to-use-svd-to-perform-pca</a></p>
<h2><span id="singular-value-decomposition">singular value decomposition</span></h2><p> SVD is basically a matrix factorization technique, which decomposes any matrix into 3 generic and familiar matrices.</p>
<h2><span id="eigenvalues-and-eigenvectors">Eigenvalues and Eigenvectors</span></h2><p> The concept of eigenvectors is applicable only for square matrices. The vector space spanned by an eigenvector is called an eigenspace.<br> A square matrix is called a diagonalizable matrix if it can be written in the format: $ A=PDP^{-1} $, D is the diagonal matrix comprises of the eigenvalues as diagonal elements<br> A Symmetric Matrix where the matrix is equal to the transpose of itself.<br> Special properties of a Symmetric Matrix with respect to eigenvalues and eigenvectors:Has only Real eigenvalues;Always diagonalizable;Has orthogonal eigenvectors.<br> A matrix is called an Orthogonal Matrix if the transpose of the matrix is the inverse of that matrix.<br>ince the eigenvectors of a Symmetric matrix are orthogonal to each other, matrix P in the diagonalized matrix A is an orthogonal matrix. So we say that any Symmetric Matrix is Orthogonally Diagonalizable.</p>
<p><a target="_blank" rel="noopener" href="https://towardsdatascience.com/singular-value-decomposition-and-its-applications-in-principal-component-analysis-5b7a5f08d0bd">https://towardsdatascience.com/singular-value-decomposition-and-its-applications-in-principal-component-analysis-5b7a5f08d0bd</a></p>
<p>% For the PCA derived from maximal preserved variance \cite{lee2007nonlinear}, we have the covariance<br>% of $\mathbf{y}$, which is<br>% \begin{equation}<br>% \mathbf{C}_{\mathbf{y} \mathbf{y}}=E\left{\mathbf{y} \mathbf{y}^T\right}<br>% \end{equation}<br>% This equation is valid only when $\mathbf{y}$ is centered.<br> The goal of PCA is to maximize the variance of the data along each of the principal components. Centering is an important step because it ensures that the resulting components are only looking at the variance of features, and not capturing the means of the features as important. Without mean-centering, the first principal component found by PCA might correspond with the mean of the data instead of the direction of maximum variance.</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2022-12-23T15:44:05.000Z" title="2022-12-23 4:44:05 ├F10: PM┤">2022-12-23</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-10-12T17:27:34.376Z" title="2023-10-12 7:27:34 ├F10: PM┤">2023-10-12</time></span><span class="level-item"><a class="link-muted" href="/categories/theory/">theory</a></span><span class="level-item">a minute read (About 169 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2022/12/23/Network-Representation-Learning/">Network Representation Learning</a></p><div class="content"><h2><span id="background">background</span></h2><p>Recording studying during KTH. First blog about network representation learning, a project belongs to machine learning, advanced course.</p>
<h2><span id="line">LINE</span></h2><p>Reproduce paper “LINE: Large-scale Information Network Embedding”.</p>
<h3><span id="alias-table-method">Alias Table Method</span></h3><p>It’s a method of effiently drawing samples from discrete distribution.<br>reference:<br><a target="_blank" rel="noopener" href="https://www.keithschwarz.com/darts-dice-coins/">https://www.keithschwarz.com/darts-dice-coins/</a><br><a target="_blank" rel="noopener" href="https://blog.csdn.net/haolexiao/article/details/65157026">https://blog.csdn.net/haolexiao/article/details/65157026</a></p>
<h3><span id="negative-sampling">Negative Sampling</span></h3><h4><span id="word2vec">word2vec</span></h4><p>Original paper:<br>Efficient estimation of word representations in vector space.<br>reference:<br>word2vec Explained: Deriving Mikolov et al.’s<br>Negative-Sampling Word-Embedding Method</p>
<h4><span id="skip-gram-model">Skip-Gram Model</span></h4><p>Original papaer:Distributed Representations of Words and Phrases<br>and their Compositionality.<br>The idea behind the word2vec models is that the words that appear in the same context (near each other) should have similar word vectors. Therefore, we should consider some notion of similarity in our objective when training the model. This is done using the dot product since when vectors are similar, their dot product is larger.<br>reference:<br><a target="_blank" rel="noopener" href="https://www.baeldung.com/cs/nlps-word2vec-negative-sampling">https://www.baeldung.com/cs/nlps-word2vec-negative-sampling</a></p>
<h2><span id="graphsage">graphSage</span></h2></div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous"><a href="/page/2/">Previous</a></div><div class="pagination-next"><a href="/page/4/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link" href="/">1</a></li><li><a class="pagination-link" href="/page/2/">2</a></li><li><a class="pagination-link is-current" href="/page/3/">3</a></li><li><a class="pagination-link" href="/page/4/">4</a></li><li><a class="pagination-link" href="/page/5/">5</a></li><li><a class="pagination-link" href="/page/6/">6</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/tx.png" alt="s-serenity"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">s-serenity</p><p class="is-size-6 is-block">student</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Shanghai, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">56</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">8</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">21</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/s-serenity" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/s-serenity"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://github.com/s-serenity" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/algorithm/"><span class="level-start"><span class="level-item">algorithm</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/computer-system/"><span class="level-start"><span class="level-item">computer system</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/database/"><span class="level-start"><span class="level-item">database</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/development-tools/"><span class="level-start"><span class="level-item">development tools</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/others/"><span class="level-start"><span class="level-item">others</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/programming-language/"><span class="level-start"><span class="level-item">programming language</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/theory/"><span class="level-start"><span class="level-item">theory</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li><li><a class="level is-mobile" href="/categories/web-development/"><span class="level-start"><span class="level-item">web development</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-10-22T15:55:36.000Z">2023-10-22</time></p><p class="title"><a href="/2023/10/22/llm1/">llm1</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-10-20T08:17:16.000Z">2023-10-20</time></p><p class="title"><a href="/2023/10/20/pai2/">Probabilistic Artificial Intelligence - Variational Inference</a></p><p class="categories"><a href="/categories/theory/">theory</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-10-17T13:23:43.000Z">2023-10-17</time></p><p class="title"><a href="/2023/10/17/bigdata4/">bigdata - wide column stores</a></p><p class="categories"><a href="/categories/database/">database</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-10-13T12:42:15.000Z">2023-10-13</time></p><p class="title"><a href="/2023/10/13/pai0/">Probabilistic Artificial Intelligence - Bayesian Linear Regression</a></p><p class="categories"><a href="/categories/theory/">theory</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-10-12T17:22:39.000Z">2023-10-12</time></p><p class="title"><a href="/2023/10/12/hexo-upgrade/">hexo upgrade</a></p><p class="categories"><a href="/categories/others/">others</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2023/"><span class="level-start"><span class="level-item">2023</span></span><span class="level-end"><span class="level-item tag">28</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/"><span class="level-start"><span class="level-item">2022</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/"><span class="level-start"><span class="level-item">2021</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/"><span class="level-start"><span class="level-item">2020</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/"><span class="level-start"><span class="level-item">2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/"><span class="level-start"><span class="level-item">2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Algorithm/"><span class="tag">Algorithm</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Elasticsearch/"><span class="tag">Elasticsearch</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux/"><span class="tag">Linux</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Markdown/"><span class="tag">Markdown</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SQL/"><span class="tag">SQL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/algorithm/"><span class="tag">algorithm</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/big-data/"><span class="tag">big data</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/computer-vision/"><span class="tag">computer vision</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/deep-learning/"><span class="tag">deep learning</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/git/"><span class="tag">git</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/graph/"><span class="tag">graph</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/hexo/"><span class="tag">hexo</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/java/"><span class="tag">java</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/machine-learning/"><span class="tag">machine learning</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/probability/"><span class="tag">probability</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python/"><span class="tag">python</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/reinforcement-learning/"><span class="tag">reinforcement learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/speech/"><span class="tag">speech</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/tools/"><span class="tag">tools</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="s-serenity" height="28"></a><p class="is-size-7"><span>&copy; 2023 s-serenity</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2019</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>