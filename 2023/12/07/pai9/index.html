<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>pai - Model-free Approximate Reinforcement Learning - s-serenity</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="s-serenity"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="s-serenity"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Model-free Approximate Reinforcement LearningTabular Reinforcement Learning as OptimizationIn particular, in the tabular setting (i.e., over a discrete domain), we can parameterize the value function"><meta property="og:type" content="blog"><meta property="og:title" content="pai - Model-free Approximate Reinforcement Learning"><meta property="og:url" content="http://yoursite.com/2023/12/07/pai9/"><meta property="og:site_name" content="s-serenity"><meta property="og:description" content="Model-free Approximate Reinforcement LearningTabular Reinforcement Learning as OptimizationIn particular, in the tabular setting (i.e., over a discrete domain), we can parameterize the value function"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://yoursite.com/2023/12/07/pai9/image-77.png"><meta property="og:image" content="http://yoursite.com/2023/12/07/pai9/image-78.png"><meta property="og:image" content="http://yoursite.com/2023/12/07/pai9/image-79.png"><meta property="og:image" content="http://yoursite.com/2023/12/07/pai9/image-80.png"><meta property="og:image" content="http://yoursite.com/2023/12/07/pai9/image-90.png"><meta property="og:image" content="http://yoursite.com/2023/12/07/pai9/image-91.png"><meta property="og:image" content="http://yoursite.com/2023/12/07/pai9/image-92.png"><meta property="og:image" content="http://yoursite.com/2023/12/07/pai9/image-93.png"><meta property="og:image" content="http://yoursite.com/2023/12/07/pai9/image-94.png"><meta property="og:image" content="http://yoursite.com/2023/12/07/pai9/image-95.png"><meta property="og:image" content="http://yoursite.com/2023/12/07/pai9/image-96.png"><meta property="og:image" content="http://yoursite.com/2023/12/07/pai9/image-97.png"><meta property="og:image" content="http://yoursite.com/2023/12/07/pai9/image-99.png"><meta property="og:image" content="http://yoursite.com/2023/12/07/pai9/image-98.png"><meta property="og:image" content="http://yoursite.com/2023/12/07/pai9/image-100.png"><meta property="og:image" content="http://yoursite.com/2023/12/07/pai9/image-101.png"><meta property="og:image" content="http://yoursite.com/2023/12/07/pai9/image-102.png"><meta property="og:image" content="http://yoursite.com/2023/12/07/pai9/image-103.png"><meta property="og:image" content="http://yoursite.com/2023/12/07/pai9/image-105.png"><meta property="og:image" content="http://yoursite.com/2023/12/07/pai9/image-104.png"><meta property="og:image" content="http://yoursite.com/2023/12/07/pai9/image-106.png"><meta property="og:image" content="http://yoursite.com/2023/12/07/pai9/image-107.png"><meta property="og:image" content="http://yoursite.com/2023/12/07/pai9/image-108.png"><meta property="og:image" content="http://yoursite.com/2023/12/07/pai9/image-109.png"><meta property="og:image" content="http://yoursite.com/2023/12/07/pai9/image-110.png"><meta property="og:image" content="http://yoursite.com/2023/12/07/pai9/image-111.png"><meta property="og:image" content="http://yoursite.com/2023/12/07/pai9/image-81.png"><meta property="article:published_time" content="2023-12-07T14:26:12.000Z"><meta property="article:modified_time" content="2024-02-21T11:20:40.430Z"><meta property="article:author" content="s-serenity"><meta property="article:tag" content="machine learning"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://yoursite.com/2023/12/07/pai9/image-77.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://yoursite.com/2023/12/07/pai9/"},"headline":"pai - Model-free Approximate Reinforcement Learning","image":["http://yoursite.com/2023/12/07/pai9/image-77.png","http://yoursite.com/2023/12/07/pai9/image-78.png","http://yoursite.com/2023/12/07/pai9/image-79.png","http://yoursite.com/2023/12/07/pai9/image-80.png","http://yoursite.com/2023/12/07/pai9/image-90.png","http://yoursite.com/2023/12/07/pai9/image-91.png","http://yoursite.com/2023/12/07/pai9/image-92.png","http://yoursite.com/2023/12/07/pai9/image-93.png","http://yoursite.com/2023/12/07/pai9/image-94.png","http://yoursite.com/2023/12/07/pai9/image-95.png","http://yoursite.com/2023/12/07/pai9/image-96.png","http://yoursite.com/2023/12/07/pai9/image-97.png","http://yoursite.com/2023/12/07/pai9/image-99.png","http://yoursite.com/2023/12/07/pai9/image-98.png","http://yoursite.com/2023/12/07/pai9/image-100.png","http://yoursite.com/2023/12/07/pai9/image-101.png","http://yoursite.com/2023/12/07/pai9/image-102.png","http://yoursite.com/2023/12/07/pai9/image-103.png","http://yoursite.com/2023/12/07/pai9/image-105.png","http://yoursite.com/2023/12/07/pai9/image-104.png","http://yoursite.com/2023/12/07/pai9/image-106.png","http://yoursite.com/2023/12/07/pai9/image-107.png","http://yoursite.com/2023/12/07/pai9/image-108.png","http://yoursite.com/2023/12/07/pai9/image-109.png","http://yoursite.com/2023/12/07/pai9/image-110.png","http://yoursite.com/2023/12/07/pai9/image-111.png","http://yoursite.com/2023/12/07/pai9/image-81.png"],"datePublished":"2023-12-07T14:26:12.000Z","dateModified":"2024-02-21T11:20:40.430Z","author":{"@type":"Person","name":"s-serenity"},"publisher":{"@type":"Organization","name":"s-serenity","logo":{"@type":"ImageObject","url":"http://yoursite.com/img/logo.svg"}},"description":"Model-free Approximate Reinforcement LearningTabular Reinforcement Learning as OptimizationIn particular, in the tabular setting (i.e., over a discrete domain), we can parameterize the value function"}</script><link rel="canonical" href="http://yoursite.com/2023/12/07/pai9/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.1.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="s-serenity" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-12-07T14:26:12.000Z" title="2023-12-7 3:26:12 ├F10: PM┤">2023-12-07</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-02-21T11:20:40.430Z" title="2024-2-21 12:20:40 ├F10: PM┤">2024-02-21</time></span><span class="level-item"><a class="link-muted" href="/categories/theory/">theory</a></span><span class="level-item">10 minutes read (About 1566 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">pai - Model-free Approximate Reinforcement Learning</h1><div class="content"><h1><span id="model-free-approximate-reinforcement-learning">Model-free Approximate Reinforcement Learning</span></h1><h2><span id="tabular-reinforcement-learning-as-optimization">Tabular Reinforcement Learning as Optimization</span></h2><p>In particular, in the tabular setting (i.e., over a discrete domain), we can parameterize the value function exactly by learning a separate parameter for each state.<br><img src="/2023/12/07/pai9/image-77.png" alt="Alt text"></p>
<p><img src="/2023/12/07/pai9/image-78.png" alt="Alt text"></p>
<p>Now, we cannot compute this derivative because we cannot compute the expectation. Firstly, the expectation is over the true value function which is unknown to us. Secondly, the expectation is over the transition model which we are trying to avoid in model-free methods. To resolve the first issue, analogously to TD-learning, instead of learning the true value function vπ which is unknown, we learn the bootstrapping estimate Vπ. To resolve the first issue, analogously to TD-learning, instead of learning the true value function vπ which is unknown, we learn the bootstrapping estimate Vπ. we will use a Monte Carlo estimate using a single sample. Recall that this is only possible because the transitions are conditionally independent given the state-action pair. </p>
<p><img src="/2023/12/07/pai9/image-79.png" alt="Alt text"></p>
<p><img src="/2023/12/07/pai9/image-80.png" alt="Alt text"></p>
<p>Therefore, TD-learning is essentially performing stochastic gradient descent using the TD-error as an unbiased gradient estimate.<br>Stochastic gradient descent with a bootstrapping estimate is also called stochastic semi-gradient descent.</p>
<h2><span id="value-function-approximation">Value Function Approximation</span></h2><p>Our goal for large state-action spaces is to exploit the smoothness properties5 of the value function to condense the representation. </p>
<h3><span id="heuristics">Heuristics</span></h3><p>The vanilla stochastic semi-gradient descent is very slow.<br>There are mainly two problems.<br>As we are trying to learn an approximate value function that depends on the bootstrapping estimate, this means that the optimization target is “moving” between iterations. In practice, moving targets lead to stability issues. One such technique aiming to “stabilize” the optimization targets is called neural fitted Q-iteration or deep Q-networks (DQN). DQN updates the neural network used for the approximate bootstrapping estimate infrequently to maintain a constant optimization target across multiple episodes. One approach is to clone the neural network and maintain one changing neural network (“online network”) for the most recent estimate of the Q-function which is parameterized by θ, and one fixed neural network (“target network”) used as the target which is parameterized by θold and which is updated infrequently.</p>
<p><img src="/2023/12/07/pai9/image-90.png" alt="Alt text"></p>
<p>This technique is known as experience replay. Another approach is Polyak averaging where the target network is gradually “nudged” by the neural network used to estimate the Q function.</p>
<p>Now, observe that the estimates Q⋆ are noisy estimates of q⋆. The fact that the update rules can be affected by inaccuracies (i.e., noise in the estimates) of the learned Q-function is known as the “maximization bias”. Double DQN (DDQN) is an algorithm that addresses this maximization bias. Instead of picking the optimal action with respect to the old network, it picks the optimal action with respect to the new network. Intuitively, this change ensures that the evaluation of the target network is consistent with the updated Q-function, which makes it more difficult for the algorithm to be affected by noise. </p>
<p><img src="/2023/12/07/pai9/image-91.png" alt="Alt text"></p>
<h2><span id="policy-approximation">Policy Approximation</span></h2><p>Methods that find an approximate policy are also called policy search methods or policy gradient methods. Policy gradient methods use randomized policies for encouraging exploration.</p>
<h3><span id="estimating-policy-values">Estimating Policy Values</span></h3><p><img src="/2023/12/07/pai9/image-92.png" alt="Alt text"><br>The policy value function measures the expected discounted payoff of policy π. </p>
<p><img src="/2023/12/07/pai9/image-93.png" alt="Alt text"><br><img src="/2023/12/07/pai9/image-94.png" alt="Alt text"></p>
<h3><span id="reinforce-gradient">Reinforce Gradient</span></h3><p><img src="/2023/12/07/pai9/image-95.png" alt="Alt text"></p>
<p><img src="/2023/12/07/pai9/image-96.png" alt="Alt text"><br>In this context, however, we cannot apply the reparameterization trick. Fortunately, there is another way of estimating this gradient.</p>
<p><img src="/2023/12/07/pai9/image-97.png" alt="Alt text"></p>
<p><img src="/2023/12/07/pai9/image-99.png" alt="Alt text"></p>
<p>When using a neural network for the parameterization of the policy π, we can use automatic differentiation to obtain unbiased gradient estimates. However, it turns out that the variance of these estimates is very large. Using so-called baselines can reduce the variance dramatically.</p>
<p>The baseline essentially captures the expected or average value, providing a reference point. By subtracting this reference point, the updates become more focused on the deviations from the expected values, which can reduce the variance in these deviations.</p>
<p><img src="/2023/12/07/pai9/image-98.png" alt="Alt text"></p>
<p><img src="/2023/12/07/pai9/image-100.png" alt="Alt text"></p>
<p><img src="/2023/12/07/pai9/image-101.png" alt="Alt text"></p>
<p><img src="/2023/12/07/pai9/image-102.png" alt="Alt text"></p>
<p>Typically, policy gradient methods are slow due to the large variance in the score gradient estimates. Because of this, they need to take small steps and require many rollouts of a Markov chain. Moreover, we cannot reuse data from previous rollouts, as policy gradient methods are fundamentally on-policy.</p>
<h2><span id="actor-critic-methods">Actor-Critic Methods</span></h2><p>Actor-Critic methods reduce the variance of policy gradient estimates by using ideas from value function approximation. They use function approximation both to approximate value functions and to approximate policies.</p>
<h3><span id="advantage-function">Advantage Function</span></h3><p><img src="/2023/12/07/pai9/image-103.png" alt="Alt text"></p>
<p>Intuitively, the advantage function is a shifted version of the state-action function q that is relative to 0. It turns out that using this quantity instead, has numerical advantages.</p>
<h3><span id="policy-gradient-theorem">Policy Gradient Theorem</span></h3><p><img src="/2023/12/07/pai9/image-105.png" alt="Alt text"></p>
<p><img src="/2023/12/07/pai9/image-104.png" alt="Alt text"></p>
<p>Intuitively, ρθ(x) measures how often we visit state x when following policy πθ. It can be thought of as a “discounted frequency”. Importantly, ρθ is not a probability distribution, as it is not normalized to integrate to 1. Instead, ρθ is what is often called a finite measure. Therefore, eq. (12.57) is not a real expectation!</p>
<h3><span id="on-policy-actor-critics">On-policy Actor-Critics</span></h3><p><img src="/2023/12/07/pai9/image-106.png" alt="Alt text"></p>
<h4><span id="oac">OAC</span></h4><p><img src="/2023/12/07/pai9/image-107.png" alt="Alt text"><br>Due to the use of TD-learning for learning the critic, this algorithm is fundamentally on-policy. </p>
<h4><span id="a2c">A2C</span></h4><p><img src="/2023/12/07/pai9/image-108.png" alt="Alt text"></p>
<p>that the Q-function is an absolute quantity, whereas the advantage function is a relative quantity, where the sign is informative for the gradient direction. Intuitively, an absolute value is harder to estimate than the sign. Actor-Critic methods are therefore often implemented with respect to the advantage function rather than the Q-function. </p>
<h4><span id="gaegaae">GAE/GAAE</span></h4><p>Taking a step back, observe that the policy gradient methods such as REINFORCE generally have high variance in their gradient estimates. However, due to using Monte Carlo estimates of Gt, the gradient estimates are unbiased. In contrast, using a bootstrapped Q-function to obtain gradient estimates yields estimates with a smaller variance, but those estimates are biased. We are therefore faced with a bias-variance tradeoff. A natural approach is therefore to blend both gradient estimates to allow for effectively trading bias and variance. This leads to algorithms such as generalized advantage estimation (GAE/GAAE). </p>
<h4><span id="improving-sample-efficiencytrpoppo">Improving sample efficiency(TRPO/PPO)</span></h4><p>Actor-Critic methods generally suffer from low sample efficiency. One well-known variant that slightly improves the sample efficiency is trust-region policy optimization (TRPO). </p>
<p><img src="/2023/12/07/pai9/image-109.png" alt="Alt text"><br>Intuitively, taking the expectation with respect to the previous policy πθk , means that we can reuse data from rollouts within the same iteration. TRPO allows reusing past data as long as it can still be “trusted”. This makes TRPO “somewhat” off-policy. Fundamentally, though, TRPO is still an on-policy method. Proximal policy optimization (PPO) is a heuristic variant of TRPO that often works well in practice.</p>
<h3><span id="off-policy-actor-critics">Off-policy Actor-Critics</span></h3><p>These algorithms use the reparameterization gradient estimates, instead of score gradient estimators. </p>
<h4><span id="ddpg">DDPG</span></h4><p>As our method is off-policy, a simple idea in continuous action spaces is to add Gaussian noise to the action selected by πθ — also known as Gaussian noise “dithering”. This corresponds to an algorithm called deep deterministic policy gradients.<br>This algorithm is essentially equivalent to Q-learning with function approximation (e.g., DQN), with the only exception that we replace the maximization over actions with the learned policy πθ.<br><img src="/2023/12/07/pai9/image-110.png" alt="Alt text"></p>
<p>Twin delayed DDPG (TD3) is an extension of DDPG that uses two separate critic networks for predicting the maximum action and evaluating the policy. This addresses the maximization bias akin to Double-DQN. TD3 also applies delayed updates to the actor network, which increases stability.</p>
<h3><span id="off-policy-actor-critics-with-randomized-policies">Off-Policy Actor Critics with Randomized Policies</span></h3><p><img src="/2023/12/07/pai9/image-111.png" alt="Alt text"></p>
<p>The algorithm that uses eq. (12.81) to obtain gradients for the critic and reparameterization gradients for the actor is called stochastic value gradients (SVG).</p>
<h3><span id="off-policy-actor-critics-with-entropy-regularization">Off-policy Actor-Critics with Entropy Regularization</span></h3><p>In practice, algorithms like SVG often do not explore enough. A key issue with relying on randomized policies for exploration is that they might collapse to deterministic policies. </p>
<p>A simple trick that encourages a little bit of extra exploration is to regularize the randomized policies “away” from putting all mass on a single action. This approach is known as entropy regularization and it leads to an analogue of Markov decision processes called entropy-regularized Markov decision process, where suitably defined regularized state-action value functions (so-called soft value functions) are used.</p>
<h4><span id="soft-actor-criticsac">soft actor critic(SAC)</span></h4><p><img src="/2023/12/07/pai9/image-81.png" alt="Alt text"></p>
<h2><span id="references">references</span></h2><p><a target="_blank" rel="noopener" href="https://towardsdatascience.com/soft-actor-critic-demystified-b8427df61665">https://towardsdatascience.com/soft-actor-critic-demystified-b8427df61665</a><br><a target="_blank" rel="noopener" href="https://github.com/tsmatz/reinforcement-learning-tutorials/blob/master/06-sac.ipynb">https://github.com/tsmatz/reinforcement-learning-tutorials/blob/master/06-sac.ipynb</a><br><a target="_blank" rel="noopener" href="https://spinningup.openai.com/en/latest/algorithms/sac.html">https://spinningup.openai.com/en/latest/algorithms/sac.html</a><br><a target="_blank" rel="noopener" href="https://towardsdatascience.com/policy-gradients-in-a-nutshell-8b72f9743c5d">https://towardsdatascience.com/policy-gradients-in-a-nutshell-8b72f9743c5d</a><br><a target="_blank" rel="noopener" href="https://lilianweng.github.io/posts/2018-04-08-policy-gradient/">https://lilianweng.github.io/posts/2018-04-08-policy-gradient/</a></p>
</div><div class="article-licensing box"><div class="licensing-title"><p>pai - Model-free Approximate Reinforcement Learning</p><p><a href="http://yoursite.com/2023/12/07/pai9/">http://yoursite.com/2023/12/07/pai9/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>s-serenity</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2023-12-07</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2024-02-21</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/machine-learning/">machine learning</a></div><div class="notification is-danger">You need to set <code>install_url</code> to use ShareThis. Please set it in <code>_config.yml</code>.</div></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button donate" href="/" target="_blank" rel="noopener" data-type="afdian"><span class="icon is-small"><i class="fas fa-charging-station"></i></span><span>Afdian.net</span></a><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>Alipay</span><span class="qrcode"><img src="/" alt="Alipay"></span></a><a class="button donate" href="/" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>Buy me a coffee</span></a><a class="button donate" href="/" target="_blank" rel="noopener" data-type="patreon"><span class="icon is-small"><i class="fab fa-patreon"></i></span><span>Patreon</span></a><div class="notification is-danger">You forgot to set the <code>business</code> or <code>currency_code</code> for Paypal. Please set it in <code>_config.yml</code>.</div><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>Wechat</span><span class="qrcode"><img src="/" alt="Wechat"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2023/12/07/pai10/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">pai - Model-based Approximate Reinforcement Learning</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2023/11/28/bigdata10/"><span class="level-item">bigdata - JSONiq</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div class="notification is-danger">You forgot to set the <code>shortname</code> for Disqus. Please set it in <code>_config.yml</code>.</div></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/tx.png" alt="s-serenity"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">s-serenity</p><p class="is-size-6 is-block">student</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Shanghai, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">75</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">9</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">21</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/s-serenity" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/s-serenity"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://github.com/s-serenity" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/algorithm/"><span class="level-start"><span class="level-item">algorithm</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/computer-system/"><span class="level-start"><span class="level-item">computer system</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/database/"><span class="level-start"><span class="level-item">database</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/development-tools/"><span class="level-start"><span class="level-item">development tools</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/others/"><span class="level-start"><span class="level-item">others</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/practice/"><span class="level-start"><span class="level-item">practice</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/programming-language/"><span class="level-start"><span class="level-item">programming language</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/theory/"><span class="level-start"><span class="level-item">theory</span></span><span class="level-end"><span class="level-item tag">20</span></span></a></li><li><a class="level is-mobile" href="/categories/web-development/"><span class="level-start"><span class="level-item">web development</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-03-29T12:00:15.000Z">2024-03-29</time></p><p class="title"><a href="/2024/03/29/llm1/">Sampling</a></p><p class="categories"><a href="/categories/theory/">theory</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-02-21T11:25:29.000Z">2024-02-21</time></p><p class="title"><a href="/2024/02/21/llm0/">Measuring sentence similarity</a></p><p class="categories"><a href="/categories/theory/">theory</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-01-25T16:14:35.000Z">2024-01-25</time></p><p class="title"><a href="/2024/01/25/pai11/">pai - review notes</a></p><p class="categories"><a href="/categories/theory/">theory</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-01-03T15:06:23.000Z">2024-01-03</time></p><p class="title"><a href="/2024/01/03/bigdata13/">bigdata - review notes</a></p><p class="categories"><a href="/categories/database/">database</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-12-12T14:22:00.000Z">2023-12-12</time></p><p class="title"><a href="/2023/12/12/bigdata12/">bigdata - Cube Data</a></p><p class="categories"><a href="/categories/database/">database</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2024/"><span class="level-start"><span class="level-item">2024</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/"><span class="level-start"><span class="level-item">2023</span></span><span class="level-end"><span class="level-item tag">43</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/"><span class="level-start"><span class="level-item">2022</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/"><span class="level-start"><span class="level-item">2021</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/"><span class="level-start"><span class="level-item">2020</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/"><span class="level-start"><span class="level-item">2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/"><span class="level-start"><span class="level-item">2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Algorithm/"><span class="tag">Algorithm</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Elasticsearch/"><span class="tag">Elasticsearch</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux/"><span class="tag">Linux</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Markdown/"><span class="tag">Markdown</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SQL/"><span class="tag">SQL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/algorithm/"><span class="tag">algorithm</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/big-data/"><span class="tag">big data</span><span class="tag">13</span></a></div><div class="control"><a class="tags has-addons" href="/tags/computer-vision/"><span class="tag">computer vision</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/deep-learning/"><span class="tag">deep learning</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/git/"><span class="tag">git</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/graph/"><span class="tag">graph</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/hexo/"><span class="tag">hexo</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/java/"><span class="tag">java</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/machine-learning/"><span class="tag">machine learning</span><span class="tag">18</span></a></div><div class="control"><a class="tags has-addons" href="/tags/probability/"><span class="tag">probability</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python/"><span class="tag">python</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/reinforcement-learning/"><span class="tag">reinforcement learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/speech/"><span class="tag">speech</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/tools/"><span class="tag">tools</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="s-serenity" height="28"></a><p class="is-size-7"><span>&copy; 2024 s-serenity</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2019</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>