<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="robots" content="noindex"><meta><title>Tag: NLP - s-serenity</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="s-serenity"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="s-serenity"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Recording and sharing my learning process."><meta property="og:type" content="blog"><meta property="og:title" content="s-serenity"><meta property="og:url" content="http://yoursite.com/"><meta property="og:site_name" content="s-serenity"><meta property="og:description" content="Recording and sharing my learning process."><meta property="og:locale" content="en_US"><meta property="og:image" content="http://yoursite.com/img/og_image.png"><meta property="article:author" content="s-serenity"><meta property="article:tag" content="Algorithm study development"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://yoursite.com/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://yoursite.com"},"headline":"s-serenity","image":["http://yoursite.com/img/og_image.png"],"author":{"@type":"Person","name":"s-serenity"},"publisher":{"@type":"Organization","name":"s-serenity","logo":{"@type":"ImageObject","url":"http://yoursite.com/img/logo.svg"}},"description":"Recording and sharing my learning process."}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.1.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="s-serenity" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/tags">Tags</a></li><li class="is-active"><a href="#" aria-current="page">NLP</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-10-24T02:36:56.000Z" title="2024-10-24 10:36:56 ├F10: AM┤">2024-10-24</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-10-25T02:04:20.761Z" title="2024-10-25 10:04:20 ├F10: AM┤">2024-10-25</time></span><span class="level-item"><a class="link-muted" href="/categories/theory/">theory</a></span><span class="level-item">12 minutes read (About 1726 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/10/24/nlp0/">词嵌入</a></p><div class="content"><h2><span id="word2vec">word2vec</span></h2><p>word2vec工具包含两个模型，即跳元模型（skip-gram）和连续词袋（CBOW （Continuous Bag-of-Words）。Skip-gram是给定一个中心词，预测其周围的上下文词。CBOW是给定一段文本中的一个中心词周围的上下文词，预测中心词。</p>
<h3><span id="负采样">负采样</span></h3><p>在标准的 softmax 方法中，我们有</p>
<script type="math/tex; mode=display">
P(w_o \mid w_c) = \frac{\exp(\mathbf{u}_o^\top \mathbf{v}_c)}{ \sum_{i \in \mathcal{V}} \exp(\mathbf{u}_i^\top \mathbf{v}_c)},</script><p>这涉及到计算所有词汇表中词汇的条件概率, 在词汇表非常大的时候是非常耗时的。</p>
<p>负采样的核心思想是通过随机选择一部分非上下文词（负样本）来近似这个条件概率分布。具体来说，对于每个中心词和对应的上下文词，我们不仅更新这两个词的向量表示，而且还随机选择若干个不在上下文中的词作为“负样本”，并更新它们的向量表示。在负采样中，我们采用一个简化的逻辑回归目标函数来替代标准的 softmax 函数。对于每个训练样本,我们希望模型能够正确区分真正的上下文词和负样本词。因此，我们可以定义一个目标函数，它希望对于上下文词，有$\sigma(\u_o\v_i+b)=1$而对于每个负样本，有$\sigma(\u_o\v_i+b)=0$。 通过这种方式，负采样实际上是在模拟原始 softmax 的行为，但它只考虑了一小部分词汇，即一个正样本加上几个负样本。这意味着负采样通过一系列独立的二元分类任务来近似这个条件概率。</p>
<h3><span id="层次softmax">层次softmax</span></h3><p>层次softmax 的核心思想是构建一个词汇的层次结构，通常是一棵哈夫曼树（Huffman Tree），并将分类问题转化为沿着树的路径进行的一系列二元分类问题。这样可以将原始的 softmax 层的计算复杂度从线性减少到对数级别。层次softmax 的目标是通过计算从根节点到词汇的叶子节点的路径概率来近似原条件概率。</p>
<h2><span id="glove">GloVe</span></h2><p>GloVe模型的主要特点是它试图捕捉单词之间的共现频率信息，即一个单词出现时另一个单词出现的概率。通过这种方式，模型可以学习到词汇语义以及词汇间的关系，比如同义词、反义词、以及类比关系等。</p>
<p>训练GloVe模型涉及到最小化词对共现概率预测值与实际共现概率之间的损失函数。</p>
<h2><span id="子词嵌入">子词嵌入</span></h2><h3><span id="fasttext">fastText</span></h3><p>在跳元模型和连续词袋模型中，同一词的不同变形形式直接由不同的向量表示，不需要共享参数。为了使用形态信息，fastText模型提出了一种子词嵌入方法，其中子词是一个字符n-gram。fastText可以被认为是子词级跳元模型，而非学习词级向量表示，其中每个中心词由其子词级向量之和表示。</p>
<h3><span id="字节对编码byte-pair-encoding">字节对编码（Byte Pair Encoding）</span></h3><p>字节对编码（Byte Pair Encoding，简称 BPE）是一种用于词汇归一化和文本压缩的技术，近年来，BPE 被重新引入到自然语言处理领域，特别是在机器翻译和语言建模中，作为一种生成子词单位的有效方法。BPE 的基本思想是不断地合并最常出现的相邻字符对，直到达到预定的词汇表大小。</p>
<h2><span id="elmo">ELMo</span></h2><p>word2vec和GloVe都将相同的预训练向量分配给同一个词，而不考虑词的上下文，考虑到自然语言中丰富的多义现象和复杂的语义，上下文无关表示具有明显的局限性，同一个词可以根据上下文被赋予不同的表示。</p>
<p>ELMo（Embeddings from Language Models）是一种上下文敏感的词嵌入方法，使用双向 LSTM（长短期记忆网络）构建深度语言模型，这种模型可以捕获来自句子左右两边的信息。通过训练这种模型，可以得到一个对上下文敏感的词嵌入。</p>
<h2><span id="gpt">GPT</span></h2><p>初代 GPT 基于 Transformer 架构，使用的是单向的 Transformer，这意味着它在生成文本时只能访问之前的位置信息，而不能访问当前位置之后的信息。通过自回归方式训练，即模型学习给定前面的文字后预测下一个文字。</p>
<h2><span id="bert">BERT</span></h2><p>ELMo对上下文进行双向编码，但使用特定于任务的架构；而GPT是任务无关的，但是从左到右编码上下文，BERT结合了这两个方面的优点。</p>
<p>BERT输入序列明确地表示单个文本和文本对。当输入为单个文本时，BERT输入序列是特殊类别词元“\<cls\>”、文本序列的标记、以及特殊分隔词元“\<sep\>”的连结。当输入为文本对时，BERT输入序列是“\<cls\>”、第一个文本序列的标记、“\<sep\>”、第二个文本序列标记、以及“\<sep\>”的连结。</sep\></sep\></cls\></sep\></cls\></p>
<p>为了双向编码上下文以表示每个词元，BERT随机掩蔽词元并使用来自双向上下文的词元以自监督的方式预测掩蔽词元。此任务称为掩蔽语言模型。</p>
<p>尽管掩蔽语言建模能够编码双向上下文来表示单词，但它不能显式地建模文本对之间的逻辑关系。为了帮助理解两个文本序列之间的关系，BERT在预训练中考虑了一个二元分类任务——下一句预测。在为预训练生成句子对时，有一半的时间它们确实是标签为“真”的连续句子；在另一半的时间里，第二个句子是从语料库中随机抽取的，标记为“假”。</p>
<p>在预训练BERT时，最终的损失函数是掩蔽语言模型损失函数和下一句预测损失函数的线性组合。</p>
<h2><span id="参考文献">参考文献</span></h2><p><a target="_blank" rel="noopener" href="https://zh.d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html">https://zh.d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html</a></p>
<p><a target="_blank" rel="noopener" href="https://zh.d2l.ai/chapter_natural-language-processing-pretraining/subword-embedding.html">https://zh.d2l.ai/chapter_natural-language-processing-pretraining/subword-embedding.html</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-10-10T08:27:35.000Z" title="2024-10-10 4:27:35 ├F10: PM┤">2024-10-10</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-10-11T12:17:39.987Z" title="2024-10-11 8:17:39 ├F10: PM┤">2024-10-11</time></span><span class="level-item"><a class="link-muted" href="/categories/theory/">theory</a></span><span class="level-item">a few seconds read (About 27 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/10/10/llm8/">perplexity</a></p><div class="content"><h2><span id="references">references</span></h2><p><a target="_blank" rel="noopener" href="https://thegradient.pub/understanding-evaluation-metrics-for-language-models/">https://thegradient.pub/understanding-evaluation-metrics-for-language-models/</a><br><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/perplexity">https://huggingface.co/docs/transformers/perplexity</a><br><a target="_blank" rel="noopener" href="https://blog.uptrain.ai/decoding-perplexity-and-its-significance-in-llms/">https://blog.uptrain.ai/decoding-perplexity-and-its-significance-in-llms/</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-07-04T07:14:44.000Z" title="2024-7-4 3:14:44 ├F10: PM┤">2024-07-04</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-07-04T13:15:33.232Z" title="2024-7-4 9:15:33 ├F10: PM┤">2024-07-04</time></span><span class="level-item"><a class="link-muted" href="/categories/theory/">theory</a></span><span class="level-item">a few seconds read (About 10 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/07/04/llm7/">Important papers</a></p><div class="content"><h2><span id="references">references</span></h2><p><a target="_blank" rel="noopener" href="https://magazine.sebastianraschka.com/p/understanding-large-language-models">https://magazine.sebastianraschka.com/p/understanding-large-language-models</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-07-04T06:57:56.000Z" title="2024-7-4 2:57:56 ├F10: PM┤">2024-07-04</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-07-04T13:34:08.076Z" title="2024-7-4 9:34:08 ├F10: PM┤">2024-07-04</time></span><span class="level-item"><a class="link-muted" href="/categories/theory/">theory</a></span><span class="level-item">3 minutes read (About 431 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/07/04/llm6/">finetuning large language models</a></p><div class="content"><h2><span id="the-3-conventional-feature-based-and-finetuning-approaches">The 3 Conventional Feature-Based and Finetuning Approaches</span></h2><h3><span id="feature-based-approach">Feature-Based Approach</span></h3><p>In the feature-based approach, we load a pretrained LLM and apply it to our target dataset. Here, we are particularly interested in generating the output embeddings for the training set, which we can use as input features to train a classification model.</p>
<h3><span id="finetuning-i-updating-the-output-layers">Finetuning I – Updating The Output Layers</span></h3><p>A popular approach related to the feature-based approach described above is finetuning the output layers (we will refer to this approach as finetuning I). Similar to the feature-based approach, we keep the parameters of the pretrained LLM frozen. We only train the newly added output layers.</p>
<h3><span id="finetuning-ii-updating-all-layers">Finetuning II – Updating All Layers</span></h3><p>when optimizing the modeling performance, the gold standard for using pretrained LLMs is to update all layers.</p>
<h2><span id="parameter-efficient-finetuning-techniques-peft">parameter-efficient finetuning techniques (PEFT)</span></h2><p>To finetune LLM with high modeling performance while only requiring the training of only a small number of parameters. These methods are usually referred to as parameter-efficient finetuning techniques (PEFT). Techniques such as prefix tuning, adapters, and low-rank adaptation, all of which “modify” multiple layers, achieve much better predictive performance (at a low cost).</p>
<h2><span id="reinforcement-learning-with-human-feedback-rlhf">Reinforcement Learning with Human Feedback (RLHF)</span></h2><p>In RLHF, human feedback is collected by having humans rank or rate different model outputs, providing a reward signal. The collected reward labels can then be used to train a reward model that is then in turn used to guide the LLMs adaptation to human preferences.</p>
<p>The reward model itself is learned via supervised learning (typically using a pretrained LLM as base model). Next, the reward model is used to update the pretrained LLM that is to be adapted to human preferences — the training uses a flavor of reinforcement learning called proximal policy optimization.</p>
<h2><span id="prompt-tuning">prompt tuning</span></h2><p>In a nutshell, prompt tuning (different from prompting) appends a tensor to the embedded inputs of a pretrained LLM. The tensor is then tuned to optimize a loss function for the finetuning task and data while all other parameters in the LLM remain frozen. </p>
<p>The main idea behind prompt tuning, and parameter-efficient finetuning methods in general, is to add a small number of new parameters to a pretrained LLM and only finetune the newly added parameters to make the LLM perform better on (a) a target dataset (for example, a domain-specific dataset like medical or legal documents) and (b) a target task (for example, sentiment classification).</p>
<h2><span id="references">references</span></h2><p><a target="_blank" rel="noopener" href="https://magazine.sebastianraschka.com/p/finetuning-large-language-models">https://magazine.sebastianraschka.com/p/finetuning-large-language-models</a><br><a target="_blank" rel="noopener" href="https://magazine.sebastianraschka.com/p/understanding-parameter-efficient">https://magazine.sebastianraschka.com/p/understanding-parameter-efficient</a><br><a target="_blank" rel="noopener" href="https://magazine.sebastianraschka.com/p/finetuning-llms-with-adapters">https://magazine.sebastianraschka.com/p/finetuning-llms-with-adapters</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-05-18T09:08:37.000Z" title="2024-5-18 5:08:37 ├F10: PM┤">2024-05-18</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-05-18T17:34:57.680Z" title="2024-5-19 1:34:57 ├F10: AM┤">2024-05-19</time></span><span class="level-item"><a class="link-muted" href="/categories/theory/">theory</a></span><span class="level-item">a minute read (About 114 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/05/18/llm5/">Batch processing for sequences</a></p><div class="content"><h2><span id="padding">padding</span></h2><p>In natural language processing (NLP), padding refers to the practice of adding special tokens to sequences (such as sentences or texts) so that all sequences in a batch have the same length. Padding is essential when working with mini-batch processing in neural networks because it ensures that all sequences in a batch can be processed simultaneously, despite their varying lengths.</p>
<h2><span id="attention-masks">Attention masks</span></h2><p>Attention masks are tensors with the exact same shape as the input IDs tensor, filled with 0s and 1s: 1s indicate the corresponding tokens should be attended to, and 0s indicate the corresponding tokens should not be attended to. </p>
<h2><span id="references">references</span></h2><p><a target="_blank" rel="noopener" href="https://huggingface.co/learn/nlp-course/en/chapter2/5?fw=pt">https://huggingface.co/learn/nlp-course/en/chapter2/5?fw=pt</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-05-18T07:17:07.000Z" title="2024-5-18 3:17:07 ├F10: PM┤">2024-05-18</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-05-18T15:00:50.634Z" title="2024-5-18 11:00:50 ├F10: PM┤">2024-05-18</time></span><span class="level-item"><a class="link-muted" href="/categories/theory/">theory</a></span><span class="level-item">2 minutes read (About 296 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/05/18/llm4/">Tokenizers</a></p><div class="content"><h2><span id="what-is-tokenizer">what is tokenizer</span></h2><p>A tokenizer is a crucial component in natural language processing (NLP) and text analysis that breaks down text into smaller, manageable units called tokens. These tokens can be words, phrases, symbols, or other meaningful elements depending on the specific requirements of the application. </p>
<h2><span id="how-tokenizer-works">how tokenizer works</span></h2><p>There are different types of tokenizer methods.Whitespace Tokenizers, Punctuation-Based Tokenizers,  Word Tokenizers,Sentence Tokenizers,Character Tokenizers, N-gram Tokenizers, Regular Expression Tokenizers and<br>Subword Tokenizers.</p>
<h3><span id="word-tokenizers">Word Tokenizers</span></h3><p>Word tokenization, also known as lexical analysis, is the process of splitting a piece of text into individual words or tokens. Word tokenization typically involves breaking the text into words based on spaces and punctuation. </p>
<h3><span id="subword-tokenizers">Subword Tokenizers</span></h3><p>Subword tokenization algorithms rely on the principle that frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords. A subword tokenizer is a type of tokenizer used in natural language processing (NLP) that breaks down words into smaller units or subwords. This approach is particularly useful for handling rare or out-of-vocabulary words, reducing the vocabulary size, and improving the efficiency of language models.</p>
<h4><span id="common-subword-tokenization-methods">Common Subword Tokenization Methods</span></h4><h5><span id="byte-pair-encoding-bpe">Byte-Pair Encoding (BPE)</span></h5><p>BPE is an iterative algorithm that merges the most frequent pairs of characters or subwords in a corpus until a desired vocabulary size is reached.</p>
<h5><span id="wordpiece-tokenization">WordPiece Tokenization</span></h5><p>Similar to BPE, WordPiece builds a vocabulary of subwords based on frequency, optimizing for a balance between vocabulary size and the ability to handle rare words.</p>
<h5><span id="sentencepiece">SentencePiece</span></h5><p>SentencePiece is an unsupervised text tokenizer and detokenizer mainly designed for Neural Network-based text generation systems. It treats the input text as a sequence of Unicode characters and uses a subword model to create subwords.</p>
<h2><span id="references">references</span></h2><p><a target="_blank" rel="noopener" href="https://huggingface.co/learn/nlp-course/en/chapter2/4">https://huggingface.co/learn/nlp-course/en/chapter2/4</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-05-01T07:56:46.000Z" title="2024-5-1 3:56:46 ├F10: PM┤">2024-05-01</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-05-01T14:19:57.133Z" title="2024-5-1 10:19:57 ├F10: PM┤">2024-05-01</time></span><span class="level-item"><a class="link-muted" href="/categories/practice/">practice</a></span><span class="level-item">a few seconds read (About 37 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/05/01/llm3/">Natural Language Inference(Recognizing Textual Entailment)</a></p><div class="content"><h1><span id="definition">definition</span></h1><p>Natural language inference (NLI) is the task of determining whether a “hypothesis” is true (entailment), false (contradiction), or undetermined (neutral) given a “premise”.</p>
<h2><span id="benchmarks">benchmarks</span></h2><p>Benchmark datasets used for NLI include SNLI, MultiNLI, SciTail, SuperGLUE, RTE, WNLI.</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-04-02T05:52:28.000Z" title="2024-4-2 1:52:28 ├F10: PM┤">2024-04-02</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-05-01T14:05:06.884Z" title="2024-5-1 10:05:06 ├F10: PM┤">2024-05-01</time></span><span class="level-item"><a class="link-muted" href="/categories/practice/">practice</a></span><span class="level-item">a few seconds read (About 49 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/04/02/llm2/">Problems record of using OpenAI&#039;s API</a></p><div class="content"><h1><span id="gpt">GPT</span></h1><h2><span id="gpt-35-turbo-instruct-generates-empty-text-after-calling-several-times">gpt-3.5-turbo-instruct generates empty text after calling several times.</span></h2><p>Tried adding space or adding newline, but didn’t work.</p>
<h2><span id="gpt-35-turbo-1106-generates-different-results-from-same-prompt-even-though-t-is-set-as-0">gpt-3.5-turbo-1106 generates different results from same prompt even though T is set as 0.</span></h2><p>Tried setting seed but did’t work. Switched to another version mitigated this problem.</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-03-29T05:00:15.000Z" title="2024-3-29 1:00:15 ├F10: PM┤">2024-03-29</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-03-29T14:40:00.506Z" title="2024-3-29 10:40:00 ├F10: PM┤">2024-03-29</time></span><span class="level-item"><a class="link-muted" href="/categories/theory/">theory</a></span><span class="level-item">a few seconds read (About 108 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/03/29/llm1/">Sampling</a></p><div class="content"><h1><span id="sampling">Sampling</span></h1><h2><span id="top-p-sampling">top-p sampling</span></h2><p>This method only considers the tokens whose cumulative probability exceed the probability p and then redistributes the probability mass across the remaining tokens so that the sum of probabilities is 1. </p>
<h2><span id="temperature">temperature</span></h2><p>What the temperature does is: it controls the relative weights in the probability distribution. It controls the extent to which differences in probability play a role in the sampling. At temperature t=0 this sampling technique turns into what we call greedy search/argmax sampling where the token with the highest probability is always selected. </p>
<h2><span id="reference">reference</span></h2><p><a target="_blank" rel="noopener" href="https://blog.ml6.eu/why-openais-api-models-cannot-be-forced-to-behave-fully-deterministically-4934a7e8f184">https://blog.ml6.eu/why-openais-api-models-cannot-be-forced-to-behave-fully-deterministically-4934a7e8f184</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-02-21T04:25:29.000Z" title="2024-2-21 12:25:29 ├F10: PM┤">2024-02-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-10-29T05:32:43.130Z" title="2024-10-29 1:32:43 ├F10: PM┤">2024-10-29</time></span><span class="level-item"><a class="link-muted" href="/categories/theory/">theory</a></span><span class="level-item">2 minutes read (About 234 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/02/21/nlp1/">Measuring sentence similarity</a></p><div class="content"><h2><span id="metrics">metrics</span></h2><h3><span id="bleu-bilingual-evaluation-understudy">BLEU (Bilingual Evaluation Understudy)</span></h3><p>BLEU computes a score based on the n-gram overlap between the generated text and the reference text, as well as the brevity penalty to handle cases where the generated text is too short. The score ranges from 0 to 1, where 1 indicates a perfect match with the reference translations.</p>
<h3><span id="rouge-recall-oriented-understudy-for-gisting-evaluation">ROUGE (Recall-Oriented Understudy for Gisting Evaluation)</span></h3><p>ROUGE score measures the similarity between the machine-generated summary and the reference summaries using overlapping n-grams, word sequences that appear in both the machine-generated summary and the reference summaries. ROUGE score ranges from 0 to 1, with higher values indicating better summary quality. </p>
<p>ROUGE scores are branched into ROUGE-N,ROUGE-L, and ROUGE-S.<br>ROUGE-N measures the overlap of n-grams (contiguous sequences of n words) between the candidate text and the reference text. It computes the precision, recall, and F1-score based on the n-gram overlap.<br>ROUGE-L measures the longest common subsequence (LCS) between the candidate text and the reference text. It computes the precision, recall, and F1-score based on the length of the LCS.<br>ROUGE-S measures the skip-bigram (bi-gram with at most one intervening word) overlap between the candidate text and the reference text. It computes the precision, recall, and F1-score based on the skip-bigram overlap. </p>
<h2><span id="references">references</span></h2><p><a target="_blank" rel="noopener" href="https://medium.com/@sthanikamsanthosh1994/understanding-bleu-and-rouge-score-for-nlp-evaluation-1ab334ecadcb">https://medium.com/@sthanikamsanthosh1994/understanding-bleu-and-rouge-score-for-nlp-evaluation-1ab334ecadcb</a></p>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous is-invisible is-hidden-mobile"><a href="/tags/NLP/page/0/">Previous</a></div><div class="pagination-next"><a href="/tags/NLP/page/2/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link is-current" href="/tags/NLP/">1</a></li><li><a class="pagination-link" href="/tags/NLP/page/2/">2</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/tx.PNG" alt="s-serenity"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">s-serenity</p><p class="is-size-6 is-block">student</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Shanghai, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">103</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">12</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">28</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/s-serenity" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/s-serenity"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://github.com/s-serenity" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/LLM/"><span class="level-start"><span class="level-item">LLM</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/algorithm/"><span class="level-start"><span class="level-item">algorithm</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/computer-science/"><span class="level-start"><span class="level-item">computer science</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/computer-system/"><span class="level-start"><span class="level-item">computer system</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/database/"><span class="level-start"><span class="level-item">database</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/categories/development-tools/"><span class="level-start"><span class="level-item">development tools</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/engineering/"><span class="level-start"><span class="level-item">engineering</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/others/"><span class="level-start"><span class="level-item">others</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/practice/"><span class="level-start"><span class="level-item">practice</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/programming-language/"><span class="level-start"><span class="level-item">programming language</span></span><span class="level-end"><span class="level-item tag">8</span></span></a></li><li><a class="level is-mobile" href="/categories/theory/"><span class="level-start"><span class="level-item">theory</span></span><span class="level-end"><span class="level-item tag">28</span></span></a></li><li><a class="level is-mobile" href="/categories/web-development/"><span class="level-start"><span class="level-item">web development</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-11-07T09:03:42.000Z">2024-11-07</time></p><p class="title"><a href="/2024/11/07/SQL1/">SQL(2) - 开窗函数</a></p><p class="categories"><a href="/categories/database/">database</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-11-06T03:42:14.000Z">2024-11-06</time></p><p class="title"><a href="/2024/11/06/ir0/">信息检索（1）- BM25</a></p><p class="categories"><a href="/categories/algorithm/">algorithm</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-11-05T09:20:14.000Z">2024-11-05</time></p><p class="title"><a href="/2024/11/05/web0/">Web(1) - Cookie 和 Session</a></p><p class="categories"><a href="/categories/web-development/">web development</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-11-05T09:16:04.000Z">2024-11-05</time></p><p class="title"><a href="/2024/11/05/cpp1/">C++（2）- 静态成员函数</a></p><p class="categories"><a href="/categories/programming-language/">programming language</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-11-05T09:07:48.000Z">2024-11-05</time></p><p class="title"><a href="/2024/11/05/cpp0/">C++（1）- 虚函数</a></p><p class="categories"><a href="/categories/programming-language/">programming language</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2024/"><span class="level-start"><span class="level-item">2024</span></span><span class="level-end"><span class="level-item tag">32</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/"><span class="level-start"><span class="level-item">2023</span></span><span class="level-end"><span class="level-item tag">43</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/"><span class="level-start"><span class="level-item">2022</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/"><span class="level-start"><span class="level-item">2021</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/"><span class="level-start"><span class="level-item">2020</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/"><span class="level-start"><span class="level-item">2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/"><span class="level-start"><span class="level-item">2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Algorithm/"><span class="tag">Algorithm</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C++</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Elasticsearch/"><span class="tag">Elasticsearch</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LLM/"><span class="tag">LLM</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux/"><span class="tag">Linux</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Markdown/"><span class="tag">Markdown</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Math/"><span class="tag">Math</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">12</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Recommender-System/"><span class="tag">Recommender System</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SQL/"><span class="tag">SQL</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/algorithm/"><span class="tag">algorithm</span><span class="tag">8</span></a></div><div class="control"><a class="tags has-addons" href="/tags/big-data/"><span class="tag">big data</span><span class="tag">13</span></a></div><div class="control"><a class="tags has-addons" href="/tags/computer-network/"><span class="tag">computer network</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/computer-vision/"><span class="tag">computer vision</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/deep-learning/"><span class="tag">deep learning</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/git/"><span class="tag">git</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/graph/"><span class="tag">graph</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/hexo/"><span class="tag">hexo</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/information-retrieval/"><span class="tag">information retrieval</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/java/"><span class="tag">java</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/machine-learning/"><span class="tag">machine learning</span><span class="tag">18</span></a></div><div class="control"><a class="tags has-addons" href="/tags/operating-system/"><span class="tag">operating system</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python/"><span class="tag">python</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/reinforcement-learning/"><span class="tag">reinforcement learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/speech/"><span class="tag">speech</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/tools/"><span class="tag">tools</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/web/"><span class="tag">web</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="s-serenity" height="28"></a><p class="is-size-7"><span>&copy; 2024 s-serenity</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2019</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>