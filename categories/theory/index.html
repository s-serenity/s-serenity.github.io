<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="robots" content="noindex"><meta><title>Category: theory - s-serenity</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="s-serenity"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="s-serenity"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Recording and sharing my learning process."><meta property="og:type" content="blog"><meta property="og:title" content="s-serenity"><meta property="og:url" content="http://yoursite.com/"><meta property="og:site_name" content="s-serenity"><meta property="og:description" content="Recording and sharing my learning process."><meta property="og:locale" content="en_US"><meta property="og:image" content="http://yoursite.com/img/og_image.png"><meta property="article:author" content="s-serenity"><meta property="article:tag" content="Algorithm study development"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://yoursite.com/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://yoursite.com"},"headline":"s-serenity","image":["http://yoursite.com/img/og_image.png"],"author":{"@type":"Person","name":"s-serenity"},"publisher":{"@type":"Organization","name":"s-serenity","logo":{"@type":"ImageObject","url":"http://yoursite.com/img/logo.svg"}},"description":"Recording and sharing my learning process."}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.1.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="s-serenity" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">Categories</a></li><li class="is-active"><a href="#" aria-current="page">theory</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-10-24T02:36:56.000Z" title="2024-10-24 10:36:56 ├F10: AM┤">2024-10-24</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-10-25T02:04:20.761Z" title="2024-10-25 10:04:20 ├F10: AM┤">2024-10-25</time></span><span class="level-item"><a class="link-muted" href="/categories/theory/">theory</a></span><span class="level-item">12 minutes read (About 1726 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/10/24/nlp0/">词嵌入</a></p><div class="content"><h2><span id="word2vec">word2vec</span></h2><p>word2vec工具包含两个模型，即跳元模型（skip-gram）和连续词袋（CBOW （Continuous Bag-of-Words）。Skip-gram是给定一个中心词，预测其周围的上下文词。CBOW是给定一段文本中的一个中心词周围的上下文词，预测中心词。</p>
<h3><span id="负采样">负采样</span></h3><p>在标准的 softmax 方法中，我们有</p>
<script type="math/tex; mode=display">
P(w_o \mid w_c) = \frac{\exp(\mathbf{u}_o^\top \mathbf{v}_c)}{ \sum_{i \in \mathcal{V}} \exp(\mathbf{u}_i^\top \mathbf{v}_c)},</script><p>这涉及到计算所有词汇表中词汇的条件概率, 在词汇表非常大的时候是非常耗时的。</p>
<p>负采样的核心思想是通过随机选择一部分非上下文词（负样本）来近似这个条件概率分布。具体来说，对于每个中心词和对应的上下文词，我们不仅更新这两个词的向量表示，而且还随机选择若干个不在上下文中的词作为“负样本”，并更新它们的向量表示。在负采样中，我们采用一个简化的逻辑回归目标函数来替代标准的 softmax 函数。对于每个训练样本,我们希望模型能够正确区分真正的上下文词和负样本词。因此，我们可以定义一个目标函数，它希望对于上下文词，有$\sigma(\u_o\v_i+b)=1$而对于每个负样本，有$\sigma(\u_o\v_i+b)=0$。 通过这种方式，负采样实际上是在模拟原始 softmax 的行为，但它只考虑了一小部分词汇，即一个正样本加上几个负样本。这意味着负采样通过一系列独立的二元分类任务来近似这个条件概率。</p>
<h3><span id="层次softmax">层次softmax</span></h3><p>层次softmax 的核心思想是构建一个词汇的层次结构，通常是一棵哈夫曼树（Huffman Tree），并将分类问题转化为沿着树的路径进行的一系列二元分类问题。这样可以将原始的 softmax 层的计算复杂度从线性减少到对数级别。层次softmax 的目标是通过计算从根节点到词汇的叶子节点的路径概率来近似原条件概率。</p>
<h2><span id="glove">GloVe</span></h2><p>GloVe模型的主要特点是它试图捕捉单词之间的共现频率信息，即一个单词出现时另一个单词出现的概率。通过这种方式，模型可以学习到词汇语义以及词汇间的关系，比如同义词、反义词、以及类比关系等。</p>
<p>训练GloVe模型涉及到最小化词对共现概率预测值与实际共现概率之间的损失函数。</p>
<h2><span id="子词嵌入">子词嵌入</span></h2><h3><span id="fasttext">fastText</span></h3><p>在跳元模型和连续词袋模型中，同一词的不同变形形式直接由不同的向量表示，不需要共享参数。为了使用形态信息，fastText模型提出了一种子词嵌入方法，其中子词是一个字符n-gram。fastText可以被认为是子词级跳元模型，而非学习词级向量表示，其中每个中心词由其子词级向量之和表示。</p>
<h3><span id="字节对编码byte-pair-encoding">字节对编码（Byte Pair Encoding）</span></h3><p>字节对编码（Byte Pair Encoding，简称 BPE）是一种用于词汇归一化和文本压缩的技术，近年来，BPE 被重新引入到自然语言处理领域，特别是在机器翻译和语言建模中，作为一种生成子词单位的有效方法。BPE 的基本思想是不断地合并最常出现的相邻字符对，直到达到预定的词汇表大小。</p>
<h2><span id="elmo">ELMo</span></h2><p>word2vec和GloVe都将相同的预训练向量分配给同一个词，而不考虑词的上下文，考虑到自然语言中丰富的多义现象和复杂的语义，上下文无关表示具有明显的局限性，同一个词可以根据上下文被赋予不同的表示。</p>
<p>ELMo（Embeddings from Language Models）是一种上下文敏感的词嵌入方法，使用双向 LSTM（长短期记忆网络）构建深度语言模型，这种模型可以捕获来自句子左右两边的信息。通过训练这种模型，可以得到一个对上下文敏感的词嵌入。</p>
<h2><span id="gpt">GPT</span></h2><p>初代 GPT 基于 Transformer 架构，使用的是单向的 Transformer，这意味着它在生成文本时只能访问之前的位置信息，而不能访问当前位置之后的信息。通过自回归方式训练，即模型学习给定前面的文字后预测下一个文字。</p>
<h2><span id="bert">BERT</span></h2><p>ELMo对上下文进行双向编码，但使用特定于任务的架构；而GPT是任务无关的，但是从左到右编码上下文，BERT结合了这两个方面的优点。</p>
<p>BERT输入序列明确地表示单个文本和文本对。当输入为单个文本时，BERT输入序列是特殊类别词元“\<cls\>”、文本序列的标记、以及特殊分隔词元“\<sep\>”的连结。当输入为文本对时，BERT输入序列是“\<cls\>”、第一个文本序列的标记、“\<sep\>”、第二个文本序列标记、以及“\<sep\>”的连结。</sep\></sep\></cls\></sep\></cls\></p>
<p>为了双向编码上下文以表示每个词元，BERT随机掩蔽词元并使用来自双向上下文的词元以自监督的方式预测掩蔽词元。此任务称为掩蔽语言模型。</p>
<p>尽管掩蔽语言建模能够编码双向上下文来表示单词，但它不能显式地建模文本对之间的逻辑关系。为了帮助理解两个文本序列之间的关系，BERT在预训练中考虑了一个二元分类任务——下一句预测。在为预训练生成句子对时，有一半的时间它们确实是标签为“真”的连续句子；在另一半的时间里，第二个句子是从语料库中随机抽取的，标记为“假”。</p>
<p>在预训练BERT时，最终的损失函数是掩蔽语言模型损失函数和下一句预测损失函数的线性组合。</p>
<h2><span id="参考文献">参考文献</span></h2><p><a target="_blank" rel="noopener" href="https://zh.d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html">https://zh.d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html</a></p>
<p><a target="_blank" rel="noopener" href="https://zh.d2l.ai/chapter_natural-language-processing-pretraining/subword-embedding.html">https://zh.d2l.ai/chapter_natural-language-processing-pretraining/subword-embedding.html</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-10-22T08:11:40.000Z" title="2024-10-22 4:11:40 ├F10: PM┤">2024-10-22</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-10-23T10:36:53.642Z" title="2024-10-23 6:36:53 ├F10: PM┤">2024-10-23</time></span><span class="level-item"><a class="link-muted" href="/categories/theory/">theory</a></span><span class="level-item">a minute read (About 116 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/10/22/math0/">Optimization</a></p><div class="content"><h2><span id="optimization">Optimization</span></h2><p>The solution of the function could be a local minimum, a local maximum, or a saddle point at a position where the function gradient is zero:</p>
<p>When the eigenvalues of the function’s Hessian matrix at the zero-gradient position are all positive, we have a local minimum for the function.</p>
<p>When the eigenvalues of the function’s Hessian matrix at the zero-gradient position are all negative, we have a local maximum for the function.</p>
<p>When the eigenvalues of the function’s Hessian matrix at the zero-gradient position are negative and positive, we have a saddle point for the function.</p>
<h2><span id="references">references</span></h2><p><a target="_blank" rel="noopener" href="https://d2l.ai/chapter_optimization/optimization-intro.html">https://d2l.ai/chapter_optimization/optimization-intro.html</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-10-10T08:27:35.000Z" title="2024-10-10 4:27:35 ├F10: PM┤">2024-10-10</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-10-11T12:17:39.987Z" title="2024-10-11 8:17:39 ├F10: PM┤">2024-10-11</time></span><span class="level-item"><a class="link-muted" href="/categories/theory/">theory</a></span><span class="level-item">a few seconds read (About 27 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/10/10/llm8/">perplexity</a></p><div class="content"><h2><span id="references">references</span></h2><p><a target="_blank" rel="noopener" href="https://thegradient.pub/understanding-evaluation-metrics-for-language-models/">https://thegradient.pub/understanding-evaluation-metrics-for-language-models/</a><br><a target="_blank" rel="noopener" href="https://huggingface.co/docs/transformers/perplexity">https://huggingface.co/docs/transformers/perplexity</a><br><a target="_blank" rel="noopener" href="https://blog.uptrain.ai/decoding-perplexity-and-its-significance-in-llms/">https://blog.uptrain.ai/decoding-perplexity-and-its-significance-in-llms/</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-07-04T07:14:44.000Z" title="2024-7-4 3:14:44 ├F10: PM┤">2024-07-04</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-07-04T13:15:33.232Z" title="2024-7-4 9:15:33 ├F10: PM┤">2024-07-04</time></span><span class="level-item"><a class="link-muted" href="/categories/theory/">theory</a></span><span class="level-item">a few seconds read (About 10 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/07/04/llm7/">Important papers</a></p><div class="content"><h2><span id="references">references</span></h2><p><a target="_blank" rel="noopener" href="https://magazine.sebastianraschka.com/p/understanding-large-language-models">https://magazine.sebastianraschka.com/p/understanding-large-language-models</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-07-04T06:57:56.000Z" title="2024-7-4 2:57:56 ├F10: PM┤">2024-07-04</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-07-04T13:34:08.076Z" title="2024-7-4 9:34:08 ├F10: PM┤">2024-07-04</time></span><span class="level-item"><a class="link-muted" href="/categories/theory/">theory</a></span><span class="level-item">3 minutes read (About 431 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/07/04/llm6/">finetuning large language models</a></p><div class="content"><h2><span id="the-3-conventional-feature-based-and-finetuning-approaches">The 3 Conventional Feature-Based and Finetuning Approaches</span></h2><h3><span id="feature-based-approach">Feature-Based Approach</span></h3><p>In the feature-based approach, we load a pretrained LLM and apply it to our target dataset. Here, we are particularly interested in generating the output embeddings for the training set, which we can use as input features to train a classification model.</p>
<h3><span id="finetuning-i-updating-the-output-layers">Finetuning I – Updating The Output Layers</span></h3><p>A popular approach related to the feature-based approach described above is finetuning the output layers (we will refer to this approach as finetuning I). Similar to the feature-based approach, we keep the parameters of the pretrained LLM frozen. We only train the newly added output layers.</p>
<h3><span id="finetuning-ii-updating-all-layers">Finetuning II – Updating All Layers</span></h3><p>when optimizing the modeling performance, the gold standard for using pretrained LLMs is to update all layers.</p>
<h2><span id="parameter-efficient-finetuning-techniques-peft">parameter-efficient finetuning techniques (PEFT)</span></h2><p>To finetune LLM with high modeling performance while only requiring the training of only a small number of parameters. These methods are usually referred to as parameter-efficient finetuning techniques (PEFT). Techniques such as prefix tuning, adapters, and low-rank adaptation, all of which “modify” multiple layers, achieve much better predictive performance (at a low cost).</p>
<h2><span id="reinforcement-learning-with-human-feedback-rlhf">Reinforcement Learning with Human Feedback (RLHF)</span></h2><p>In RLHF, human feedback is collected by having humans rank or rate different model outputs, providing a reward signal. The collected reward labels can then be used to train a reward model that is then in turn used to guide the LLMs adaptation to human preferences.</p>
<p>The reward model itself is learned via supervised learning (typically using a pretrained LLM as base model). Next, the reward model is used to update the pretrained LLM that is to be adapted to human preferences — the training uses a flavor of reinforcement learning called proximal policy optimization.</p>
<h2><span id="prompt-tuning">prompt tuning</span></h2><p>In a nutshell, prompt tuning (different from prompting) appends a tensor to the embedded inputs of a pretrained LLM. The tensor is then tuned to optimize a loss function for the finetuning task and data while all other parameters in the LLM remain frozen. </p>
<p>The main idea behind prompt tuning, and parameter-efficient finetuning methods in general, is to add a small number of new parameters to a pretrained LLM and only finetune the newly added parameters to make the LLM perform better on (a) a target dataset (for example, a domain-specific dataset like medical or legal documents) and (b) a target task (for example, sentiment classification).</p>
<h2><span id="references">references</span></h2><p><a target="_blank" rel="noopener" href="https://magazine.sebastianraschka.com/p/finetuning-large-language-models">https://magazine.sebastianraschka.com/p/finetuning-large-language-models</a><br><a target="_blank" rel="noopener" href="https://magazine.sebastianraschka.com/p/understanding-parameter-efficient">https://magazine.sebastianraschka.com/p/understanding-parameter-efficient</a><br><a target="_blank" rel="noopener" href="https://magazine.sebastianraschka.com/p/finetuning-llms-with-adapters">https://magazine.sebastianraschka.com/p/finetuning-llms-with-adapters</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-05-18T09:08:37.000Z" title="2024-5-18 5:08:37 ├F10: PM┤">2024-05-18</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-05-18T17:34:57.680Z" title="2024-5-19 1:34:57 ├F10: AM┤">2024-05-19</time></span><span class="level-item"><a class="link-muted" href="/categories/theory/">theory</a></span><span class="level-item">a minute read (About 114 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/05/18/llm5/">Batch processing for sequences</a></p><div class="content"><h2><span id="padding">padding</span></h2><p>In natural language processing (NLP), padding refers to the practice of adding special tokens to sequences (such as sentences or texts) so that all sequences in a batch have the same length. Padding is essential when working with mini-batch processing in neural networks because it ensures that all sequences in a batch can be processed simultaneously, despite their varying lengths.</p>
<h2><span id="attention-masks">Attention masks</span></h2><p>Attention masks are tensors with the exact same shape as the input IDs tensor, filled with 0s and 1s: 1s indicate the corresponding tokens should be attended to, and 0s indicate the corresponding tokens should not be attended to. </p>
<h2><span id="references">references</span></h2><p><a target="_blank" rel="noopener" href="https://huggingface.co/learn/nlp-course/en/chapter2/5?fw=pt">https://huggingface.co/learn/nlp-course/en/chapter2/5?fw=pt</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-05-18T07:17:07.000Z" title="2024-5-18 3:17:07 ├F10: PM┤">2024-05-18</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-05-18T15:00:50.634Z" title="2024-5-18 11:00:50 ├F10: PM┤">2024-05-18</time></span><span class="level-item"><a class="link-muted" href="/categories/theory/">theory</a></span><span class="level-item">2 minutes read (About 296 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/05/18/llm4/">Tokenizers</a></p><div class="content"><h2><span id="what-is-tokenizer">what is tokenizer</span></h2><p>A tokenizer is a crucial component in natural language processing (NLP) and text analysis that breaks down text into smaller, manageable units called tokens. These tokens can be words, phrases, symbols, or other meaningful elements depending on the specific requirements of the application. </p>
<h2><span id="how-tokenizer-works">how tokenizer works</span></h2><p>There are different types of tokenizer methods.Whitespace Tokenizers, Punctuation-Based Tokenizers,  Word Tokenizers,Sentence Tokenizers,Character Tokenizers, N-gram Tokenizers, Regular Expression Tokenizers and<br>Subword Tokenizers.</p>
<h3><span id="word-tokenizers">Word Tokenizers</span></h3><p>Word tokenization, also known as lexical analysis, is the process of splitting a piece of text into individual words or tokens. Word tokenization typically involves breaking the text into words based on spaces and punctuation. </p>
<h3><span id="subword-tokenizers">Subword Tokenizers</span></h3><p>Subword tokenization algorithms rely on the principle that frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords. A subword tokenizer is a type of tokenizer used in natural language processing (NLP) that breaks down words into smaller units or subwords. This approach is particularly useful for handling rare or out-of-vocabulary words, reducing the vocabulary size, and improving the efficiency of language models.</p>
<h4><span id="common-subword-tokenization-methods">Common Subword Tokenization Methods</span></h4><h5><span id="byte-pair-encoding-bpe">Byte-Pair Encoding (BPE)</span></h5><p>BPE is an iterative algorithm that merges the most frequent pairs of characters or subwords in a corpus until a desired vocabulary size is reached.</p>
<h5><span id="wordpiece-tokenization">WordPiece Tokenization</span></h5><p>Similar to BPE, WordPiece builds a vocabulary of subwords based on frequency, optimizing for a balance between vocabulary size and the ability to handle rare words.</p>
<h5><span id="sentencepiece">SentencePiece</span></h5><p>SentencePiece is an unsupervised text tokenizer and detokenizer mainly designed for Neural Network-based text generation systems. It treats the input text as a sequence of Unicode characters and uses a subword model to create subwords.</p>
<h2><span id="references">references</span></h2><p><a target="_blank" rel="noopener" href="https://huggingface.co/learn/nlp-course/en/chapter2/4">https://huggingface.co/learn/nlp-course/en/chapter2/4</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-03-29T05:00:15.000Z" title="2024-3-29 1:00:15 ├F10: PM┤">2024-03-29</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-03-29T14:40:00.506Z" title="2024-3-29 10:40:00 ├F10: PM┤">2024-03-29</time></span><span class="level-item"><a class="link-muted" href="/categories/theory/">theory</a></span><span class="level-item">a few seconds read (About 108 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/03/29/llm1/">Sampling</a></p><div class="content"><h1><span id="sampling">Sampling</span></h1><h2><span id="top-p-sampling">top-p sampling</span></h2><p>This method only considers the tokens whose cumulative probability exceed the probability p and then redistributes the probability mass across the remaining tokens so that the sum of probabilities is 1. </p>
<h2><span id="temperature">temperature</span></h2><p>What the temperature does is: it controls the relative weights in the probability distribution. It controls the extent to which differences in probability play a role in the sampling. At temperature t=0 this sampling technique turns into what we call greedy search/argmax sampling where the token with the highest probability is always selected. </p>
<h2><span id="reference">reference</span></h2><p><a target="_blank" rel="noopener" href="https://blog.ml6.eu/why-openais-api-models-cannot-be-forced-to-behave-fully-deterministically-4934a7e8f184">https://blog.ml6.eu/why-openais-api-models-cannot-be-forced-to-behave-fully-deterministically-4934a7e8f184</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-02-21T04:25:29.000Z" title="2024-2-21 12:25:29 ├F10: PM┤">2024-02-21</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-03-15T10:58:19.589Z" title="2024-3-15 6:58:19 ├F10: PM┤">2024-03-15</time></span><span class="level-item"><a class="link-muted" href="/categories/theory/">theory</a></span><span class="level-item">2 minutes read (About 234 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/02/21/llm0/">Measuring sentence similarity</a></p><div class="content"><h2><span id="metrics">metrics</span></h2><h3><span id="bleu-bilingual-evaluation-understudy">BLEU (Bilingual Evaluation Understudy)</span></h3><p>BLEU computes a score based on the n-gram overlap between the generated text and the reference text, as well as the brevity penalty to handle cases where the generated text is too short. The score ranges from 0 to 1, where 1 indicates a perfect match with the reference translations.</p>
<h3><span id="rouge-recall-oriented-understudy-for-gisting-evaluation">ROUGE (Recall-Oriented Understudy for Gisting Evaluation)</span></h3><p>ROUGE score measures the similarity between the machine-generated summary and the reference summaries using overlapping n-grams, word sequences that appear in both the machine-generated summary and the reference summaries. ROUGE score ranges from 0 to 1, with higher values indicating better summary quality. </p>
<p>ROUGE scores are branched into ROUGE-N,ROUGE-L, and ROUGE-S.<br>ROUGE-N measures the overlap of n-grams (contiguous sequences of n words) between the candidate text and the reference text. It computes the precision, recall, and F1-score based on the n-gram overlap.<br>ROUGE-L measures the longest common subsequence (LCS) between the candidate text and the reference text. It computes the precision, recall, and F1-score based on the length of the LCS.<br>ROUGE-S measures the skip-bigram (bi-gram with at most one intervening word) overlap between the candidate text and the reference text. It computes the precision, recall, and F1-score based on the skip-bigram overlap. </p>
<h2><span id="references">references</span></h2><p><a target="_blank" rel="noopener" href="https://medium.com/@sthanikamsanthosh1994/understanding-bleu-and-rouge-score-for-nlp-evaluation-1ab334ecadcb">https://medium.com/@sthanikamsanthosh1994/understanding-bleu-and-rouge-score-for-nlp-evaluation-1ab334ecadcb</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-01-25T09:14:35.000Z" title="2024-1-25 5:14:35 ├F10: PM┤">2024-01-25</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-02-03T12:11:22.353Z" title="2024-2-3 8:11:22 ├F10: PM┤">2024-02-03</time></span><span class="level-item"><a class="link-muted" href="/categories/theory/">theory</a></span><span class="level-item">20 minutes read (About 2965 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/01/25/pai11/">pai - review notes</a></p><div class="content"><h1><span id="fundamentals">fundamentals</span></h1><h2><span id="probability">probability</span></h2><h3><span id="sample-space-event-space-σ-algebra-and-probability-space">sample space, event space, σ-algebra and probability space</span></h3><p>A probability space is a mathematical construct that consists of three elements: the sample space (S), the event space (E), and a probability measure (P). Additionally, a sigma-algebra (σ-algebra) is associated with the event space.</p>
<p>The sample space is the set of all possible outcomes of an experiment.The event space is a collection of subsets of the sample space.<br>A sigma-algebra is a collection of subsets of the sample space. It includes the sample space, is closed under complementation, and is closed under countable unions. The probability measure is a function that assigns probabilities to events.</p>
<h3><span id="probability-mass-function-pmf-and-cumulative-distribution-function-cdfprobability-density-function-pdf">probability mass function (PMF) and cumulative distribution function (CDF),probability density function (PDF)</span></h3><p>The Probability Mass Function (PMF) is a function that gives the probability that a discrete random variable is exactly equal to a certain value. The Cumulative Distribution Function (CDF) of a random variable gives the probability that<br>X takes on a value less than or equal to x.<br>The Probability Density Function (PDF) is applicable to continuous random variables.The total area under the PDF curve is equal to 1.</p>
<h3><span id="continuous-distributions">Continuous Distributions</span></h3><p>normal distribution(Gaussian):The Gaussian CDF cannot be expressed in closed-form. Note that the mean of a Gaussian distribution coincides with the maximizer of its PDF, also called mode of a distribution.</p>
<h3><span id="joint-probability-conditional-probability-sum-rule-product-rulechain-rulethe-law-of-total-probability">Joint Probability, Conditional Probability, Sum rule, Product rule(chain rule),the law of total probability</span></h3><p>Joint probability refers to the probability of the occurrence of two or more events simultaneously. Conditional probability is the probability of one event occurring given that another event has already occurred. The sum rule, or addition rule, gives the probability of the union of two events. The product rule, also known as the chain rule, provides a way to express the joint probability of multiple events. The law of total probability is a way to express the probability of an event B by considering all possible ways in which<br>B can occur. </p>
<h3><span id="independence-conditional-independence">independence, conditional independence</span></h3><p>Conditional independence does not necessarily imply unconditional independence, and vice versa.</p>
<h3><span id="directed-graphical-modelsbayesian-networks">Directed Graphical Models(Bayesian networks)</span></h3><p>Directed graphical models (also called Bayesian networks) are often used to visually denote the (conditional) independence relationships of a large number of random variables.</p>
<h3><span id="expectation-covariance-and-variance-standard-deviationlaw-of-total-variance">Expectation, Covariance and Variance, Standard deviation,Law of total variance</span></h3><h3><span id="change-of-variables-formula">Change of variables formula</span></h3><h2><span id="probabilistic-inference">Probabilistic inference</span></h2><h3><span id="bayes-ruleconjugate-priors">Bayes’ rule,Conjugate Priors</span></h3><h3><span id="gaussianguassian-random-vector">gaussian,Guassian random vector</span></h3><p>Any affine transformation of a Gaussian random vector is a Gaussian random vector.</p>
<h2><span id="supervised-learning-and-point-estimates">Supervised Learning and Point Estimates</span></h2><h3><span id="maximum-likelihood-estimationmaximum-a-posteriori-estimation">Maximum Likelihood Estimation,Maximum a Posteriori Estimation,</span></h3><p>The MLE and MAP estimate can be seen as a naïve approximation of probabilistic inference, represented by a point density which “collapses” all probability mass at the mode of the posterior distribution.</p>
<h2><span id="exercise">exercise</span></h2><h3><span id="affine-transformationjacobian-matrix">affine transformation,Jacobian matrix,</span></h3><h1><span id="pml">PML</span></h1><h2><span id="linear-regression">linear regression</span></h2><h3><span id="linear-regressionmle-ridge-regressionmap">linear regression(MLE), ridge regression(MAP)</span></h3><h3><span id="ridgelasso">ridge,lasso</span></h3><p>least absolute shrinkage and selection operator (lasso): Laplace prior, L1 regularization.<br>Ridge: Gaussian prior, L2 regularization.</p>
<p>The primary difference lies in the penalty terms: L1 regularization uses the sum of absolute values, and L2 regularization uses the sum of squared values.<br>L1 regularization tends to result in exact zeros, leading to sparse solutions, whereas L2 regularization generally leads to smaller, non-zero coefficients.</p>
<h3><span id="bayesian-linear-regression-blr">Bayesian linear regression (BLR)</span></h3><h3><span id="aleatoric-and-epistemic-uncertainty">Aleatoric and Epistemic Uncertainty</span></h3><p>epistemic uncertainty: corresponds to the uncertainty about our model due to the lack of data. aleatoric uncertainty: “irreducible noise”, cannot be explained by the inputs and any model from the model class. </p>
<p>equation under the law of total variance. </p>
<h3><span id="kernel">kernel</span></h3><h2><span id="filtering">Filtering</span></h2><p>The process of keeping track of the state using noisy observations is also known as Bayesian filtering or recursive Bayesian estimation.</p>
<h3><span id="kalman-filter">Kalman filter</span></h3><p>A Kalman filter is simply a Bayes filter using a Gaussian distribution over the states and conditional linear Gaussians to describe the evolution of states and observations.</p>
<h2><span id="gaussian-process">Gaussian Process</span></h2><p>A Gaussian process is an infinite set of random variables such that any finite number of them are jointly Gaussian.</p>
<h3><span id="kernel-function-feature-space-rkhsstationarity-and-isotropy">kernel function, feature space, RKHS,Stationarity and isotropy</span></h3><p>A Gaussian process with a linear kernel is equivalent to Bayesian linear regression.</p>
<p>For ν = 1/2, the Matérn kernel is equivalent to the Laplace kernel. For ν → ∞, the Matérn kernel is equivalent to the Gaussian kernel.</p>
<p>Note that stationarity is a necessary condition for isotropy. In other words, isotropy implies stationarity.</p>
<p>skip 4.3.4</p>
<h2><span id="model-selection">model selection</span></h2><h3><span id="maximizing-the-marginal-likelihood">Maximizing the Marginal Likelihood</span></h3><p>Marginal likelihood maximization is an empirical Bayes method. Often it is simply referred to as empirical Bayes.<br>this approach typically avoids overfitting even though we do not use a separate training and validation set. maximizing the marginal likelihood naturally encourages trading between a large likelihood and a large prior.</p>
<h2><span id="approximations">Approximations</span></h2><h3><span id="random-fourier-featuresbochners-theoremuniform-convergence-of-fourier-features">random Fourier features,Bochner’s theorem,Uniform convergence of Fourier features</span></h3><h3><span id="inducing-points-methodsubset-of-regressors-sor-approximationfully-independent-training-conditional-fitc-approximation">inducing points method,subset of regressors (SoR) approximation,fully independent training conditional (FITC) approximation</span></h3><h2><span id="variational-inference">Variational Inference</span></h2><h3><span id="laplace-approximationbayesian-logistic-regression">Laplace Approximation,Bayesian Logistic Regression,</span></h3><p>The Laplace approximation matches the shape of the true posterior around its mode but may not represent it accurately elsewhere — often leading to extremely overconfident predictions.</p>
<h3><span id="variational-familymean-field-distribution">Variational family,mean-field distribution</span></h3><h3><span id="information-theorysurpriseentropyjensens-inequalitycross-entropykl-divergenceforward-and-reverse-kl-divergence">Information Theory,Surprise,entropy,Jensen’s Inequality,Cross-entropy,KL-divergence,Forward and Reverse KL-divergence</span></h3><p>The uniform distribution has the maximum entropy among all discrete distributions supported on {1, . . . , n}.</p>
<p>In words, KL(p∥q) measures the additional expected surprise when observing samples from p that is due to assuming the (wrong) distribution q.</p>
<p>It can be seen that the reverse KL-divergence tends to greedily select the mode and underestimating the variance which, in this case, leads to an overconfident prediction. The forward KL-divergence, in contrast, is more conservative.</p>
<p>Note, however, that reverse-KL is not greedy in the same sense as Laplace approximation, as it does still take the variance into account and does not purely match the mode of p.</p>
<h3><span id="minimizing-forward-kl-as-maximum-likelihood-estimationminimizing-forward-kl-as-moment-matching">Minimizing Forward-KL as Maximum Likelihood Estimation,Minimizing Forward-KL as Moment Matching</span></h3><p> First, we observe that minimizing the forward KL-divergence is equivalent to maximum likelihood estimation on an infinitely large sample size.</p>
<p>A Gaussian qλ minimizing KL(p∥qλ) has the same first and second moment as p.</p>
<h3><span id="evidence-lower-boundgaussian-vi-vs-laplace-approximation-gradient-of-evidence-lower-boundscore-gradients-and-reparameterization-trick">Evidence Lower Bound,Gaussian VI vs Laplace approximation, Gradient of Evidence Lower Bound(score gradients and Reparameterization trick)</span></h3><p>maximizing the ELBO coincides with minimizing reverse-KL.</p>
<p>maximizing the ELBO selects a variational distribution q that is close to the prior distribution p(·) while also maximizing the average likelihood of the data p(y1:n | x1:n, θ) for θ ∼ q.</p>
<p>Note that for a noninformative prior p(·) ∝ 1, maximizing the ELBO is equivalent to maximum likelihood estimation.</p>
<p>skip 5.5.2</p>
<h2><span id="markov-chain-monte-carlo-methods">Markov Chain Monte Carlo Methods</span></h2><p>The key idea of Markov chain Monte Carlo methods is to construct a Markov chain, which is efficient to simulate and has the stationary distribution p.</p>
<h3><span id="markov-chainsstationarityconvergence-markov-propertytime-homogeneous-markov-chains">Markov Chains(Stationarity,convergence), Markov property,time-homogeneous Markov chains</span></h3><p>Intuitively, the Markov property states that future behavior is independent of past states given the present state.</p>
<h3><span id="stationary-distributionaperiodicergodicityfundamental-theorem-of-ergodic-markov-chains">Stationary distribution,aperiodic,Ergodicity,Fundamental theorem of ergodic Markov chains</span></h3><p>After entering a stationary distribution π, a Markov chain will always remain in the stationary distribution.</p>
<p>It can be shown that there exists a unique stationary distribution π if the Markov chain is irreducible, that is, if every state is reachable from every other state with a positive probability when the Markov chain is run for enough steps.</p>
<p>Even if a Markov chain has a unique stationary distribution, it must not converge to it.</p>
<p>In words, a Markov chain is aperiodic iff for every state x, the transition graph has a closed path from x to x with length k for all k ∈ N greater than some k0 ∈ N.</p>
<p>A Markov chain is ergodic iff there exists a t ∈ N0 such that for any x, x′ ∈ S we have p(t)(x′ | x) &gt; 0.</p>
<p>A commonly used strategy to ensure that a Markov chain is ergodic is to add “self-loops” to every vertex in the transition graph.</p>
<p>An ergodic Markov chain has a unique stationary distribution π (with full support) irrespectively of the initial distribution q0. This naturally suggests constructing an ergodic Markov chain such that its stationary distribution coincides with the posterior distribution. If we then sample “sufficiently long”, Xt is drawn from a distribution that is “very close” to the posterior distribution.</p>
<h3><span id="how-quickly-does-a-markov-chain-convergetotal-variation-distance-and-mixing-time">How quickly does a Markov chain converge?(Total variation distance and Mixing time)</span></h3><h3><span id="detailed-balance-equationergodic-theorem">Detailed Balance Equation,Ergodic Theorem</span></h3><p>A Markov chain that satisfies the detailed balance equation with respect to π is called reversible with respect to π.</p>
<p>Ergodic Theorem is a way to generalize the (strong) law of large numbers to Markov chains.</p>
<h3><span id="elementary-sampling-methodsmetropolis-hastings-algorithmmetropolis-hastings-theorem-gibbs-sampling">Elementary Sampling Methods,Metropolis-Hastings Algorithm,Metropolis-Hastings theorem, Gibbs Sampling</span></h3><p>A popular example of a Metropolis-Hastings algorithm is Gibbs sampling. </p>
<h3><span id="sampling-as-optimizationgibbs-distributionlangevin-dynamicsmetropolis-adjusted-langevin-algorithm-mala-or-langevin-monte-carlo-lmcunadjusted-langevin-algorithm-ula-stochastic-gradient-langevin-dynamics-hamiltonian-monte-carlo">Sampling as Optimization,Gibbs distribution,Langevin Dynamics,Metropolis adjusted Langevin algorithm (MALA) or Langevin Monte Carlo (LMC),unadjusted Langevin algorithm (ULA), Stochastic Gradient Langevin Dynamics, Hamiltonian Monte Carlo</span></h3><p>A useful property is that Gibbs distributions always have full support. Observe that the posterior distribution can always be interpreted as a Gibbs distribution as long as prior and likelihood have full support.</p>
<p>Langevin dynamics adapts the Gaussian proposals of the Metropolis-Hastings algorithm to search the state space in an “informed” direction. The simple idea is to bias the sampling towards states with lower energy, thereby making it more likely that a proposal is accepted. A natural idea is to shift the proposal distribution perpendicularly to the gradient of the energy function. The resulting variant of Metropolis-Hastings is known as the Metropolis adjusted Langevin algorithm (MALA) or Langevin Monte Carlo (LMC).</p>
<p>The HMC algorithm is an instance of Metropolis-Hastings which uses momentum to propose distant points that conserve energy, with high acceptance probability.</p>
<h3><span id="path">path</span></h3><p>target: show that the stationary distribution of a Markov chain coincides with the posterior distribution.<br>base: Detailed Balance Equation which shows that A Markov chain that satisfies the detailed balance equation with respect to π is called reversible with respect to π and if the Markov chain is reversible with respect to π then π is a stationary distribution. With posterior distribution p(x) = 1/Z q(x), substitute the posterior for π in the detailed balance equation, we can remove z, so we do not need to know the true posterior p to check that the stationary distribution of our Markov chain coincides with p, it suffices to know the finite measure q. However, until now, this does not allow us to estimate expectations over the posterior distribution. Note that although constructing such a Markov chain allows us to obtain samples from the posterior distribution, they are not independent. Thus, the law of large numbers and Hoeffding’s inequality do not apply, but there is a way to generalize the (strong) law of large numbers to Markov chains, which is Ergodic theorem. </p>
<p>Next we need to consider how to construct Markov chain with the goal of approximating samples from the posterior distribution p. One way is Metropolis-Hastings Algorithm, in which we are given a proposal distribution and we use the acceptance distribution to decide whether to follow the proposal. Another way is Gibbs sampling. </p>
<p>MCMC techniques can be generalized to<br>continuous random variables / vectors. Observe that the posterior distribution can always be interpreted as a Gibbs distribution as long as prior and likelihood have full support.</p>
<p>Langevin dynamics adapts the Gaussian proposals of the Metropolis-Hastings algorithm to search the state space in an “informed” direction. The simple idea is to bias the sampling towards states with lower energy, thereby making it more likely that a proposal is accepted. A natural idea is to shift the proposal distribution perpendicularly to the gradient of the energy function. The resulting variant of Metropolis-Hastings is known as the Metropolis adjusted Langevin algorithm (MALA) or Langevin Monte Carlo (LMC).</p>
<p>skip 6.3.5</p>
<h2><span id="bdl">BDL</span></h2><h3><span id="artificial-neural-networksactivation-functions">Artificial Neural Networks,Activation Functions,</span></h3><p>Non-linear activation functions allow the network to represent arbitrary functions. This is known as the universal approximation theorem.</p>
<h3><span id="bayesian-neural-networks-heteroscedastic-noise-variational-inferencemarkov-chain-monte-carlo-swaswagdropout-and-dropconnect-probabilistic-ensembles">Bayesian Neural Networks, Heteroscedastic Noise, Variational Inference,Markov Chain Monte Carlo, SWA,SWAG,Dropout and Dropconnect, Probabilistic Ensembles</span></h3><p>Intuitively, variational inference in Bayesian neural networks can be interpreted as averaging the predictions of multiple neural networks drawn according to the variational posterior qλ.</p>
<h3><span id="calibrationexpected-calibration-error-ece-maximum-calibration-error-mce-histogram-binning-isotonic-regression-platt-scaling-temperature-scaling">Calibration,expected calibration error (ECE), maximum calibration error (MCE), Histogram binning, Isotonic regression, Platt scaling, Temperature scaling</span></h3><p>We say that a model is wellcalibrated if its confidence coincides with its accuracy across many predictions. Compare within each bin, how often the model thought the inputs belonged to the class (confidence) with how often the inputs actually belonged to the class (frequency).</p>
<h1><span id="sequential-decision-making">Sequential Decision-Making</span></h1><h2><span id="active-learning">Active Learning</span></h2><h3><span id="conditional-entropyjoint-entropy">Conditional Entropy,Joint entropy</span></h3><p>A very intuitive property of entropy is its monotonicity: when conditioning on additional observations the entropy can never increase.</p>
<h3><span id="mutual-informationinformation-gain-the-law-of-total-expectation-data-processing-inequality-interaction-informationsynergy-and-redundancy-submodularity-of-mutual-information-marginal-gain-submodularity-monotone">Mutual Information(information gain), the law of total expectation, data processing inequality, interaction information(Synergy and Redundancy), Submodularity of Mutual Information, Marginal gain, Submodularity, monotone,</span></h3><p>data processing inequality: which says that processing cannot increase the information contained in a signal.</p>
<p>F is submodular iff “adding” x to the smaller set A yields more marginal gain than adding x to the larger set B.</p>
<p>I is monotone submodular.</p>
<h3><span id="maximizing-mutual-information-greedy-submodular-function-maximization-uncertainty-sampling-marginal-gain-of-maximizing-mutual-informationbayesian-active-learning-by-disagreement-bald">Maximizing Mutual Information, Greedy submodular function maximization, Uncertainty Sampling, Marginal gain of maximizing mutual information,Bayesian active learning by disagreement (BALD)</span></h3><p>skip proof of Theorem 8.15</p>
<p>Therefore, if f is modeled by a Gaussian and we assume homoscedastic noise, greedily maximizing mutual information corresponds to simply picking the point x with the largest variance. This algorithm is also called uncertainty sampling.</p>
<h3><span id="experimental-design-entropy-search">Experimental Design, Entropy Search</span></h3><p>skip</p>
<h2><span id="bayesian-optimization">Bayesian Optimization</span></h2><h3><span id="exploration-exploitation-dilemmaonline-learning-and-bandits-multi-armed-bandits-regret-sublinear-regret-cesàro-mean">Exploration-Exploitation Dilemma,Online Learning and Bandits, Multi-Armed Bandits, Regret, sublinear regret, Cesàro mean</span></h3><p>Bayesian optimization can be interpreted as a variant of the MAB problem where there can be a potentially infinite number of actions (arms), but their rewards are correlated (because of the smoothness of the Gaussian process prior).</p>
<h3><span id="acquisition-functions-upper-confidence-bound-bayesian-confidence-intervals-regret-of-gp-ucb-information-gain-of-common-kernels-frequentist-confidence-intervals-probability-of-improvement-expected-improvement-ei-thompson-sampling-probability-matching-information-directed-sampling">Acquisition Functions, Upper Confidence Bound, Bayesian confidence intervals, Regret of GP-UCB, Information gain of common kernels, Frequentist confidence intervals, probability of improvement, expected improvement (EI), Thompson Sampling, probability matching, Information-Directed Sampling</span></h3><p>skip Information-Directed Sampling</p>
<h2><span id="markov-decision-processes">Markov Decision Processes</span></h2><p>Planning deals with the problem of deciding which action an agent should play in a (stochastic) environment. A key formalism for probabilistic planning in known environments are so-called Markov decision processes.</p>
<h3><span id="policydiscounted-payoffstate-value-functionstate-action-value-function-also-called-q-function">Policy,discounted payoff,state value function,state-action value function (also called Q-function)</span></h3><p>A policy is a function that maps each state x ∈ X to a probability distribution over the actions.</p>
<h3><span id="bellman-expectation-equation">Bellman Expectation Equation</span></h3><h3><span id="policy-evaluation-fixed-point-iteration-contractionbanach-fixed-point-theorem">Policy Evaluation, Fixed-point Iteration, contraction,Banach fixed-point theorem</span></h3><h3><span id="policy-optimization-greedy-policies-bellman-optimality-equation-bellmans-theorem-policy-iteration-value-iteration">Policy Optimization, Greedy Policies, Bellman Optimality Equation, Bellman’s theorem, Policy Iteration, Value Iteration,</span></h3><p>In particular, if for every state there is a unique action that maximizes the state-action value function, the policy π⋆ is deterministic and unique.</p>
<p>Intuitively, the Bellman optimality equations express that the value of a state under an optimal policy must equal the expected return for the best action from that state.</p>
<p>Value iteration converges to an ε-optimal solution in a polynomial number of iterations. Unlike policy iteration, value iteration does not converge to an exact solution in general.</p>
<h3><span id="partial-observability">Partial Observability,</span></h3><p>Whereas MDPs are controlled Markov chains, POMDPs are controlled hidden Markov models.</p>
<h2><span id="tabular-reinforcement-learning">Tabular Reinforcement Learning</span></h2><h3><span id="trajectories-episodic-setting-continuous-setting-on-policy-and-off-policy-methods">Trajectories, episodic setting, continuous setting, On-policy and Off-policy Methods,</span></h3><p>on-policy methods are used when the agent has control over its own actions, in other words, the agent can freely choose to follow any policy. off-policy methods can be used even when the agent cannot freely choose its actions.</p>
<h3><span id="model-based-approaches-balancing-exploration-and-exploitation-ε-greedy-softmax-explorationboltzmann-exploration-rmax-algorithm">Model-based Approaches, Balancing Exploration and Exploitation, ε-greedy, Softmax Exploration(Boltzmann exploration), Rmax algorithm</span></h3><p>skip Remark 11.3: Asymptotic convergence</p>
<p>Note that ε-greedy is GLIE with probability 1 if the sequence (εt)t∈N0 satisfies the RM-conditions (A.56), e.g., if εt = 1/t.</p>
<p>A significant benefit to model-based reinforcement learning is that it is inherently off-policy. That is, any trajectory regardless of the policy used to obtain it can be used to improve the model of the underlying Markov decision process. In the model-free setting, this not necessarily true.</p>
<h3><span id="model-free-approaches-on-policy-value-estimation-bootstrapping-temporal-difference-learning-sarsa-off-policy-value-estimation-q-learning-optimistic-q-learning">Model-free Approaches, On-policy Value Estimation, bootstrapping, temporal-difference learning, SARSA, Off-policy Value Estimation, Q-learning, optimistic Q-learning</span></h3><h2><span id="model-free-reinforcement-learning">Model-free Reinforcement Learning</span></h2><h3><span id="value-function-approximation-dqn-experience-replay-maximization-bias-double-dqn">Value Function Approximation, DQN, experience replay, maximization bias, Double DQN</span></h3><h3><span id="policy-approximationpolicy-gradient-methods-policy-value-function-policy-gradient-score-gradient-estimator-score-gradients-with-baselines-downstream-returns-reinforce-algorithm">Policy Approximation(policy gradient methods),  policy value function, Policy Gradient, Score gradient estimator, Score gradients with baselines, Downstream returns, REINFORCE algorithm</span></h3><p>The main advantage of policy gradient methods such as REINFORCE is that they can be used in continuous action spaces. However, REINFORCE is not guaranteed to find an optimal policy. Even when operating in very small domains, REINFORCE can get stuck in local optima.</p>
<h3><span id="on-policy-actor-criticsadvantage-function-policy-gradient-theorem-actor-criticq-actor-criticonline-actor-critic-advantage-actor-critic-bias-variance-tradeoff-trust-region-policy-optimization-trpo-proximal-policy-optimization-ppo">On-policy Actor-Critics,Advantage Function, Policy Gradient Theorem, Actor-Critic(Q actor-critic,Online actor-critic), advantage actor-critic, bias-variance tradeoff, trust-region policy optimization (TRPO), Proximal policy optimization (PPO)</span></h3><p>One approach in the online setting (i.e., non-episodic setting), is to simply use SARSA for learning the critic. To learn the actor, we use stochastic gradient descent with gradients obtained using single samples.</p>
<h3><span id> </span></h3><h2><span id="model-based-reinforcement-learning">Model-based Reinforcement Learning</span></h2><h3><span id="planning-over-finite-horizons-receding-horizon-control-rhc-model-predictive-control-mpc-random-shooting-methods">Planning over Finite Horizons, receding horizon control (RHC), model predictive control (MPC), Random shooting methods</span></h3><h2><span id="cheat-sheet">cheat sheet</span></h2><h3><span id="from-book">from book</span></h3><p>conditional distribution for gaussian;<br>Gaussian process posterior;<br>the predictive posterior at the test point;<br>common kernels;<br>the Hessian of the logistic loss;<br>Surprise,entropy,Jensen’s Inequality,Cross-entropy,KL-divergence,<br>ELBO,<br>the law of large numbers and Hoeffding’s inequality,<br>Hoeffding bound, </p>
<h3><span id="from-exercise">from exercise</span></h3><p> Woodbury push-through identity;<br> Solution to problem 3.6;</p>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous is-invisible is-hidden-mobile"><a href="/categories/theory/page/0/">Previous</a></div><div class="pagination-next"><a href="/categories/theory/page/2/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link is-current" href="/categories/theory/">1</a></li><li><a class="pagination-link" href="/categories/theory/page/2/">2</a></li><li><a class="pagination-link" href="/categories/theory/page/3/">3</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/tx.PNG" alt="s-serenity"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">s-serenity</p><p class="is-size-6 is-block">student</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Shanghai, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">89</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">11</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">23</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/s-serenity" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/s-serenity"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://github.com/s-serenity" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/LLM/"><span class="level-start"><span class="level-item">LLM</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/algorithm/"><span class="level-start"><span class="level-item">algorithm</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/computer-system/"><span class="level-start"><span class="level-item">computer system</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/database/"><span class="level-start"><span class="level-item">database</span></span><span class="level-end"><span class="level-item tag">15</span></span></a></li><li><a class="level is-mobile" href="/categories/development-tools/"><span class="level-start"><span class="level-item">development tools</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/engineering/"><span class="level-start"><span class="level-item">engineering</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/others/"><span class="level-start"><span class="level-item">others</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/practice/"><span class="level-start"><span class="level-item">practice</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/programming-language/"><span class="level-start"><span class="level-item">programming language</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/theory/"><span class="level-start"><span class="level-item">theory</span></span><span class="level-end"><span class="level-item tag">27</span></span></a></li><li><a class="level is-mobile" href="/categories/web-development/"><span class="level-start"><span class="level-item">web development</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-10-26T09:11:10.000Z">2024-10-26</time></p><p class="title"><a href="/2024/10/26/DL2/">深度学习(3) - 计算框架PyTorch</a></p><p class="categories"><a href="/categories/engineering/">engineering</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-10-25T08:28:05.000Z">2024-10-25</time></p><p class="title"><a href="/2024/10/25/RS0/">Recommender Systems</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-10-25T02:36:13.000Z">2024-10-25</time></p><p class="title"><a href="/2024/10/25/RL1/">RL1</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-10-25T02:12:01.000Z">2024-10-25</time></p><p class="title"><a href="/2024/10/25/algo1/">算法(2)--二叉树</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-10-24T02:36:56.000Z">2024-10-24</time></p><p class="title"><a href="/2024/10/24/nlp0/">词嵌入</a></p><p class="categories"><a href="/categories/theory/">theory</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2024/"><span class="level-start"><span class="level-item">2024</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/"><span class="level-start"><span class="level-item">2023</span></span><span class="level-end"><span class="level-item tag">43</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/"><span class="level-start"><span class="level-item">2022</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/"><span class="level-start"><span class="level-item">2021</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/"><span class="level-start"><span class="level-item">2020</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/"><span class="level-start"><span class="level-item">2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/"><span class="level-start"><span class="level-item">2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Algorithm/"><span class="tag">Algorithm</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Elasticsearch/"><span class="tag">Elasticsearch</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/LLM/"><span class="tag">LLM</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux/"><span class="tag">Linux</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Markdown/"><span class="tag">Markdown</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Math/"><span class="tag">Math</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">12</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Recommender-System/"><span class="tag">Recommender System</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SQL/"><span class="tag">SQL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/algorithm/"><span class="tag">algorithm</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/big-data/"><span class="tag">big data</span><span class="tag">13</span></a></div><div class="control"><a class="tags has-addons" href="/tags/computer-vision/"><span class="tag">computer vision</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/deep-learning/"><span class="tag">deep learning</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/git/"><span class="tag">git</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/graph/"><span class="tag">graph</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/hexo/"><span class="tag">hexo</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/java/"><span class="tag">java</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/machine-learning/"><span class="tag">machine learning</span><span class="tag">18</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python/"><span class="tag">python</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/reinforcement-learning/"><span class="tag">reinforcement learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/speech/"><span class="tag">speech</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/tools/"><span class="tag">tools</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="s-serenity" height="28"></a><p class="is-size-7"><span>&copy; 2024 s-serenity</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2019</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>