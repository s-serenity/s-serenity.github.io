<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="robots" content="noindex"><meta><title>Category: database - s-serenity</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="s-serenity"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="s-serenity"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Recording and sharing my learning process."><meta property="og:type" content="blog"><meta property="og:title" content="s-serenity"><meta property="og:url" content="http://yoursite.com/"><meta property="og:site_name" content="s-serenity"><meta property="og:description" content="Recording and sharing my learning process."><meta property="og:locale" content="en_US"><meta property="og:image" content="http://yoursite.com/img/og_image.png"><meta property="article:author" content="s-serenity"><meta property="article:tag" content="Algorithm study development"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://yoursite.com/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://yoursite.com"},"headline":"s-serenity","image":["http://yoursite.com/img/og_image.png"],"author":{"@type":"Person","name":"s-serenity"},"publisher":{"@type":"Organization","name":"s-serenity","logo":{"@type":"ImageObject","url":"http://yoursite.com/img/logo.svg"}},"description":"Recording and sharing my learning process."}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.1.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="s-serenity" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-content"><nav class="breadcrumb" aria-label="breadcrumbs"><ul><li><a href="/categories">Categories</a></li><li class="is-active"><a href="#" aria-current="page">database</a></li></ul></nav></div></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-12-12T14:22:00.000Z" title="2023-12-12 3:22:00 ├F10: PM┤">2023-12-12</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-12-12T17:08:42.096Z" title="2023-12-12 6:08:42 ├F10: PM┤">2023-12-12</time></span><span class="level-item"><a class="link-muted" href="/categories/database/">database</a></span><span class="level-item">3 minutes read (About 454 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/12/12/bigdata12/">bigdata - Cube Data</a></p><div class="content"><h1><span id="on-line-analytic-processingolap">On-Line Analytic Processing(OLAP)</span></h1><p>On-Line Analytic Processing generally involves highly complex queries that use one or more aggregations.</p>
<h2><span id="olap-and-data-warehouses">OLAP and Data Warehouses</span></h2><p>Data from many separate databases may be integrated into the warehouse. The warehouse is usually only updated overnight. Data warehouses play an important role in OLAP applications. First, the warehouse is necessary to organize and centralize data in a way that supports OLAP queries. Second, OLAP queries are usually complex and touching much of the data and take too much time to be executed in a transaction-processing system with high throughput requirements.</p>
<h2><span id="a-multidimensional-view-of-olap-data">A Multidimensional View of OLAP Data</span></h2><p>In typical OLAP applications there is a central relation or collection of data called the fact table. Often, it helps to think of the objects in the fact table as arranged in a multidimensional space. Two broad directions that have been taken by specialized systemns that support cube-structured data for OLAP: ROLAP and MOLAP. ROLAP, which is Relational OLAP. In this approach, data may be stored in relations with a specialized structure called a “star schema”. MOLAP, which is Multidimensional OLAP. A specialized structure “data cude” is used to hold the data, including its aggregates.</p>
<h2><span id="star-schemas">Star Schemas</span></h2><p>A star schema consists of the schema for the fact table, which links to several other relations, called “dimension tables”. </p>
<h2><span id="slicing-and-dicing">Slicing and Dicing</span></h2><p>A choice of partition for each dimension “dices” the cude. The result is that the cude is divided into smaller cubes that represent groups of points whose statistics are aggregated by a query that performs this partitioning in its “group by” clause. Through the “where” clause, a query has the option of focusing on particular partitions alone one or more dimensions.(on a particular “slice” of the cube).<br><img src="/2023/12/12/bigdata12/image-82.png" alt="Alt text"></p>
<p><img src="/2023/12/12/bigdata12/image-83.png" alt="Alt text"></p>
<p><img src="/2023/12/12/bigdata12/image-84.png" alt="Alt text"></p>
<p><img src="/2023/12/12/bigdata12/image-85.png" alt="Alt text"></p>
<h1><span id="data-cubes">Data Cubes</span></h1><p>The formal data cube precomputes all possible aggregates in a systematic way. </p>
<h2><span id="the-cube-operator">The Cube Operator</span></h2><p>Given a fact table F, we can define an augmented table CUBE(F) that adds an additional value, denoted <em>, to each dimension. The </em> has the intuitive meaning “any,” and it represents aggregation along the dimension in which it appears. </p>
<h2><span id="the-cube-operator-in-sql">The Cube Operator in SQL</span></h2><p>SQL gives us a way to apply the cube operator within queries. If we add the term “WITH CUBE” to a group-by clause, then we get not only the tuple for each group, but also the tuples that represent aggregation along one or more of the dimensions along which we have grouped. These tuples appear in the result with NULL where we have used *.<br><img src="/2023/12/12/bigdata12/image-86.png" alt="Alt text"></p>
<p><img src="/2023/12/12/bigdata12/image-87.png" alt="Alt text"></p>
<p>However, SalesRollup wpuld not contain tuples such as<br><img src="/2023/12/12/bigdata12/image-88.png" alt="Alt text"></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-12-12T14:21:07.000Z" title="2023-12-12 3:21:07 ├F10: PM┤">2023-12-12</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-12-13T20:10:17.476Z" title="2023-12-13 9:10:17 ├F10: PM┤">2023-12-13</time></span><span class="level-item"><a class="link-muted" href="/categories/database/">database</a></span><span class="level-item">9 minutes read (About 1361 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/12/12/bigdata11/">bigdata - Graph Database</a></p><div class="content"><h2><span id="why-graphs">Why graphs</span></h2><p>Now, we do know a way to avoid joins and studied it at length: denormalizing the data to (homogeneous) collections of trees is a way of “pre-computing” the joins statically, so that the data is already joined (via nesting) at runtime.</p>
<p>Now, why is it efficient? Because doing down a tree only necessitates following pointers in memory. But trees cannot have cycles. Graph databases provide a way of generalizing the use in-memory pointers to traverse data to the general case in which cycles are present: this is called “index-free adjacency.”</p>
<h2><span id="kinds-of-graph-databases">Kinds of graph databases</span></h2><p>There are many different graph database system products on the market, and they can be classified along several dimensions: Labeled property graph model vs. triple stores;Read-intensive vs. write-intensive;Local vs. distributed;Native vs. non-native. </p>
<h2><span id="graph-data-models">Graph data models</span></h2><h3><span id="labeled-property-graphs">Labeled property graphs</span></h3><p>Computer scientists need to go one step further and also design how to store graphs physically. One way of doing so is to create an associative array mapping each node to the list of nodes that it connects to via an edge (adjacency lists). Another storage form is with an adjacency matrix: each row and each column represent a node, and a 0 or a 1 indicate the absence or presence of an edge between the row node and the column node. </p>
<p>Now, this does not quite work for us, because labeled property graphs enhance mathematical graphs with extra ingredients: properties, and labels.<br>how to “convert” a relational table to a labeled property graph: labels can be seen as table names, nodes as records, and properties as the attribute values for the records. This shows that relational tables can be physically stored as labeled property graphs. Of course, this does not work the other way round: given a graph, it will often not be possible to convert it “back” to a table in this specific way.</p>
<h3><span id="triple-stores">Triple stores</span></h3><p>Triple stores are a different and simpler model. It views the graph as nodes and edges that all have labels, but without any properties. The graph is then represented as a list of edges, where each edge is a triple with the label of the origin node (called the subject), the label of the edge (called the property), and the label of the destination node (called the object).</p>
<p>Triple stores typically provide SPARQL capabilities to reason about and stored RDF data.</p>
<h2><span id="querying-graph-data">Querying graph data</span></h2><p>We will now have a look at query languages for querying graphs, with a focus on Cypher, which is neo4j’s query language.<br><img src="/2023/12/12/bigdata11/image-89.png" alt="Alt text"></p>
<h3><span id="cypher-philosophy">Cypher Philosophy</span></h3><p>Cypher enables a user (or an application acting on behalf of a user) to ask the database to find data that matches a specific pattern. Colloquially, we ask the database to “find things like this.” And the way we describe what “things like this” look like is to draw them, using ASCII art.</p>
<p>Like most query languages, Cypher is composed of clauses. The simplest queries consist of a MATCH clause followed by a RETURN clause. There are other clauses we can use in a Cypher query: WHERE,WITH…AS…,CREATE,MERGE,DELETE,SET,UNION,FORWACH and so on. </p>
<h3><span id="a-comparison-of-relational-and-graph-modeling">A Comparison of Relational and Graph Modeling</span></h3><p>Relational databases—with their rigid schemas and complex modeling characteristics—are not an especially good tool for supporting rapid change. What we need is a model that is closely aligned with the domain, but that doesn’t sacrifice performance, and that supports evolution while maintaining the integrity of the data as it undergoes rapid change and growth. That model is the graph model.</p>
<h3><span id="creating-a-graph">creating a graph</span></h3><p>In practice, we tend to use CREATE when we’re adding to the graph and don’t mind duplication, and MERGE when duplication is not permitted by the domain.</p>
<h3><span id="beginning-a-query">Beginning a Query</span></h3><p>In Cypher we always begin our queries from one or more well-known starting points in the graph—what are called bound nodes. Cypher uses any labels and property predicates supplied in the MATCH and WHERE clauses, together with metadata supplied by indexes and constraints, to find the starting points that anchor our graph patterns.</p>
<h3><span id="indexes-and-constraints">INDEXES AND CONSTRAINTS</span></h3><p>To support efficient node lookup, Cypher allows us to create indexes per label and property combinations. For unique property values we can also specify constraints that assure uniqueness.</p>
<h1><span id="neo4j">Neo4j</span></h1><p> Neo4j is a graph database with native processing capabilities as well as native graph storage.</p>
<h2><span id="native-graph-processing">Native Graph Processing</span></h2><p>Of the many different engine architectures, we say that a graph database has native processing capabilities if it exhibits a property called index-free adjacency.</p>
<p>A database engine that utilizes index-free adjacency is one in which each node maintains direct references to its adjacent nodes. Each node, therefore, acts as a micro-index of other nearby nodes, which is much cheaper than using global indexes. It means that query times are independent of the total size of the graph, and are instead simply proportional to the amount of the graph searched. A nonnative graph database engine, in contrast, uses (global) indexes to link nodes together.</p>
<p>Proponents for native graph processing argue that index-free adjacency is crucial for fast, efficient graph traversals. To understand why native graph processing is so much more efficient than graphs based on heavy indexing, consider the following. Depending on the implementation, index lookups could be O(log n) in algorithmic complexity versus O(1) for looking up immediate relationships. To traverse a network of m steps, the cost of the indexed approach, at O(m log n), dwarfs the cost of O(m) for an implementation that uses index-free adjacency.</p>
<h2><span id="native-graph-storage">Native Graph Storage</span></h2><p>Neo4j stores graph data in a number of different store files. Each store file contains the data for a specific part of the graph (e.g., there are separate stores for nodes, relationships, labels, and properties).The division of storage responsibilities—particularly the separation of graph structure from property data—facilitates performant graph traversals, even though it means the user’s view of their graph and the actual records on disk are structurally dissimilar.</p>
<p>Like most of the Neo4j store files, the node store is a fixed-size record store, where each record is nine bytes in length. Fixed-size records enable fast lookups for nodes in the store file. If we have a node with id 100, then we know its record begins 900 bytes into the file. Based on this format, the database can directly compute a record’s location, at cost O(1), rather than performing a search, which would be cost O(log n).</p>
<h2><span id="transactions">Transactions</span></h2><p>Transactions in Neo4j are semantically identical to traditional database transactions. Writes occur within a transaction context, with write locks being taken for consistency purposes on any nodes and relationships involved in the transaction. On successful completion of the transaction, changes are flushed to disk for durability, and the write locks released. These actions maintain the atomicity guarantees of the transaction. </p>
<h3><span id="core-api-traversal-framework-or-cypher">CORE API, TRAVERSAL FRAMEWORK, OR CYPHER?</span></h3><p>The Core API allows developers to fine-tune their queries so that they exhibit high affinity with the underlying graph. A well-written Core API query is often faster than any other approach. The downside is that such queries can be verbose, requiring considerable developer effort. Moreover, their high affinity with the underlying graph makes them tightly coupled to its structure. When the graph structure changes, they can often break. Cypher can be more tolerant of structural changes—things such as variable-length paths help mitigate variation and change.</p>
<p>The Traversal Framework is both more loosely coupled than the Core API (because it allows the developer to declare informational goals), and less verbose, and as a result a query written using the Traversal Framework typically requires less developer effort than the equivalent written using the Core API. Because it is a general-purpose framework, however, the Traversal Framework tends to perform marginally less well than a well-written Core API query.</p>
<h2><span id="references">references</span></h2><p>Robinson, I. et al. (2015). Graph Databases (2nd ed.)</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-11-28T20:58:26.000Z" title="2023-11-28 9:58:26 ├F10: PM┤">2023-11-28</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-12-12T15:35:16.899Z" title="2023-12-12 4:35:16 ├F10: PM┤">2023-12-12</time></span><span class="level-item"><a class="link-muted" href="/categories/database/">database</a></span><span class="level-item">17 minutes read (About 2564 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/11/28/bigdata10/">bigdata - JSONiq</a></p><div class="content"><h1><span id="querying-denormalized-data">Querying denormalized data</span></h1><h2><span id="imperative-vs-declarative">imperative vs declarative</span></h2><p>Imperative programming is a programming paradigm that expresses computation as a series of statements that change a program’s state. In imperative programming, the focus is on describing how a program operates step by step. Common imperative languages include C, C++, Java, and Python.</p>
<p>In contrast to imperative programming, declarative programming focuses on describing what the program should accomplish rather than how to achieve it. In a declarative host language, the emphasis is on specifying the desired outcome or properties, and the language itself takes care of the underlying implementation details. Such as SQL, HTML.</p>
<h2><span id="motivation">motivation</span></h2><h3><span id="denormalized-data">Denormalized data</span></h3><p>it is characterized with two features: nestedness, and heterogeneity. In fact, denormalized datasets should not be seen as “broken tables pushed to their limits”, but rather as collections of trees. </p>
<h3><span id="features-of-a-query-language">Features of a query language</span></h3><p>A query language for datasets has three main features: Declarative, Functional, and Set-based. First, it is declarative. This means that the users do not focus on how the query is computed, but on what it should return. Being functional means that the query language is made of composable expressions that nest with each other, like a Lego game. Finally, it is set-based, in the sense that the values taken and returned by expressions are not only single values (scalars), but are large sequences of items (in the case of SQL, an item is a row).</p>
<h3><span id="query-languages-for-denormalized-data">Query languages for denormalized data</span></h3><p>For denormalized data though, sadly, the number of languages keeps increasing: the oldest ones being XQuery, JSONiq, but then now also JMESPath, SpahQL, JSON Query, PartiQL, UnQL, N1QL, ObjectPath, JSONPath, ArangoDB Query Language (AQL), SQL++, GraphQL, MRQL, Asterix Query Language (AQL), RQL.</p>
<h3><span id="jsoniq-as-a-data-calculator">JSONiq as a data calculator</span></h3><p>it can perform arithmetics, but also comparison and logic. It is, however, more powerful than a common calculator and supports more complex constructs, for example variable binding.<br>It also supports all JSON values. Any copy-pasted JSON value literally returns itself.<br>And, unlike a calculator, it can access storage. </p>
<h2><span id="the-jsoniq-data-model">The JSONiq Data Model</span></h2><p>Every expression of the JSONiq “data calculator” returns a sequence of items.An item can be either an object, an array, an atomic item, or a function item. A sequence can also be empty. Caution, the empty sequence is not the same logically as a null item.</p>
<h2><span id="navigation">Navigation</span></h2><p>The general idea of navigation is that it is possible to “dive” into the nested data with dots and square brackets (originally, these were slashes with XPath) – all in parallel: starting with an original collection of objects (or, possibly, a single document), each step (i.e., for each dot and each pair of square brackets) goes down the nested structure and returns another sequence of nested items. </p>
<h3><span id="object-lookups-dot-syntax">Object lookups (dot syntax)</span></h3><p>json-doc(“file.json”).o</p>
<h3><span id="array-unboxing-empty-square-bracket-syntax">Array unboxing (empty square bracket syntax)</span></h3><p>We can unbox the array, meaning, extract its members as a sequence of seven object items, with empty square brackets.json-doc(“file.json”).o[].</p>
<h3><span id="parallel-navigation">Parallel navigation</span></h3><p>The dot syntax, in fact, works on sequences, too and will extract the value associated with a key in every object of the sequence (anything else than an object is ignored and thrown away).</p>
<h3><span id="filtering-with-predicates-simple-square-bracket-syntax">Filtering with predicates (simple square bracket syntax)</span></h3><p>It is possible to filter any sequence with a predicate, where <script type="math/tex">in the predicate refers to the current item being tested. json-doc("file.json").o[].a.b[][</script>.c = 3].It is also possible to access the item at position n in a sequence with this same notation: json-doc(“file.json”).o[].a.b[][5].</p>
<h3><span id="array-lookup-double-square-bracket-syntax">Array lookup (double square bracket syntax)</span></h3><p>To access the n-th member of an array, you can use double-squarebrackets, like so:<br>json-doc(“file.json”).o[[2]].a.</p>
<h3><span id="a-common-pitfall-array-lookup-vs-sequence-predicates">A common pitfall: Array lookup vs. Sequence predicates</span></h3><p>Do not confuse sequence positions (single square brackets) with array positions (double square brackets)!<br>([1, 2], [3, 4])[2] -&gt; [ 3, 4 ]<br>([1, 2], [3, 4])[[2]] -&gt; 2 4</p>
<h2><span id="schema-discovery">Schema discovery</span></h2><h3><span id="collections">Collections</span></h3><p>many datasets are in fact found in the form of large collections of smaller objects (as in document stores). Such collections are access with a function call together with a name or (if reading from a data lake) a path.</p>
<h3><span id="getting-all-top-level-keys">Getting all top-level keys</span></h3><p>The keys function retrieves all keys. It can be called on the entire sequence of objects and will return all unique keys found (at the top level) in that collection.</p>
<h3><span id="getting-unique-values-associated-with-a-key">Getting unique values associated with a key</span></h3><p>With distinct-values, it is then possible to eliminate duplicates and look at unique values:<br>distinct-values(collection( “<a target="_blank" rel="noopener" href="https://www.rumbledb.org/samples/git-archive.jsonl">https://www.rumbledb.org/samples/git-archive.jsonl</a>“ ).type)</p>
<h3><span id="aggregations">Aggregations</span></h3><p>Aggregations can be made on entire sequences with a single function call:The five basic functions are count, sum, avg, min, max.<br>count(distinct-values(collection( “<a target="_blank" rel="noopener" href="https://www.rumbledb.org/samples/git-archive.jsonl">https://www.rumbledb.org/samples/git-archive.jsonl</a>“ ).type))</p>
<h2><span id="construction">Construction</span></h2><h3><span id="construction-of-atomic-values">Construction of atomic values</span></h3><p>Atomic values that are core to JSON can be constructed with exactly the same syntax as JSON.</p>
<h3><span id="construction-of-objects-and-arrays">Construction of objects and arrays</span></h3><p>In fact, one can copy-paste any JSON value, and it will always be recognized as a valid JSONiq query returning that value.</p>
<h3><span id="construction-of-sequences">Construction of sequences</span></h3><p>Sequences can be constructed (and concatenated) using commas.Increasing sequences of integers can also be built with the to keyword.</p>
<h2><span id="scalar-expressions">Scalar expressions</span></h2><p>Sequences of items can have any number of items. A few JSONiq expression (arithmetic, logic, value comparison…) work on the particular case that a sequence has zero or one items.</p>
<h3><span id="arithmetic">Arithmetic</span></h3><p>JSONiq supports basic arithmetic: addition (+), subtraction (-), division (div), integer division (idiv) and modulo (mod).If the data types are different, then conversions are made automatically. The empty sequence enjoys special treatment: if one of the sides (or both) is the empty sequence, then the arithmetic expression returns an empty sequence (no error). If one of the two sides is null (and the other side is not the empty sequence), then the arithmetic expression returns null. If one of the sides (or both) is not a number, null, or the empty sequence, then a type error is thrown.</p>
<h3><span id="string-manipulation">String manipulation</span></h3><p>String concatenation is done with a double vertical bar: “foo” || “bar”.<br>Most other string manipulation primitives are available from the rich JSONiq builtin function library:<br>concat(“foo”, “bar”),string-join((“foo”, “bar”, “foobar”), “-“),substr(“foobar”, 4, 3),<br>string-length(“foobar”). </p>
<h3><span id="value-comparison">Value comparison</span></h3><p>Sequences of one atomic item can be compared with eq (equal), ne (not equal), le (lower or equal), gt (greater or equal), lt (lower than) and gt (greater than).</p>
<h3><span id="logic">Logic</span></h3><p>JSONiq supports the three basic logic expressions and, or, and not. not has the highest precedence, then and, then or.<br>JSONiq also supports universal and existential quantification:<br>every $i in 1 to 10 satisfies $i gt 0, some $i in 1 to 10 satisfies $i gt 5. </p>
<p>If one of the two sides is not a sequence of a single Boolean item, then implicit conversions are made. This mechanism is called the Effective Boolean Value (EBV). For example, an empty sequence, or a sequence of one empty string, or a sequence of one zero integer, is considered false. A sequence of one non-empty string, or a sequence or one non-zero integer, or a sequence starting with one object (or array) is considered true.</p>
<h3><span id="general-comparison">General comparison</span></h3><p>JSONiq has a shortcut for existential quantification on value comparisons. This is called general comparison.</p>
<p>some $i in (1, 2, 3, 4, 5) satisfies $i eq 1 ==<br>(1, 2, 3, 4, 5) = 1.</p>
<h2><span id="composability">Composability</span></h2><h3><span id="data-flow">Data flow</span></h3><p>A few expressions give some control over the data flow by picking the output or this or that expression based on the value of another expression. This includes conditional expressions. This includes conditional expressions. This also includes try-catch expressions.</p>
<h2><span id="binding-variables-with-cascades-of-let-clauses">Binding variables with cascades of let clauses</span></h2><p>Variables in JSONiq start with a dollar sign. It is important to understand that this is not a variable assignment that would change the value of a variable. This is only a declarative binding. </p>
<h2><span id="flwor-expressions">FLWOR expressions</span></h2><p>One of the most important and powerful features of JSONiq is the FLWOR expression. It corresponds to SELECT-FROM-WHERE queries in SQL, however, it is considerably more expressive and generic than them in several aspects. In JSONiq the clauses can appear in any order with the exception of the first and last clause. JSONiq supports a let clause, which does not exist in SQL.<br>In SQL, when iterating over multiple tables in the FROM clause, they “do not see each other”. In JSONiq, for clauses (which correspond to FROM clauses in SQL), do see each other, meaning that it is possible to iterate in higher and higher levels of nesting by referring to a previous for variable.</p>
<h3><span id="for-clauses">For clauses</span></h3><p>It can thus be seen that the for clause is akin to the FROM clause in SQL, and the return is akin to the SELECT clause. Projection in JSONiq can be made with a project() function call, with the keys to keep.  It is possible to implement a join with a sequence of two for clauses and a predicate. Note that allowing empty can be used to perform a left outer join, i.e., to account for the case when there are no matching records in the second collection. </p>
<h3><span id="let-clauses">Let clauses</span></h3><p>A let clause outputs exactly one outgoing tuple for each incoming tuple (think of a map transformation in Spark). Let clauses also allow for joining the two datasets. </p>
<h3><span id="where-clauses">Where clauses</span></h3><p>Where clauses are used to filter variable bindings (tuples) based on a predicate on these variables. They are the equivalent to a WHERE clause in SQL.</p>
<h3><span id="order-by-clauses">Order by clauses</span></h3><p>Order by clauses are used to reorganize the order of the tuples, but without altering them. They are the same as ORDER BY clauses in SQL.It is also possible, like in SQL, to specify an ascending or a descending order. In case of ties between tuples, the order is arbitrary. But it is possible to sort on another variable in case there is a tie with the first one (compound sorting keys). It is possible to control what to do with empty sequences: they can be considered smallest or greatest.</p>
<h3><span id="group-by-clauses">Group by clauses</span></h3><p>Group by clauses organize tuples in groups based on matching keys, and then output only one tuple for each group, aggregating other variables (count, sum, max, min…). It is also possible to opt out of aggregating other (non-grouping-key) variables.</p>
<h2><span id="types">Types</span></h2><h3><span id="variable-types">Variable types</span></h3><p>Since every value in JSONiq is a sequence of item, a sequence type consists of two parts: an item type, and a cardinality.<br>Item types can be any of the builtin atomic types. as well as “object”, “array” and the most generic item type, “item”.<br>Cardinality can be one of the following four:Any number of items (suffix <em>); for example object</em>, One or more items (suffix +); for example array+,Zero or one item (suffix ?); for example integer,Exactly one item (no suffix); for example boolean?</p>
<h3><span id="type-expressions">Type expressions</span></h3><p>An “instance of” expression checks whether a sequences matches a sequence type, and returns true or false. A “cast as” expression casts single items to an expected item type.<br>A “castable as” expression tests whether a cast would succeed (in which case it returns true) or not (false).<br>A “treat as” expression checks whether its input sequence matches an expected type (like a type on a variable); if it does, the input sequence is returned unchanged. If not, an error is raised.</p>
<h3><span id="types-in-user-defined-functions">Types in user-defined functions</span></h3><p>JSONiq supports user-defined functions. Parameter types can be optionally specified, and a return type can also be optionally specified.</p>
<h3><span id="validating-against-a-schema">Validating against a schema</span></h3><p>It is possible to declare a schema, associating it with a user-defined type, and to validate a sequence of items against this user-defined type.</p>
<h2><span id="architecture-of-a-query-engine">Architecture of a query engine</span></h2><h3><span id="static-phase">Static phase</span></h3><p>When a query is received by an engine, it is text that needs to be parsed. The output of this is a tree structure called an Abstract Syntax Tree. An Abstract Syntax Tree, even though it already has the structure of a tree, is tightly tied to the original syntax. Thus, it needs to be converted into a more abstract Intermediate Representation called an expression tree. Every node in this tree corresponds to either an expression or a clause in the JSONiq language, making the design modular. At this point, static typing takes place, meaning that the engine infers the static type of each expression, that is, the most specific type possible expected at runtime (but without actually running the program). Engines like RumbleDB perform their optimization round on this Intermediate Representation. Once optimizations have been done, RumbleDB decides the mode with which each expression and clause will be evaluated (locally, sequentially, in parallel, in DataFrames, etc). The resulting expression tree is then converted to a runtime iterator tree; this is the query plan that will actually be evaluated by the engine.</p>
<h3><span id="dynamic-phase">Dynamic phase</span></h3><p>During the dynamic phase, the root of the tree is asked to produce a sequence of items, which is to be the final output of the query as a whole. Then, recursively, each node in the tree will ask its children to produce sequences of items (or tuple streams). Each node then combines the sequences of items (or tuple streams) it receives from its children in order to produce its own sequence of items according to its semantics, and pass it to its parent. </p>
<h4><span id="materialization">Materialization</span></h4><p>When a sequence of items is materialized, it means that an actual List (or Array, or Vector), native to the language of implementation (in this case Java) is stored in local memory, filled with the items. </p>
<h4><span id="streaming">Streaming</span></h4><p>When a sequence of items (or tuple stream) is produced and consumed in a streaming fashion, it means that the items (or tuples) are produced and consumed one by one, iteratively. But the whole sequence of items (or tuple stream) is never stored anywhere. The classical pattern for doing so is known as the Volcano iterator architecture.</p>
<p>However, there are two problems with this: first, it can take a lot of time to go through the entire sequence (imagine doing so with billions or trillions of items). Second, there are expressions or clauses that are not compatible with streaming (consider, for example, the group by or order by clause, which cannot be implemented without materializing their full input).</p>
<h4><span id="parallel-execution-with-rdds">Parallel execution (with RDDs)</span></h4><p>When a sequence becomes unreasonably large, RumbleDB switches to a parallel execution, leveraging Spark capabilities: the sequences of items are passed and processed as RDDs of Item objects.</p>
<h4><span id="parallel-execution-with-dataframes">Parallel execution (with DataFrames)</span></h4><p>The RDD implementation supports heterogeneous sequences by leveraging the polymorphism of Item objects. However, this is not efficient in the case that Items in the same sequence happen to have a regular structure. Thus, if the Items in a sequence are valid against a specific schema, or even against an array type or an atomic type, the underlying physical storage in memory relies on Spark DataFrames instead of RDDs.</p>
<p>To summarize, homogeneous sequences of the most common types are stored in DataFrames, and RDDs are used in all other cases.</p>
<h4><span id="parallel-execution-with-native-sql">Parallel execution (with Native SQL)</span></h4><p>In some cases (more in every release), RumbleDB is able to evaluate the query using only Spark SQL, compiling JSONiq to SQL directly instead of packing Java runtime iterators in UDFs. This leads to faster execution, because UDFs are slower than a native execution in SQL. This is because, to a SQL optimizer, UDFs are opaque and prevent automatic optimizations.</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-11-28T13:19:33.000Z" title="2023-11-28 2:19:33 ├F10: PM┤">2023-11-28</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-12-12T15:35:34.301Z" title="2023-12-12 4:35:34 ├F10: PM┤">2023-12-12</time></span><span class="level-item"><a class="link-muted" href="/categories/database/">database</a></span><span class="level-item">17 minutes read (About 2537 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/11/28/bigdata9/">bigdata - MongoDB</a></p><div class="content"><h1><span id="document-stores">Document stores</span></h1><p>Can we rebuild a similar system for collections of trees, in the sense that we drop all three constraints: relational integrity, domain integrity, and atomic integrity? Document stores bring us one step in this direction.</p>
<h2><span id="challenges">Challenges</span></h2><h3><span id="schema-on-read">Schema on read</span></h3><p>In a relational database management system, it is not possible to populate a table without having defined its schema first. However, when encountering such denormalized data, in the real world, there is often no schema. In fact, one of the important features of a system that deals with denormalized data is the ability to discover a schema.</p>
<h3><span id="making-trees-fit-in-tables">Making trees fit in tables</span></h3><p>Several XML elements (or, likewise, several JSON objects) can be naturally mapped to a relational table with several rows if the collection is flat and homogeneous, but semi-structured data can generally be nested and heterogeneous. if we map nested and heterogeneous into a table,such mapping will at best have to be done for every single dataset, and requires in most cases a schema, whereas we are looking for a generic solution for semistructured data with no a-priori schema information.</p>
<h2><span id="document-stores">Document stores</span></h2><p>Document stores provide a native database management system for semi-structured data. Document stores work on collections of records, generalizing the way that relational tables can be seen as collections of rows. It is important to understand that document stores are optimized for the typical use cases of many records of small to medium sizes. Typically, a collection can have millions or billions of documents, while each single document weighs no more than 16 MB (or a size in a similar magnitude). Finally, a collection of documents need not have a schema: it is possible to insert random documents that have dissimilar structures with no problem at all. Most document stores, however, do provide the ability to add a schema. Document stores can generally do selection, projection, aggregation and sorting quite well, but many of them are typically not (yet) optimized for joining collections. In fact, often, their language or API does not offer any joining functionality at all, which pushes the burden to reimplement joins in a host language to the users. This is a serious breach of data independence.</p>
<h2><span id="implementations">Implementations</span></h2><p>There is a very large number of products in the document store space for both JSON and XML, let us mention for example MongoDB, CouchDB, ElasticSearch, Cloudant, existDB, ArangoDB, BaseX, MarkLogic, DocumentDB, CosmosDB, and so on. We will focus, as an example, on MongoDB.</p>
<h2><span id="physical-storage">Physical storage</span></h2><p>Just like the storage format is optimized for tabular data in a relational database management system, it is optimized for tree-like data in a document store. In MongoDB, the format is a binary version of JSON called BSON. BSON is basically based on a sequence of tokens that efficiently encode the JSON constructs found in a document. The immediate benefit of BSON is that it takes less space in storage than JSON stored as a text file: for example, null, true and false literals need four or five bytes in text format at best, while they can be efficiently encoded as single bytes in BSON. Furthermore, BSON supports additional types that JSON does not have, such as dates. </p>
<h2><span id="querying-paradigm-crud">Querying paradigm (CRUD)</span></h2><p>The API of MongoDB, like many document stores, is based on the CRUD paradigm. CRUD means Create, Read, Update, Delete and corresponds to low-level primitives similar to those for HBase. MongoDB supports several host languages to query collections via an API. This includes in particular JavaScript and Python, but many other languages are supported via drivers. We will use JavaScript here because this is the native host language of MongoDB. It is important to note that these APIs are not query languages. MongoDB also provides access to the data via a shall called mongo or, newly, mongosh. This is a simple JavaScript interface wrapped around the MongoDB’s node.js driver. </p>
<h3><span id="populating-a-collection">Populating a collection</span></h3><p>To create a collection, one can simply insert a document in it, and it will be automatically created if it does not exist.</p>
<p>MongoDB automatically adds to every inserted document a special field called “ id” and associated with a value called an Object ID and with a type of its own.Object IDs are convenient for deleting or updating a specific document with no ambiguity.</p>
<h3><span id="querying-a-collection">Querying a collection</span></h3><h4><span id="scan-a-collection">Scan a collection</span></h4><p>Asking for the contents of an entire collection is done with a simple find() call on the previous object:db.collection.find().This function does not in fact return the entire collection; rather, it returns some pointer, called a cursor, to the collection; the user can then iterate on the cursor in an imperative fashion in the host language.</p>
<h4><span id="selection">Selection</span></h4><p>It is possible to perform a selection on a collection by passing a parameter to find() that is a JSON object:<br>db.collection.find({ “Theory” : “Relativity” }). </p>
<p>A disjunction (OR) uses a special MongoDB keyword, prefixed with a dollar sign:<br>db.collection.find( { “$or” : [ { “Theory”:”Relativity” }, { “Last”:”Einstein” } ] } ).</p>
<p>MongoDB offers many other keywords, for example for comparison other than equality:<br>db.collection.find( { “Publications” : { “$gte” : 100 } } )</p>
<h4><span id="projection">Projection</span></h4><p>Projections are made with the second parameter of this same find() method. This is done in form of a JSON object associating all the desired keys in the projection with the value 1. db.scientists.find( { “Theory” : “Relativity” }, { “First” : 1, “Last”: 1 } ).</p>
<p>It is also possible to project fields away in the same way with 0s, however 1s and 0s cannot be mixed in the projection parameter, except in the specific above case of projecting away the object ID</p>
<h4><span id="counting">Counting</span></h4><p>Counting can be done by chaining a count() method call:<br>db.scientists.find( { “Theory” : “Relativity” } ).count().</p>
<h4><span id="sorting">Sorting</span></h4><p>Sorting can be done by chaining a sort() method call.<br>db.scientists.find( { “Theory” : “Relativity” }, { “First” : 1, “Last” : 1 } ).sort( { “First” : 1, “Name” : -1 } )</p>
<p>1 is for ascending order and -1 for descending order.It is also possible to add limits and offsets to paginate results also by chaining more method calls:<br>db.scientists.find( { “Theory” : “Relativity” }, { “First” : 1, “Last” : 1 } ).sort( { “First” : 1, “Name” : -1 } ).skip(30).limit(10).</p>
<p>Note that, contrary to intuition, the order of the calls does not matter, as this is really just the creation of a query plan by providing parameters (in any order).</p>
<h4><span id="duplicate-elimination">Duplicate elimination</span></h4><p>It is possible to obtain all the distinct values for one field with a distinct() call: db.scientists.distinct(“Theory”)</p>
<h3><span id="querying-for-heterogeneity">Querying for heterogeneity</span></h3><h4><span id="absent-fields">Absent fields</span></h4><p>Absent fields can be filtered with: db.scientists.find( { “Theory” : null } ). </p>
<h4><span id="filtering-for-values-across-types">Filtering for values across types</span></h4><p>db.collection.find( { “$or” : [ { “Theory”: “Relativity” }, { “Theory”: 42 }, { “Theory”: null } ] } )</p>
<p>db.scientists.find( { “Theory” : { “$in” : [“Relativity”, 42, null ] } } ).</p>
<p>MongoDB is also able to sort on fields that have heterogeneous data types. It does so by first order by type in some (arbitrary, but documented) order, and then within each type.</p>
<h3><span id="querying-for-nestedness">Querying for nestedness</span></h3><p>Nestedness in MongoDB is handled in several ad-hoc ways for specific use cases.</p>
<h4><span id="values-in-nested-objects">Values in nested objects</span></h4><p>We saw how to select documents based on values associated with a top-level keys. What about values that are not at the top-level, but in nested objects?<br>The first solution that might come to mind is something like this: db.scientists.find({ “Name” : { “First” : “Albert” } }) However, this query will not have the behavior many would have expected: instead of finding documents that have a value “Albert” associated with the key “First” in an object itself associated with the top-level key ”Name”, this query looks for an exact match of the entire object.<br>In order to include documents such as above, MongoDB uses a dot syntax: db.scientists.find({ “Name.First” : “Albert” }).</p>
<h4><span id="values-in-nested-arrays">Values in nested arrays</span></h4><p>MongoDB allows to filter documents based on whether a nested array contains a specific value, like so: db.scientists.find({ “Theories” : “Special relativity” }). </p>
<h4><span id="deleting-objects-from-a-collection">Deleting objects from a collection</span></h4><p>Objects can be deleted from a collection either one at a time with deleteOne(), or several at a time with deleteMany(): db.scientists.deleteMany( { “century” : “15” } ). </p>
<h4><span id="updating-objects-in-a-collection">Updating objects in a collection</span></h4><p>Documents can be updated with updateOne() and updateMany() by providing both a filtering criterion (with the same syntax as the first parameter of find()) and an update to apply. The command looks like so: db.scientists.updateMany( { “Last” : “Einstein” }, { $set : { “Century” : “20” } } ).</p>
<p>The granularity of updates is per document, that is, a single document can be updated by at most one query at the same time.However, within the same collection, several different documents can be modified concurrently by different queries in parallel.</p>
<h4><span id="complex-pipelines">Complex pipelines</span></h4><p>For grouping and such more complex queries, MongoDB provides an API in the form of aggregation pipelines.<br>db.scientists.aggregate( { $match : { “Century” : 20 }}, { $group : { “Year” : “$year”, “Count” : { “$sum” : 1 } } }, { $sort : { “Count” : -1 } }, { $limit : 5 } ).</p>
<h3><span id="limitations-of-a-document-store-querying-api">Limitations of a document store querying API</span></h3><p>Simple use cases are straightforward to handle, however more complex use cases require a lot of additional code in the host language, be it JavaScript or Python. An example is that joins must be taken care of by the end user in the host language: the burden of implementing more complex use cases is pushed to the end user.</p>
<h2><span id="architecture">Architecture</span></h2><p>The architecture of MongoDB follows similar principles to what we covered before: scaling out the hardware to multiple machine, and sharding as well as replicating the data.</p>
<h3><span id="sharding-collections">Sharding collections</span></h3><p>Collections in MongoDB can be sharded. Shards are determined by selecting one or several fields. (Lexicographically-ordered) intervals over these fields then determine the shards. The fields used to shard must be organized in a tree index structure.</p>
<h3><span id="replica-sets">Replica sets</span></h3><p>A replica set is a set of several nodes running the MongoDB server process. The nodes within the same replica set all have a copy of the same data.</p>
<p>Each shard of each collection is assigned to exactly one replica set. Note that this architecture is not the same as that of HDFS, in which the replicas are spread over the entire cluster with no notion of “walls” between replica sets and no two DataNodes having the exact same block replicas.</p>
<h3><span id="write-concerns">Write concerns</span></h3><p>When writing (be it delete, update or insert) to a collection, more exactly, to a specific shard of a collection, MongoDB checks that a specific minimum number of nodes (within the replica set that is responsible for the shard) have successfully performed the update.</p>
<h3><span id="motivation">Motivation</span></h3><p>A document store, unlike a data lake, manages the physical data layout. This has a cost: the need to import (ETL) data before it is possible to query it, but this cost comes with a nice benefit: index support, just like relational database management systems.</p>
<h3><span id="hash-indices">Hash indices</span></h3><p>Hash indices are used to optimize point queries and more generally query that select on a specific value of a field. The general idea is that all the values that a field takes in a specific collection can be hashed to an integer. The value, together with pointers to the corresponding documents, is then placed in a physical array in memory, at the position corresponding to this integer.</p>
<h3><span id="tree-indices">Tree indices</span></h3><p>Hash indices are great and fast, but have limitations: first, they consume space. The more one wants to avoid hash collisions, the larger the array needs to be. But more importantly, hash indices cannot support range queries. This is because hashes do not preserve the order of the values and distribute them “randomly” in the index array structure.</p>
<p>Range queries are supported with tree indices. Instead of an array, tree indices use some sort of tree structure in which they arrange the possible values of the indexed field, such that the values are ordered when traversing the tree in a depth-first-search manner. More precisely, the structure is called a B+-tree. Unlike a simple binary tree, nodes have a large number of children.</p>
<h3><span id="secondary-indices">Secondary indices</span></h3><p>By default, MongoDB always builds a tree index for the id field. Users can request to build hash and tree indices for more fields. These indices are called secondary indices.</p>
<p>The command for building a hash index looks like so: db.scientists.createIndex({ “Name.Last” : “hash” })</p>
<p>And for a tree index (1 means in ascending order, -1 would be descending): db.scientists.createIndex({ “Name.Last” : 1 }).</p>
<h3><span id="when-are-indices-useful">When are indices useful</span></h3><p>When building indices, it is important to get a feeling for whether a query will be faster or not with this index.</p>
<h3><span id="index-types">index types</span></h3><h4><span id="single-field-indexes">Single Field Indexes</span></h4><p>By default, all collections have an index on the _id field. You can add additional indexes to speed up important queries and operations. You can create a single-field index on any field in a document, including:Top-level document fields, Embedded documents ,Fields within embedded documents. When you create an index on an embedded document, only queries that specify the entire embedded document use the index. Queries on a specific field within the document do not use the index. In order for a dot notation query to use an index, you must create an index on the specific embedded field you are querying, not the entire embedded object. </p>
<h4><span id="compound-indexes">Compound Indexes</span></h4><p>Compound indexes collect and sort data from two or more fields in each document in a collection. Data is grouped by the first field in the index and then by each subsequent field.</p>
<p>The order of the indexed fields impacts the effectiveness of a compound index. Compound indexes contain references to documents according to the order of the fields in the index. To create efficient compound indexes, follow the ESR (Equality, Sort, Range) rule. The ESR (Equality, Sort, Range) Rule is to place fields that require exact matches first in your index. Sort follows equality matches because the equality matches reduce the number of documents that need to be sorted. Sorting after the equality matches also allows MongoDB to do a non-blocking sort. “Range” filters scan fields. The scan doesn’t require an exact match, which means range filters are loosely bound to index keys. To improve query efficiency, make the range bounds as tight as possible and use equality matches to limit the number of documents that must be scanned. MongoDB cannot do an index sort on the results of a range filter. Place the range filter after the sort predicate so MongoDB can use a non-blocking index sort.  </p>
<p>Compound indexes cannot support queries where the sort order does not match the index or the reverse of the index. </p>
<p>Index prefixes are the beginning subsets of indexed fields. Compound indexes support queries on all fields included in the index prefix. Index fields are parsed in order; if a query omits an index prefix, it is unable to use any index fields that follow that prefix.</p>
<h4><span id="multikey-indexes">Multikey Indexes</span></h4><p>Multikey indexes collect and sort data from fields containing array values. Multikey indexes improve performance for queries on array fields.</p>
<p>In a compound multikey index, each indexed document can have at most one indexed field whose value is an array. </p>
<h2><span id="exercise">exercise</span></h2><h3><span id="data-model">data model</span></h3><h4><span id="embedded-data-models">Embedded Data Models</span></h4><p>In general, embedding provides better performance for read operations, as well as the ability to request and retrieve related data in a single database operation. Embedded data models make it possible to update related data in a single atomic write operation.</p>
<h4><span id="normalized-data-models">Normalized Data Models</span></h4><p>Normalized data models describe relationships using references between documents.</p>
<h2><span id="references">references</span></h2><p><a target="_blank" rel="noopener" href="https://www.mongodb.com/docs/manual/core/data-model-design/">https://www.mongodb.com/docs/manual/core/data-model-design/</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-11-14T12:11:13.000Z" title="2023-11-14 1:11:13 ├F10: PM┤">2023-11-14</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-12-12T15:35:32.596Z" title="2023-12-12 4:35:32 ├F10: PM┤">2023-12-12</time></span><span class="level-item"><a class="link-muted" href="/categories/database/">database</a></span><span class="level-item">11 minutes read (About 1720 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/11/14/bigdata8/">bigdata - spark</a></p><div class="content"><h1><span id="generic-dataflow-management">Generic Dataflow Management</span></h1><p>MapReduce is very simple and generic, but many more complex uses involve not just one, but a sequence of several MapReduce jobs. Furthermore, the MapReduce API is low-level, and most users need higherlevel interfaces, either in the form of APIs or query languages. This is why, after MapReduce, another generation of distributed processing technologies were invented. The most popular one is the open source Apache Spark.</p>
<h2><span id="a-more-general-dataflow-model">A more general dataflow model</span></h2><p>MapReduce consists of a map phase, followed by shuffling, followed by a reduce phase. In fact, the map phase and the reduce phase are not so different: both involve the computation of tasks in parallel on slots.</p>
<h2><span id="resilient-distributed-datasets">Resilient distributed datasets</span></h2><p>The first idea behind generic dataflow processing is to allow the dataflow to be arranged in any distributed acyclic graph (DAG).</p>
<p><img src="/2023/11/14/bigdata8/image-28.png" alt="Alt text"></p>
<p>All the rectangular nodes in the above graph correspond to intermediate data. They are called resilient distributed datasets, or in short, RDDs.</p>
<p>A major difference with MapReduce, though, is that RDDs need not be collections of pairs. In fact, RDDs can be (ordered) collections of just about anything: strings, dates, integers, objects, arrays, arbitrary JSON values, XML nodes, etc.The only constraint is that the values within the same RDD share the same static type, which does not exclude the use of polymorphism.</p>
<h2><span id="the-rdd-lifecycle">The RDD lifecycle</span></h2><h3><span id="creation">Creation</span></h3><p>RDDs undergo a lifecycle. First, they get created. RDDs can be created by reading a dataset from the local disk, or from cloud storage, or from a distributed file system, or from a database source, or directly on the fly from a list of values residing in the memory of the client using Apache Spark</p>
<h3><span id="transformation">Transformation</span></h3><p>Then, RDDs can be transformed into other RDDs. Mapping or reducing, in this model, become two very specific cases of transformations. However, Spark allows for many, many more kinds of transformations. This also includes transformations with several RDDs as input.</p>
<h3><span id="action">Action</span></h3><p>RDDs can also undergo a final action leading to making an output persistent. This can be by outputting the contents of an RDD to the local disk, to cloud storage, to a distributed file system, to a database system, or directly to the screen of the user.</p>
<h3><span id="lazy-evaluation">Lazy evaluation</span></h3><p>Another important aspect of the RDD lifecycle is that the evaluation is lazy: in fact, creations and transformations on their own do nothing. It is only with an action that the entire computation pipeline is put into motion, leading to the computation of all the necessary intermediate RDDs all the way down to the final output corresponding to the action.</p>
<h2><span id="transformations">Transformations</span></h2><h3><span id="unary-transformations">Unary transformations</span></h3><p>Let us start with unary transformations, i.e., those that take a single RDD as their input.</p>
<h3><span id="binary-transformations">Binary transformations</span></h3><p>There are also transformations that take two RDDs as input.</p>
<h3><span id="pair-transformations">Pair transformations</span></h3><p>Spark has transformations specifically tailored for RDDs of key-value pairs.</p>
<h2><span id="actions">Actions</span></h2><h3><span id="gathering-output-locally">Gathering output locally</span></h3><p>The collect action downloads all values of an RDD on the client machine and outputs them as a (local) list. It is important to only call this action on an RDD that is known to be small enough (e.g., after filtering) to avoid a memory overflow.</p>
<p>The count action computes (in parallel) the total number of values in the input RDD. This one is safe even for RDDs with billions of values, as it returns a simple integer to the client.</p>
<h3><span id="writing-to-sharded-datasets">Writing to sharded datasets</span></h3><p>There is also an action called saveAsTextFile that can save the entire RDD to a sharded dataset on Cloud storage (S3, Azure blob storage) or a distributed file system (HDFS).<br>Binary outputs can be saved with saveAsObjectFile.</p>
<h3><span id="actions-on-pair-rdds">Actions on Pair RDDs</span></h3><h2><span id="physical-architecture">Physical architecture</span></h2><h3><span id="narrow-dependency-transformations">Narrow-dependency transformations</span></h3><p>In a narrow-dependency transformation, the computation of each output value involves a single input value. In a narrow-dependency transformation, the computation of each output value involves a single input value.</p>
<p>By default, if the transformation is applied to an RDD freshly created from reading a dataset from HDFS, each partition will correspond to an HDFS block. Thus, the computation of the narrow-dependency transformation mostly involves local reads by short-circuiting HDFS.</p>
<p>In fact, the way this works is quite similar to MapReduce: the sequential calls of the transformation function on each input value within a single partition is called a task. And just like MapReduce, the tasks are assigned to slots. These slots correspond to cores within YARN containers. YARN containers used by Spark are called executors. The processing of the tasks is sequential within each executor, and tasks are executed in parallel across executors. And like in MapReduce, a queue of unprocessed tasks is maintained, and everytime a slot is done, it gets a new task. When all tasks have been assigned, the slots who are done become idle and wait for all others to complete.</p>
<h3><span id="chains-of-narrow-dependency-transformations">Chains of narrow-dependency transformations</span></h3><p>In fact, on the physical level, the physical calls of the underlying map/filter/etc functions are directly chained on each input value to directly produce the corresponding final, output value, meaning that the intermediate RDDs are not even materialized anywhere and exist purely logically.</p>
<p>Such a chain of narrow-dependency transformations executed efficiently as a single set of tasks is called a stage, which would correspond to what is called a phase in MapReduce.</p>
<h3><span id="physical-parameters">Physical parameters</span></h3><p>Users can parameterize how many executors there are, how many cores there are per executor and how much memory per executor (remember that these then correspond to YARN container requests).</p>
<h3><span id="shuffling">Shuffling</span></h3><p>What about wide-dependency transformations? They involve a shuffling of the data over the network, so that the data necessary to compute each partition of the output RDD is physically at the same location.Thus, on the high-level of a job, the physical execution consists of a sequence of stages, with shuffling happening everytime a new stage begins.</p>
<h2><span id="optimizations">Optimizations</span></h2><h3><span id="pinning-rdds">Pinning RDDs</span></h3><p>Everytime an action is triggered, all the computations of the ”reverse transitive closure” (i.e., all the way up the DAG through the reverted edges) are set into motion. In some cases, several actions might share subgraphs of the DAG. It makes sense, then, to “pin” the intermediate RDD by persisting it.</p>
<h3><span id="pre-partitioning">Pre-partitioning</span></h3><p>Shuffle is needed to bring together the data that is needed to jointly contribute to individual output values. If, however, Spark knows that the data is already located where it should be, then shuffling is not needed.</p>
<h2><span id="dataframes-in-spark">DataFrames in Spark</span></h2><h3><span id="data-independence">Data independence</span></h3><p>Unlike a relational database that has everything right off-theshelf, with RDDs, the user has to re-implement all the primitives they need. This is a breach of the data independence principle. The developers behind Spark addressed this issue in a subsequent version of Spark by extending the model with support for DataFrames and Spark SQL, bringing back a widely established and popular declarative, high-level language into the ecosystem.</p>
<h3><span id="a-specific-kind-of-rdd">A specific kind of RDD</span></h3><p>A DataFrame can be seen as a specific kind of RDD: it is an RDD of rows (equivalently: tuples, records) that has relational integrity, domain integrity, but not necessarily atomic integrity.</p>
<h3><span id="performance-impact">Performance impact</span></h3><p>DataFrames are stored column-wise in memory, meaning that the values that belong to the same column are stored together. Furthermore, since there is a known schema, the names of the attributes need not be repeated in every single row, as would be the case with raw RDDs. DataFrames are thus considerably more compact in memory than raw RDDs.</p>
<p>Generally, Spark converts Spark SQL to internal DataFrame transformation and eventually to a physical query plan. An optimizer known as Catalyst is then able to find many ways of making the execution faster.</p>
<h3><span id="input-formats">Input formats</span></h3><p>Note that Spark automatically infers the schema from discovering the JSON Lines file, which adds a static performance overhead that does not exist for raw RDDs. </p>
<h3><span id="dataframe-transformations">DataFrame transformations</span></h3><p>It is also possible, instead of Spark SQL, to use a transformation API similar to (but distinct from) the RDD transformation API.</p>
<p>Unlike the RDD transformation API, there is no guarantee that the execution will happen as written, as the optimizer is free to reorganize the actual computations.</p>
<h3><span id="dataframe-column-types">DataFrame column types</span></h3><p>DataFrames also support the three structured types: arrays, structs, and maps. As a reminder, structs have string keys and arbitrary value types and correspond to generic JSON objects, while in maps, all values have the same type. Thus, structs are more common than maps.</p>
<h3><span id="the-spark-sql-dialect">The Spark SQL dialect</span></h3><p>Note that both GROUP BY and ORDER BY will trigger a shuffle in the system, even though this can be optimized as the grouping key and the sorting key are the same. The SORT BY clause can sort rows within each partition, but not across partitions, i.e., does not induce any shuffling. The DISTRIBUTE BY clause forces a repartition by putting all rows with the same value (for the specified field(s)) into the same new partition.<br>Note that the SORT BY clause is used to return the result rows sorted within each partition in the user specified order. When there is more than one partition SORT BY may return result that is partially ordered. This is different than ORDER BY clause which guarantees a total order of the output.</p>
<p>A word of warning must be given on SORT, DISTRIBUTE and CLUSTER clauses: they are, in fact, a breach of data independence, because they expose partitions. </p>
<p>Spark SQL also comes with language features to deal with nested arrays and objects. First, nested arrays can be navigated with the explode() function. Lateral views are more powerful and generic than just an explode() because they give more control, and they can also be used to go down several levels of nesting. A lateral view can be intuitively described this way: the array mentioned in the LATERAL VIEW clause is turned into a second, virtual table with the rest of the original table is joined. The other clauses can then refer to columns in both the original and second, virtual table.</p>
<h2><span id="exercise">exercise</span></h2><h3><span id="rdd">RDD</span></h3><p>Why RDD should be immutable and lazy: immutable is for lineage.<br>Why need RDD partitioning: parallel computing and reduce shuffling.</p>
<h3><span id="dataframe-api">DataFrame API</span></h3><p>For nested array,use array_contains.</p>
<h3><span id="spark-sql">spark SQL</span></h3><p>In jupyter notebook, we can use “%load_ext sparksql_magic” directly.</p>
<h2><span id="reference">reference</span></h2><p><a target="_blank" rel="noopener" href="https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/">https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-11-09T13:16:52.000Z" title="2023-11-9 2:16:52 ├F10: PM┤">2023-11-09</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-12-12T15:35:51.417Z" title="2023-12-12 4:35:51 ├F10: PM┤">2023-12-12</time></span><span class="level-item"><a class="link-muted" href="/categories/database/">database</a></span><span class="level-item">9 minutes read (About 1318 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/11/09/bigdata7/">bigdata - YARN</a></p><div class="content"><h1><span id="resource-management">Resource management</span></h1><p>The first version of MapReduce is inefficient in several respects. For this reason, the architecture was fundamentally changed by adding a resource management layer to the stack, adding one more level of decoupling between scheduling and monitoring. A resource management system, here YARN, is a very important building block not only for a better MapReduce, but also for many other technologies running on a cluster.</p>
<h2><span id="limitations-of-mapreduce-in-its-first-version">Limitations of MapReduce in its first version</span></h2><p>The JobTracker has a lot on its shoulders! It has to deal with resource management, scheduling, monitoring, the job lifecycle, and fault tolerance.The first consequence of this is scalability. The second consequence is the bottleneck that this introduces at the JobTracker level, which slows down the entire system. The third issue is that it is difficult to design a system that do many things well. The fourth issue is that resources are statically allocated to the Map or the Reduce phase, meaning that parts of the cluster remain idle during both phases. The fifth issue is the lack of fungibility between the Map phase and the Reduce phase.</p>
<h2><span id="yarn">YARN</span></h2><h3><span id="general-architecture">General architecture</span></h3><p>YARN means Yet Another Resource manager. It was introduced as an additional layer that specifically handles the management of CPU and memory resources in the cluster.</p>
<p>YARN, unsurprisingly, is based on a centralized architecture in which the coordinator node is called the ResourceManager, and the worker nodes are called NodeManagers. NodeManagers furthermore provide slots (equipped with exclusively allocated CPU and memory) known as containers.</p>
<p>When a new application is launched, the ResourceManager assigns one of the container to act as the ApplicationMaster which will take care of running the application. The ApplicationMaster can then communicate with the ResourceManager in order to book and use more containers in order to run jobs. This is a fundamental change from the initial MapReduce architecture, in which the JobTracker was also taking care of running the MapReduce job.</p>
<p><img src="/2023/11/09/bigdata7/image-27.png" alt="Alt text"></p>
<p>Thus, YARN cleanly separates between the general management of resources and bootstrapping new applications, which remains centralized on the coordinator node, and monitoring the job lifecycle, which is now delegated to one or more ApplicationMasters running concurrently. This means, in particular, that several applications can run concurrently in the same cluster. This ability, known as multi-tenancy, is very important for large companies or universities in order to optimize the use of their resources.</p>
<h3><span id="resource-management">Resource management</span></h3><p>In resource management, one abstracts away from hardware by distinguish between four specific resources used in a distributed database system. These four resources are: • Memory • CPU • Disk I/O • Network I/O.<br>ApplicationMasters can request and release containers at any time, dynamically. A container request is typically made by the ApplicationMasters with a specific demand. If the request is granted by the ResourceManager fully or partially, this is done indirectly by signing and issuing a container token to the ApplicationMaster that acts as proof that the resource was granted. The ApplicationMaster can then connect to the allocated NodeManager and send the token. The NodeManager will then check the validity of the token and provide the memory and CPU granted by the ResourceManager. The ApplicationMaster ships the code (e.g., as a jar file) as well as parameters, which then runs as a process with exclusive use of this memory and CPU.</p>
<h3><span id="job-lifecycle-management-and-fault-tolerance">Job lifecycle management and fault tolerance</span></h3><p>Version 2 of MapReduce works on top of YARN by leaving the job lifecycle management to an ApplicationMaster. The ApplicationMaster requests containers for the Map phase, and sets these containers up to execute Map tasks. As soon as a container is done executing a Map task, the ApplicationMaster will assign a new Map task to this container from the remaining queue, until no Map tasks are left.</p>
<p>a container in the Map phase can contain several Map slots. Sharing memory and containers across slots in this way improves the overall efficiency, because setting up a container adds latency. It is thus more efficient to allocate 10 containers of each 4 cores, compared to 40 containers of each 1 core.</p>
<p>In the event of a container crashing during the Map phase, the ApplicationMaster will handle this by re-requesting containers and restarting the failed tasks. In the case that some data is lost in the Reduce phase, it is possible that the entire job must be restarted, because this the only way to recreate the intermediate data is to re-execute the Map tasks.</p>
<h3><span id="scheduling">Scheduling</span></h3><p>The ResourceManager keeps track of the list of available NodeManagers (who can dynamically come and go) and their status. Just like in HDFS, NodeManagers send periodic heartbeats to the ResourceManager to give a sign of life. The ResourceManager also offers an interface so that ApplicationMasters can register and send container requests. ApplicationMasters also send periodic heartbeats to the ResourceManager. It is important to understand that, unlike the JobTracker, the ResourceManager does not monitor tasks, and will not restart slots upon failure. This job is left to the ApplicationMasters.</p>
<h2><span id="scheduling-strategies">Scheduling strategies</span></h2><h3><span id="fifo-scheduling">FIFO scheduling</span></h3><p>In FIFO scheduling, there is one application at a time running on the entire cluster. When it is done, the next application runs again on the entire cluster, and so on.</p>
<h3><span id="capacity-scheduling">Capacity scheduling</span></h3><p>In capacity scheduling, the resources of the cluster are partitioned into several sub-clusters of various sizes. Each one of these sub-clusters has its own queue of applications running in a FIFO fashion within this queue. Capacity scheduling also exists in a more “dynamic flavour” in which, when a sub-cluster is not currently used, its resources can be temporarily lent to the other sub-clusters. This is also in the spirit of usage maximization. </p>
<h3><span id="fair-scheduling">Fair scheduling</span></h3><p>Fair scheduling involves more complex algorithms that attempt to allocate resources in a way fair to all users of the cluster and based on the share they are normally entitled to. fair scheduling consists on making dynamic decisions regarding which requests get granted and which requests have to wait. Fair scheduling combines several ways to compute cluster shares: Steady fair share: this is the share of the cluster officially allocated to each department. Instantaneous fair share: this is the fair share that a department should ideally be allocated (according to economic and game theory considerations) at any point in time. This is a dynamic number that changes constantly, based on departments being idle: if a department is idle, then the instantaneous fair share of others department becomes higher than their steady fair shares. Current share: this is the actual share of the cluster that a department effectively uses at any point in time. This is highly dynamic. The current share does not necessarily match the instantaneous fair share because there is some inertia in the process: a department might be using more resources while another is idle. When the other department later stops being idle, these resources are not immediately withdrawn from the first department; rather, the first department will stop getting more resources, and the second department will gradually recover these resources as they get released by the first department.</p>
<p>The easiest case of fair scheduling is when only one resource is considered: for example, only CPU cores, or only memory. Things become more complicated when several resources are considered, in practice both CPU cores and memory. This problem was solved game-theoretically with the Dominant Resource Fairness algorithm. The two (or more) dimensions are projected again to a single dimension by looking at the dominant resource for each user.</p>
<h2><span id="exercise">exercise</span></h2><h3><span id="motivation">motivation</span></h3><p>improve hadoop 1.x by adding a layer YARN to separate resource management from data processing.</p>
<h3><span id="architecture">architecture</span></h3><p>NodeMananger is generalized taskTracker.<br>On ResourceManager, there is an ApplicationManager rsponsible for admiting new jobs and collecting finished jobs and logs.<br>A scheduler has a global view to assign an ApplicationMaster. The applicationMaster will compute how many resources needed and send a request to the scheduler. </p>
<p>fault tolerance is provided by applicationMaster+NodeManager+HDFS.</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-10-31T15:42:31.000Z" title="2023-10-31 4:42:31 ├F10: PM┤">2023-10-31</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-11-09T13:17:41.248Z" title="2023-11-9 2:17:41 ├F10: PM┤">2023-11-09</time></span><span class="level-item"><a class="link-muted" href="/categories/database/">database</a></span><span class="level-item">11 minutes read (About 1582 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/10/31/bigdata6/">bigdata - Map Reduce</a></p><div class="content"><h2><span id="patterns-of-large-scale-query-processing">Patterns of large-scale query processing</span></h2><h3><span id="textual-input">Textual input</span></h3><p>We saw that the data we want to query can take many forms. First, it can be billions of lines of text. It can also be plenty of CSV lines. Some other formats (e.g., Parquet, …) can be binary. We also encountered HFiles in Chapter 6, which are lists of keyvalue pairs. In fact, Hadoop has another such kind of key-value-based format called Sequence File, which is simply a list of key-values, but not necessarily sorted by key (although ordered) and with keys not necessarily unique.</p>
<h3><span id="shards">Shards</span></h3><p>There are several practical motivations for the many-files pattern even in HDFS.</p>
<h3><span id="querying-pattern">Querying pattern</span></h3><p>This is the motivation behind the standard MapReduce pattern: a map-like phase on the entire input, then a shuffle phase on the intermediate data, then another map-like phase (called reduce) producing the entire output</p>
<h2><span id="mapreduce-model">MapReduce Model</span></h2><p>In MapReduce, the input data, intermediate data, and output data are all made of a large collection of key-value pairs (with the keys not necessarily unique, and not necessarily sorted by key). The types of the keys and values are known at compile-time (statically), and they do not need to be the same across all three collections. </p>
<h2><span id="mapreduce-architecture">MapReduce architecture</span></h2><p>On a cluster, the architecture is centralized, just like for HDFS and HBase. In the original version of MapReduce, the main node is called JobTracker, and the worker nodes are called TaskTrackers.</p>
<p>In fact, the JobTracker typically runs on the same machine as the NameNode (and HMaster) and the TaskTrackers on the same machines as the DataNodes (and RegionServers). This is called “bring the query to the data.”</p>
<p>As the map phase progresses, there is a risk that the memory becomes full. But we have seen this before with HBase: the intermediate pairs on that machine are then sorted by key and flushed to the disk to a Sequence File. And as more flushes happen, these Sequence Files can be compacted to less of them, very similarly to HBase’s Log-Structured Merge Trees. When the map phase is over, each TaskTracker runs an HTTP server listening for connections, so that they can connect to each other and ship the intermediate data over to create the intermediate partitions ensuring that the same keys are on the same machines.This is the phase called shuffling. Then, the reduce phase can start. When the reduce phase is completed, each output partition will be output to a shard (as we saw, a file named incrementally) in the output destination (HDFS, S3, etc) and in the desired format.</p>
<p>Note that shuffling can start before the map phase is over, but the reduce phase can only start after the map phase is over.</p>
<h2><span id="mapreduce-input-and-output-formats">MapReduce input and output formats</span></h2><h3><span id="impedance-mismatch">Impedance mismatch</span></h3><p>MapReduce can read its input from files lying in a data lake as well as directly from a database system such as HBase or a relational database management system. MapReduce only reads and writes lists of key-value pairs, where keys may be duplicates and need not appear in order. However, the inputs we considered are not key-value pairs. So we need an additional mechanism that allows MapReduce to interpret this input as key-value pairs. For tables, whereas relational or in a wide column stores, this is relatively easy: indeed, tables have primary keys, consisting of either a single column or multiple columns. Thus, each tuple can be interpreted as a key-value pair.</p>
<h3><span id="mapping-files-to-pairs">Mapping files to pairs</span></h3><p>How do we read a (possibly huge) text file as a list of key-value pairs? The most natural way to do so is to turn each line of text in a key value pair1: the value is the string corresponding to the entire line, while the key is an integer that expresses the position (as a number of characters), or offset, at which the line starts in the current file being read.</p>
<h2><span id="a-few-examples">A few examples</span></h2><h3><span id="counting-words">Counting words</span></h3><p><img src="/2023/10/31/bigdata6/image-3.png" alt="Alt text"><br><img src="/2023/10/31/bigdata6/image-4.png" alt="Alt text"></p>
<h3><span id="selecting">Selecting</span></h3><p>This is something easily done by having a map function that outputs a subset of its input, based on some predicate provided by the user.<br><img src="/2023/10/31/bigdata6/image-5.png" alt="Alt text"><br>Here we notice that the output of the map phase already gives us the desired result; we still need to provide a reduce function, which is taken trivially as the identity function. This is not unusual (and there are also examples where the map function is trivial, and the reduce function is doing the actual processing).</p>
<h3><span id="projecting">Projecting</span></h3><p>The map function can project this object to an object with less attributes:<br><img src="/2023/10/31/bigdata6/image-6.png" alt="Alt text"></p>
<h3><span id="mapreduce-and-the-relational-algebra">MapReduce and the relational algebra</span></h3><p>As an exercise, try to figure out how to implement a GROUP BY clause and an ORDER BY clause. What about the HAVING clause? Naturally, executing these queries directly in MapReduce is very cumbersome because of the low-level code we need to write. </p>
<h2><span id="combine-functions-and-optimization">Combine functions and optimization</span></h2><p>In addition to the map function and the reduce function, the user can supply a combine function. This combine function can then be called by the system during the map phase as many times as it sees fit to “compress” the intermediate key-value pairs. Strategically, the combine function is likely to be called at every flush of key-value pairs to a Sequence File on disk, and at every compaction of several Sequence Files into one.</p>
<p>However, there is no guarantee that the combine function will be called at all, and there is also no guarantee on how many times it will be called. Thus, if the user provides a combine function, it is important that they think carefully about a combine function that does not affect the correctness of the output data.</p>
<p>In fact, in most of the cases, the combine function will be identical to the reduce function, which is generally possible if the intermediate key-value pairs have the same type as the output key-value pairs, and the reduce function is both associative and commutative.</p>
<h2><span id="mapreduce-programming-api">MapReduce programming API</span></h2><h3><span id="mapper-classes">Mapper classes</span></h3><p>In Java, the user needs to define a so-called Mapper class that contains the map function, and a Reducer class that contains the reduce function. A map function takes in particular a key and a value. Note that it outputs key-value pairs via the call of the write method on the context, rather than with a return statement.</p>
<h3><span id="reducer-classes">Reducer classes</span></h3><p>A reduce function takes in particular a key and a list of values. Note that it outputs key-value pairs via the call of the write method on the context, rather than with a return statement.</p>
<h3><span id="running-the-job">Running the job</span></h3><p>Finally, a MapReduce job can be created and invoked by supplying a Mapper and Reducer instance to the job. A combine function can also be supplied with the setCombinerClass method. It is also possible to use Python rather than Java, via the socalled Streaming API. The Streaming API is the general way to invoke MapReduce jobs in other languages than Java. </p>
<h2><span id="using-correct-terminology">Using correct terminology</span></h2><h3><span id="functions">Functions</span></h3><p>A map function is a mathematical, or programmed, function that takes one input key-value pair and returns zero, one or more intermediate key-value pairs.</p>
<p>A reduce function is a mathematical, or programmed, function that takes one or more intermediate key-value pairs and returns zero, one or more output key-value pairs.</p>
<p>A combine function is a mathematical, or programmed, function that takes one or more intermediate key-value pairs and returns zero, one or more intermediate key-value pairs.</p>
<p>Note that the combine function is an optional local aggregation step that occurs before shuffling and sorting, and its purpose is to reduce the amount of data that needs to be transferred to the reducers. The reduce function, on the other hand, performs the final aggregation and processing based on keys. </p>
<h3><span id="tasks">Tasks</span></h3><p>A map task is an assignment (or “homework”, or “TODO”) that consists in a (sequential) series of calls of the map function on a subset of the input.</p>
<p>A reduce task is an assignment that consists in a (sequential) series of calls of the reduce function on a subset of the intermediate input.</p>
<p>We insist that the calls within a task are sequential, meaning that there is no parallelism at all within a task.</p>
<p>There is no such thing as a combine task. Calls of the combine function are not planned as a task, but is called ad-hoc during flushing and compaction.</p>
<h3><span id="slots">Slots</span></h3><p>The map tasks are processed thanks to compute and memory resources (CPU and RAM). These resources are called map slots. One map slot corresponds to one CPU core and some allocated memory. The number of map slots is limited by the number of available cores. Each map slot then processes one map task at a time, sequentially. The resources used to process reduce tasks are called reduce slots. So, there is no parallelism either within one map slot, or one reduce slot. In fact, parallelism happens across several slots.</p>
<h3><span id="phases">Phases</span></h3><p>The map phase thus consists of several map slots processing map tasks in parallel. </p>
<h3><span id="blocks-vs-splits">blocks vs. splits</span></h3><p>HDFS blocks have a size of (at most) 128 MB. In every file, all blocks but the last one have a size of exactly 128 MB. Splits, however, only contain full records: a key-value pair will only belong to one split (and thus be processed by one map task).</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-10-25T07:24:44.000Z" title="2023-10-25 9:24:44 ├F10: AM┤">2023-10-25</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-11-08T18:08:30.891Z" title="2023-11-8 7:08:30 ├F10: PM┤">2023-11-08</time></span><span class="level-item"><a class="link-muted" href="/categories/database/">database</a></span><span class="level-item">16 minutes read (About 2332 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/10/25/bigdata5/">bigdata - data model</a></p><div class="content"><h1><span id="data-model">data model</span></h1><p>A data model is an abstract view over the data that hides the way it is stored physically.</p>
<h2><span id="the-json-information-set">The JSON Information Set</span></h2><p>The appropriate abstraction for any JSON document is a tree. The nodes of that tree, which are JSON logical values, are naturally of six possible kinds: the six syntactic building blocks of JSON.</p>
<p>These are the four leaves corresponding to atomic values:Strings, Numbers,Booleans, Nulls. As well as two intermediate nodes: Objects, Arrays. These nodes are generally called information items and form the logical building blocks of the model, called information set.</p>
<h2><span id="the-xml-information-set">The XML Information Set</span></h2><p>A fundamental difference between JSON trees and XML trees is that for JSON, the labels (object keys) are on the edges connecting an object information item to each one of its children information items. In XML, the labels (these would be element and attribute names) are on the nodes (information items) directly.</p>
<p>In XML, there are many more information items: Document information items, Element, Attribute, Character, Comment, Processing instruction, Name space, Unexpanded entity reference, DTD, Unparsed entity, Notation. Here we only focus on documents, elements, attributes, and characters.</p>
<h2><span id="validation">Validation</span></h2><p>Once documents, JSON or XML, have been parsed and logically abstracted as a tree in memory, the natural next step is to check for further structural constraints.</p>
<p>In a relational database, the schema of a table is defined before any data is populated into the table. Thus, the data in the table is guaranteed, at all times, to fulfil all the constraints of the schema. The schema was enforced at write time (schema on write). A collection of JSON and XML documents out there can exist without any schema and contain arbitrary structures. Validation happens “ex post,” that is, only after reading the data (schema on read).</p>
<p>JSON and XML documents undergo two steps: a well-formedness check: attempt to parse the document and construct a tree representation in memory;(if the first step succeeded) a validation check given a specific schema. Note that, unlike well-formedness, validation is schema dependent: a given well-formed document can be valid against schema A and invalid against schema B.</p>
<h2><span id="item-types">Item types</span></h2><p>A fundamental aspect of validation is the type system. A well-designed type system, in turn, allows for storing the data in much more efficient, binary formats tailored to the model.</p>
<h3><span id="atomic-types">Atomic types</span></h3><p>Atomic types correspond to the leaf of a tree data model: these are types that do not contain any further nestedness. The kinds of atomic types available are also relatively standard and common to most technologies. Also, all atomic types have in common that they have a logical value space and a lexical value space.</p>
<p>An atomic type also has a (not necessarily injective) mapping from its lexical value space to its logical value space (e.g., mapping the hexadecimal literal x10 to the mathematical integer sixteen).<br>Atomic types can be in a subtype relationship: a type is a subtype of another type if its logical value space is a subset of the latter.</p>
<h4><span id="strings">Strings</span></h4><p>In “pure computer science” textbooks, strings are often presented as structured values rather than as atomic values because of their complexity on the physical layer. However, for us data scientists, strings are atomic values.</p>
<h4><span id="numbers-integers">Numbers: integers</span></h4><p>Integers correspond to finite cardinalities (counting) as well as their negative counterparts. In older programming languages, support for integers used to be bounded. However, in modern databases, it is customary to support unbounded integers. Engines can optimize computations for small integers, but might become less efficient with very large integers.</p>
<h4><span id="numbers-decimals">Numbers: decimals</span></h4><p>Decimals correspond to real numbers that can be written as a finite sequence of digits in base 10, with an optional decimal period.</p>
<h4><span id="numbers-floating-point">Numbers: floating-point</span></h4><p>Support for the entire decimal value space can be costly in performance. In order to address this issue, a floating-point standard (IEEE 754) was invented and is still very popular today.</p>
<p>Floating-point numbers are limited both in precision and magnitude (both upper and lower) in order to fit on 32 bits (float) or 64 bits (double). Floats have about 7 digits of precision and their absolute value can be between roughly 10^−37 and 10^37, while doubles have 15 digits of precision and their absolute value can be between roughly 10^−307 and 10^308.</p>
<h4><span id="booleans">Booleans</span></h4><p>The logical value space for the Boolean type is made of two values: true and false as in NoSQL queries, two-valued logic is typically assumed.</p>
<h4><span id="dates-and-times">Dates and times</span></h4><p>Dates are commonly using the Gregorian calendar (with some technologies possibly supporting more) with a year (BC or AD), a month and a day of the month. Times are expressed in the hexagesimal (60) basis with hours, minutes, seconds, where the seconds commonly go all the way to microseconds (six digits after the decimal period). Datetimes are expressed with a year, a month, a day of the month, hours, minutes and (decimal) seconds.</p>
<p>Timestamp values are typically stored as longs (64-bit integers) expressing the number of milliseconds elapsed since January 1, 1970 by convention.</p>
<p>XML Schema, JSound and JSONiq follow the ISO 8601 standard, where lexical values look like so (with many parts optional): 2022-08-07T14:18:00.123456+02:00.</p>
<h4><span id="durations">Durations</span></h4><p>The lexical representation can vary, but there is a standard defined by ISO 8601 as well, starting with a P and prefixing sub-day parts with a T.<br>4 days, 3 hours, 2 minutes and 1.123456 seconds: P4DT3H2M1.123456S.</p>
<h4><span id="binary-data">Binary data</span></h4><p>Binary data is, logically, simply a sequence of bytes. There are two main lexical representations used in data: hexadecimal and base64. Hexadecimal expresses the data with two hexadecimal digits per byte. Base 64, formally, does the same but in the base 64, which “wastes” less lexical space in the text. It does so by encoding the bits six by six, encoding each sequence of six bits with one base-64 digit.</p>
<h4><span id="null">Null</span></h4><p>A schema can either allow, or disallow the null value.<br>XML also supports null values, but calls them “nil” and does so with a special attribute and no content rather than with a lexical representation</p>
<h3><span id="structured-types">Structured types</span></h3><h4><span id="lists">Lists</span></h4><p>Lists correspond to JSON arrays and are ordered sequences of (atomic or structured) values.</p>
<h4><span id="records">Records</span></h4><p>Records, or structs, correspond to JSON objects and are maps from strings to values.</p>
<h4><span id="maps">Maps</span></h4><p>Maps (not be confused with records, which are similar) are maps from any atomic value to any value, i.e., generalize objects to keys that are not necessarily strings (e.g., numbers, dates, etc).<br>With a schema, it is possible to restrict the type of the keys, as well as the type of the values. However, unlike records, the type of the values must be the same for all keys.</p>
<h4><span id="sets">Sets</span></h4><p>Sets are like lists, but without any specific ordering, and without duplicate values.</p>
<h4><span id="xml-elements-and-attributes">XML elements and attributes</span></h4><p>XML Schema stands apart from most other technologies and formats, in that it does not offer specific support for records and maps; it offers some limited support for lists, but considers them to be simple types, which are “inbetween” atomic types and structured types. n XML Schema, structure is obtained, instead, with elements and attributes, and the machinery for elements and attributes is highly specific to XML.</p>
<h4><span id="type-names">Type names</span></h4><p><img src="/2023/10/25/bigdata5/image.png" alt="Alt text"></p>
<h2><span id="sequence-types">Sequence types</span></h2><h3><span id="cardinality">Cardinality</span></h3><p>Many type system give options regarding the number of occurrences of items in a sequence.</p>
<h3><span id="collections-vs-nested-lists">Collections vs. nested lists</span></h3><p>A collection of items is on the outer level, and can be massively large (billions, trillions of items).</p>
<p>A list (or array) of items, however, usually refers to a nested structure, for example an array nested inside a document or object. Such lists of items are usually restricted in size for reasons of performance and scalability.</p>
<p>It is thus important to keep this subtle difference in mind, in particular, do not confuse a collection of integers with a collection that contains a single array of integers.</p>
<h2><span id="json-validation">JSON validation</span></h2><h3><span id="validating-flat-objects">Validating flat objects</span></h3><p>JSound is a schema language that was designed to be simple for 80% of the cases, making it particularly suitable in a teaching environment.It is independent of any programming language.JSON Schema is another technology for validating JSON documents. The available JSON Schema types are string, number, integer, boolean, null, array and object.</p>
<p>An example for a json document is like:<br>{ “name” : “Einstein”, “first” : “Albert”, “age” : 142 }<br>The JSound and the JSON Schema are as follows:<br>{ “name” : “string”, “first” : “string”, “age” : “integer” }</p>
<p>{ “type” : “object”, “properties” : { “name” : “string”, “first” : “string”, “age” : “number” } }.</p>
<p>The type system of JSON Schema is thus less rich than that of JSound, but extra checks can be done with so-called formats, which include date, time, duration, email, and so on including generic regular expressions.</p>
<h3><span id="requiring-the-presence-of-a-key">Requiring the presence of a key</span></h3><p>It is possible to require the presence of a key by adding an exclamation mark in JSound. The equivalent JSON Schema uses a “required” property associated with the list of required keys to express the same.</p>
<h3><span id="open-and-closed-object-types">Open and closed object types</span></h3><p>In the JSound compact syntax, extra keys are forbidden. The schema is said to be closed. There are ways to define JSound schemas to allow arbitrary additional keys (open schemas), with a more verbose syntax. Unlike JSound, in JSON Schema, extra properties are allowed by default. JSON Schema then allows to forbid extra properties with the “additionalProperties” property.</p>
<h3><span id="nested-structures">Nested structures</span></h3><p>{ “numbers” : [ “integer” ] }<br>Every schema can be given a name, turning into a type.<br>JSound allows for the definition not only of arbitrary array and object types, but also user-defined types.</p>
<h3><span id="primary-key-constraints-allowing-for-null-default-values">Primary key constraints, allowing for null, default values</span></h3><p>There are a few more features available in the compact JSound syntax (not in JSON Schema) with the special characters @, ? and =. The question mark (?) allows for null values.  The arobase (@) indicates that one or more fields are primary keys for a list of objects that are members of the same array. The equal sign (=) is used to indicate a default value that is automatically populated if the value is absent.</p>
<p>Note that validation only checks whether lexical values are part of the type’s lexical space.</p>
<h3><span id="accepting-any-values">Accepting any values</span></h3><p>Accepting any values in JSound can be done with the type “item”, which contains all possible values. In JSON Schema, in order to declare a field to accept any values, you can use either true or an empty object in lieu of the type. JSON Schema additionally allows to use false to forbid a field. </p>
<h3><span id="type-unions">Type unions</span></h3><p>In JSON Schema, it is also possible to combine validation checks with Boolean combinations. JSound schema allows defining unions of types with the vertical bar inside type strings, like so: “string|array”. </p>
<h3><span id="type-conjunction-exclusive-or-negation">Type conjunction, exclusive or, negation</span></h3><p>In JSON Schema only (not in JSound), it is also possible to do a conjunction (logical and), as well as exclusive or (xor), as well as negation.</p>
<h2><span id="xml-validation">XML validation</span></h2><h3><span id="simple-types">Simple types</span></h3><p>All elements in an XML Schema are in a namespace, the XML Schema namespace. It is recommended to stick to the prefix xs, or xsd, which is also quite popular. We do not recommend declaring the XML Schema namespace as a default namespace, because it can create confusion in several respects.</p>
<p>The list of predefined atomic types is the same as in JSound, except that in XML Schema, all these predefined types live in the XML Schema namespace and thus bear the prefix xs as well.</p>
<h3><span id="builtin-types">Builtin types</span></h3><p>XML Schema allows you to define user-defined atomic types, for example restricting the length of a string to 3 for airport codes, and then use it with an element.</p>
<h3><span id="complex-types">Complex types</span></h3><p>It is also possible to constrain structures and the element/attribute/text hierarchy with complex types applying to element nodes.<br>There are four main kinds of complex types:• complex content: there can be nested elements, but there can be no text nodes as direct children. • simple content: there are no nested elements: just text, but attributes are also possible. • empty content: there are neither nested elements nor text, but attributes are also possible. • mixed content: there can be nested elements and it can be intermixed with text as well.</p>
<h3><span id="attribute-declarations">Attribute declarations</span></h3><p>Finally, all types of content can additionally contain attributes. Attributes always have a simple type.</p>
<h3><span id="anonymous-types">Anonymous types</span></h3><p>Finally, it is not mandatory to give a name to all types. Be careful: if there is neither a type attribute nor a nested type declaration, then anything is allowed! </p>
<h3><span id="miscellaneous">Miscellaneous</span></h3><p>Finally, XML Schema documents are themselves XML documents, and can thus be validated against a “schema or schemas”, itself written as an XML Schema.This schema has the wonderful property of being valid against itself.</p>
<h2><span id="data-frames">Data frames</span></h2><h3><span id="heterogeneous-nested-datasets">Heterogeneous, nested datasets</span></h3><p>The beauty of the JSON data model is that, unlike the relational model and the CSV syntax, it supports nested, heterogeneous datasets, while also supporting as a particular case flat, homogeneous datasets.</p>
<h3><span id="dataframe-visuals">Dataframe visuals</span></h3><p>There is a particular subclass of semi-structured datasets that are very interesting: valid datasets, which are collections of JSON objects valid against a common schema, with some requirements on the considered schemas. The datasets belonging to this particular subclass are called data frames, or dataframes.</p>
<p>Specifically, for the dataset to qualify as a data frame, firstly, we forbid schemas that allow for open object types. secondly, we forbid schemas that allow for object or array values to be too permissive and allow any values. We, however, include schemas that allow for null values and/or absent values. Relational tables are data frames, while data frames are not necessarily relational tables. Data frames are a generalization of (normalized) relational tables allowing for (organized and structured) nestedness.</p>
<h2><span id="exercies">exercies</span></h2><p>complextType cannot contain character by default but with mixed=”true” it can.</p>
<h3><span id="protobuf">protobuf</span></h3><p>convert json-like data to columnar representation(why we want this: make it more efficient to get relevant data rather than get the whole table).</p>
<p>convert the columnar representation back to the original format. Replace the missing field with NULL. It’s a “lossless” conversion.</p>
<h3><span id="dremel">Dremel</span></h3><p>optional: 0 or 1. repeated: 1 or more.</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-10-17T13:23:43.000Z" title="2023-10-17 3:23:43 ├F10: PM┤">2023-10-17</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-11-04T14:57:08.249Z" title="2023-11-4 3:57:08 ├F10: PM┤">2023-11-04</time></span><span class="level-item"><a class="link-muted" href="/categories/database/">database</a></span><span class="level-item">12 minutes read (About 1848 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/10/17/bigdata4/">bigdata - wide column stores</a></p><div class="content"><h1><span id="wide-column-stores">wide column stores</span></h1><p>Wide column stores were invented to provide more control over performance and in particular, in order to achieve high-throughput and low latency for objects ranging from a few bytes to about 10 MB, which are too big and numerous to be efficiently stored as so-called clobs (character large objects) or blobs (binary large objects) in a relational database system, but also too small and numerous to be efficiently accessed in a distributed file system.</p>
<h2><span id="why-wide-column-stores">Why wide column stores</span></h2><p>A wide column store will be more tightly integrated with the parallel data processing systems.<br>wide column stores have a richer logical model than the simple key-value model behind object storage. wide column stores also handle very small values (bytes and kBs) well thanks to batch processing.</p>
<h2><span id="different-from-rdbms">different from RDBMS</span></h2><p>It does not have any data model for values, which are just arrays of bytes; since it efficiently handles values up to 10 MB, the values can be nested data in various formats, which breaks the first normal form; tables do not have a schema; there is no language like SQL, instead the API is on a lower level and more akin to that of a key-value store; tables can be very sparse, allowing for billions of rows and millions of columns at the same time; this is another reason why data stored in HBase is denormalized. </p>
<h2><span id="bigtable-and-hbase">BigTable and HBase</span></h2><h3><span id="rationale">Rationale</span></h3><p>HBase is an open-source equivalent to the BigTable as part of the Hadoop ecosystem.<br>The data model of HBase is based on the realization that joins are expensive, and that they should be avoided or minimized on a cluster architecture.</p>
<p>The second design principle underlying HBase is that it is efficient to store together what is accessed together. In the big picture, this is a flavor of batch processing, one of the overarching principles in Big Data. Batch processing reduces the impact of latency.</p>
<h3><span id="tables-and-row-ids">Tables and row IDs</span></h3><p>From an abstract perspective, HBase can be seen as an enhanced keyvalue store, in the sense that:a key is compound and involves a row, a column and a version;values can be larger (clobs, blobs), up to around 10 MB; keys are sortable. </p>
<p>A row ID is logically an array of bytes, although there is a library to easily create row ID bytes from specific primitive values. In HBase, the key identifying every cell consists of: the row ID, the column family, the column qualifier, the version.</p>
<h3><span id="column-families">Column families</span></h3><p>The other attributes, called columns, are split into so-called column families. This is a concept that does not exist in relational databases and that allows scaling the number of columns.</p>
<h3><span id="column-qualifiers">Column qualifiers</span></h3><p>Columns in HBase have a name (in addition to the column family) called column qualifier, however unlike traditional RDBMS, they do not have a particular type. Column qualifiers are arrays of bytes (rather than strings), and as for row IDs, there is a library to easily create column qualifiers from primitive values. Unlike the values which can be large arrays of bytes (blobs), it is important to keep column families and column qualifiers short, because as we will see, they are repeated a gigantic number of times on the physical layer.</p>
<h3><span id="versioning">Versioning</span></h3><p>HBase generally supports versioning, in the sense that it keeps track of the past versions of the data. As we will see, this is implemented by associating any value with a timestamp, also called version, at which it was created (or deleted).Users can also override timestamps with a value of their choice to have more control about versions.</p>
<h2><span id="logical-queries">Logical queries</span></h2><p>HBase supports four kinds of low-level queries: get, put, scan and delete. Unlike a traditional key-value store, HBase also supports querying ranges of row IDs and ranges of timestamps.</p>
<p>HBase offers a locking mechanism at the row level, meaning that different rows can be modified concurrently, but the cells in the same row cannot: only one user at a time can modify any given row.</p>
<h2><span id="physical-architecture">Physical architecture</span></h2><h3><span id="partitioning">Partitioning</span></h3><p>A table in HBase is physically partitioned in two ways: on the rows and on the columns. The rows are split in consecutive regions. Each region is identified by a lower and an upper row key, the lower row key being included and the upper row key excluded. A partition is called a store and corresponds to the intersection of a region and of a column family.<br><img src="/2023/10/17/bigdata4/image-1.png" alt="Alt text"></p>
<h3><span id="network-topology">Network topology</span></h3><p>HBase has exactly the same centralized architecture as HDFS. The HMaster and the RegionServers should be understood as processes running on the nodes, rather than the nodes themselves. The HMaster assigns responsibility of each region to one of the RegionServers. There is no need to attribute the responsibility of a region to more than one RegionServer at a time because, as we will see soon, fault tolerance is already handled on the storage level by HDFS. If a region grows too big, for example because of many writes in the same row ID interval, then the region will be automatically split by the responsible RegionServer. If a RegionServer has too many regions compared to other RegionServers, then the HMaster can reassign regions to other RegionServers.</p>
<h3><span id="physical-storage">Physical storage</span></h3><p>The store is, physically, nothing less than a set of cells. Each cell is thus handled physically as a key-value pair where the key is a (row ID, column family, column qualifier, version) tuple. All the cells within a store are eventually persisted on HDFS, in files that we will call HFiles.</p>
<p>An HFile is, in fact, nothing else than a (boring) flat list of KeyValues, one per cell. What is important is that, in an HFile, all these KeyValues are sorted by key in increasing order, meaning, first sorted by row ID, then by column family (trivially unique for a given store), then by column qualifier, then by version (in decreasing order, recent to old). This means that all versions of a given cell that are in the same HFile are located together, and one of the cells (within this HFile) is the latest. If we zoom in at the bit level, a KeyValue consists in four parts: The length of the keys in bits (this length is encoded on a constant, known number of bits) • The length of the value in bits (this length is encoded on a constant, known number of bits) • The actual key (of variable length) • The actual value (of variable length). Why do we not just store the key and the value? This is because their length can vary. If we do not know their length, then it is impossible to know when they stop just looking at the bits.<br><img src="/2023/10/17/bigdata4/image-2.png" alt="Alt text"></p>
<p>KeyValues, within an HFile, are organized in blocks. But to not confuse them with HDFS blocks, we will call them HBlocks. HBlocks have a size of 64 kB, but this size is variable: if the last KeyValue goes beyond this boundary, then the HBlock is simply longer and stops whenever the last KeyValue stops. The HFile then additionally contains an index of all blocks with their key boundaries. This separate index is loaded in memory prior to reading anything from the HFile.</p>
<h3><span id="log-structured-merge-trees">Log-structured merge trees</span></h3><p>When accessing data, HBase needs to generally look everywhere for cells to potentially return: in every HFile, and in memory. As long as there is room in memory, freshly created cells are added in memory. At some point, the memory becomes full (or some other limits are reached). When this happens, all the cells need to be flushed to a brand new HFile. Upon flushing, all cells are written sequentially to a new HFile in ascending key order, HBlock by HBlock, concurrently building the index structure. When cells are added to memory, they are added inside a data structure that maintains them in sorted order (such as tree maps) and then flushing is a linear traversal of the tree.</p>
<p>What happens if the machine crashes and we lose everything in memory? We have a so-called write-ahead-log for this. Before any fresh cells are written to memory, they are written in sequential order (append) to an HDFS file called the HLog. There is one HLog per RegionServer. A full write-ahead-log also triggers a flush of all cells in memory to a new HFile. If there is a problem and the memory is lost, the HLog can be retrieved from HDFS and “played back” in order to repopulate the memory and recreate the sorting tree structure.</p>
<p>After many flushes, the number of HFiles to read from grows and becomes impracticable. For this reason, there is an additional process called compaction that takes several HFiles and outputs a single,merged HFile. Since the cells within each HFile are already sorted, this can be done in linear time, as this is essentially the merge part of the merge-sort algorithm. Compaction is not done arbitrarily but follows a regular, logarithmic pattern. When the memory is flushed again, an standard-size HFile is written and the two standard-size HFiles are immediately compacted to a double-size HFile.</p>
<h2><span id="additional-design-aspects">Additional design aspects</span></h2><h3><span id="bootstrapping-lookups">Bootstrapping lookups</span></h3><p>In order to know which RegionServer a client should communicate with to receive cells corresponding to a specific region, there is a main, big lookup table that lists all regions of all tables together with the coordinates of the RegionServer in charge of this region as well as additional metadata. </p>
<h3><span id="caching">Caching</span></h3><p>In order to improve latency, cells that are normally persisted to HFiles (and thus no longer in memory) can be cached in a separate memory region, with the idea of keeping in the cache those cells that are frequently accessed.</p>
<h3><span id="bloom-filters">Bloom filters</span></h3><p>HBase has a mechanism to avoid looking for cells in every HFile. This mechanism is called a Bloom filter. It is basically a black box that can tell with absolute certainty that a certain key does not belong to an HFile, while it only predicts with good probability (albeit not certain) that it does belong to it.</p>
<h3><span id="data-locality-and-short-circuiting">Data locality and short-circuiting</span></h3><p>When a RegionServer flushes cells to a new HFile, a replica of each (HDFS) block of the HFile is written, by the DataNode process living on the same machine as the RegionServer process, to the local disk. This makes accessing the cells in future reads by the RegionServer extremely efficient, because the RegionServer can read the data locally without communicating with the NameNode: this is known as short-circuiting in HDFS.</p>
<h2><span id="using-habse">using Habse</span></h2><p>After installing Hbase, we can use Hbase shell. There are some commands in hbase shell: get, scan.<br>We can use filters with scan to query and filter data.</p>
<h2><span id="exercises">exercises</span></h2><h3><span id="bloom-filter">bloom filter</span></h3><p>perfect hash function should have uniform probability.<br>all hash functions set a bit to 1 = collide at the same place: probability of a FP case<br>deleting only happens when compacting.</p>
<h2><span id="references">references</span></h2><p><a target="_blank" rel="noopener" href="https://ghislainfourny.github.io/big-data-textbook/">https://ghislainfourny.github.io/big-data-textbook/</a><br><a target="_blank" rel="noopener" href="https://datakaresolutions.com/hbase-quick-guide-to-key-commands/">https://datakaresolutions.com/hbase-quick-guide-to-key-commands/</a><br><a target="_blank" rel="noopener" href="https://www.datapotion.io/blog/hbase-shell-column-filters/">https://www.datapotion.io/blog/hbase-shell-column-filters/</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-10-11T20:50:40.000Z" title="2023-10-11 10:50:40 ├F10: PM┤">2023-10-11</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-10-17T13:23:23.654Z" title="2023-10-17 3:23:23 ├F10: PM┤">2023-10-17</time></span><span class="level-item"><a class="link-muted" href="/categories/database/">database</a></span><span class="level-item">8 minutes read (About 1203 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/10/11/bigdata3/">bigdata - Syntax</a></p><div class="content"><h1><span id="syntax">syntax</span></h1><h2><span id="csv">CSV</span></h2><p>CSV is a textual format, in the sense that it can be opened in a text editor. CSV means comma-separated values. The main challenge with CSV files is that, in spite of a standard (RFC 4180), in practice there are many different dialects and variations, which limits interoperability. For example, another character can be used instead of the comma (tabulation, semi-colons, etc). Also, when a comma (or the special character used in its stead) needs to actually appear in a value, it needs to be escaped.</p>
<h2><span id="data-denormalization">Data denormalization</span></h2><p>Data denormalization makes a lot of sense in read-intensive scenarios in which not having to join brings a significant performance improvement.</p>
<p>The difference with CSV is that, in JSON, the attributes appear in every tuple, while in CSV they do not appear except in the header line. JSON is appropriate for data denormalization because including the attributes in every tuple allows us to drop the identical support requirement.</p>
<p>The generic name for denormalized data (in the same of heterogeneous and nested) is “semi-structured data”. Textual formats such as XML and JSON have the advantage that they can both be processed by computers, and can also be read, written and edited by humans. Another very important and characterizing aspect of XML and JSON is that they are standards: XML is a W3C standard. W3C, also known as the World Wide Web consortium, is the same body that also standardizes HTML, HTTP, etc. JSON is now an ECMA standard, which is the same body that also standardizes JavaScript.</p>
<p>Whichever syntax is used, they have in common the concept of well-formedness. A string is said to be well-formed if it belongs to the language. Concretely, when a document is well-formed XML, it means that it can be successfully opened by an editor as XML with no errors.</p>
<h2><span id="json">JSON</span></h2><p>JSON stands for JavaScript Object Notation because the way it looks like originates from JavaScript syntax, however it is now living its own life completely independently of JavaScript. </p>
<p>JSON is made of exactly six building blocks: strings, numbers, Booleans, null, objects, and arrays. Strings are simply text. In JSON, strings always appear in double quotes. Obviously, strings could contain quotes and in order not to confuse them with the surrounding quotes, they need to be differentiated. This is called escaping and, in JSON, escaping is done with backslash characters ().</p>
<p>JSON generally supports numbers, without explicitly naming any types nor making any distinction between numbers apart from how they appear in syntax. The way a number appears in syntax is called a lexical representation, or a literal. JSON places a few restrictions: a leading + is not allowed. Also, a leading 0 is not allowed except if the integer part is exactly 0. </p>
<p>There are two Booleans, true and false. Arrays are simply lists of values. The concept of list is abstract and mathematical. The concept of array is the syntactic counterpart of a list. Objects are simply maps from strings to values. The concept of object is the syntactic counterpart of a map,i.e., an object is a physical representation of an abstract map that explicitly lists all string-value pairs. The keys of an object must be strings. The JSON standard recommends for keys to be unique within an object.</p>
<h2><span id="xml">XML</span></h2><p>XML stands for eXtensible Markup Language. It resembles HTML, except that it allows for any tags and that it is stricter in what it allows. </p>
<p>XML’s most important building blocks are elements, attributes, text and comments. XML is a markup language, which means that content is “tagged”. Tagging is done with XML elements. An XML element consists of an opening tag, and a closing tag. What is “tagged” is everything inbetween the opening tag and the closing tag. ags consist of a name surrounded with angle brackets &lt; … &gt;, and the closing tag has an additional slash in front of the name. We use a convenient shortcut to denote the empty element with a single tag and a slash at the end. For example, \<person> is equal to<br>\<person>\</person>.<br>Unlike JSON keys, element names can repeat at will.</person></p>
<p>Attributes appear in any opening elements tag and are basically keyvalue pairs. Values can be either double-quoted or single-quoted. The key is never quoted, and it is not allowed to have unquoted values. Within the same opening tag, there cannot be duplicate keys.  Attributes can also appear in an empty element tag. Attributes can never appear in a closing tag. It is not allowed to create attributes that start with XML or xml, or any case combination.</p>
<p>Text, in XML syntax, is simply freely appearing in elements and without any quotes (attribute values are not text!). Within an element, text can freely alternate with other elements. This is called mixed content and is unique to XML. </p>
<p>Comments in XML look like so: \<!-- This is a comment -->. XML documents can be identified as such with an optional text declaration containing a version number and an encoding, like \&lt;?xml version=”1.0” encoding=”UTF-8”?&gt;. The version is either 1.0 or 1.1. Another tag that might appear is the doctype declaration, like \&lt;!DOCTYPE person&gt;. </p>
<p>Remember that in JSON, it is possible to escape sequences with a backslash character. In XML, this is done with an ampersand (&amp;) character. There are exactly five possible escape sequences pre-defined in XML:<br><img src="/2023/10/11/bigdata3/image-1.png" alt="Alt text">. Escape sequences can be used anywhere in text, and in attribute values. At other places (element names, attribute names, inside comments), they will not be recognized.<br>There are a few places where they are mandatory:&amp; and &lt; MUST be escaped. ” and ‘ should also be escaped in quoted qttribute values. </p>
<p>Namespaces are an extension of XML that allows users to group their elements and attributes in packages, similar to Python modules, Java packages or C++ namespaces. A namespace is identified with a URI. A point of confusion is that XML namespaces often start with http://, but are not meant to be entered as an address into a browser. A namespace declaration is like: \<persons xmlns="http://www.example.com/persons">. If you remember, we saw that attributes starting with xml are forbidden, and this is because this is reserved for namespace declarations. What about documents that use multiple namespaces? This is done by associating namespaces with prefixes, which act as shorthands for a namespace. Then, we can use the prefix shorthand in every element that we want to have in this namespace.<br><img src="/2023/10/11/bigdata3/image-2.png" alt="Alt text"><br>So, given any element, it is possible to find its local name, its (possibly absent) prefix, and its (possibly absent) namespace. The triplet (namespace, prefix, localname) is called a QName</persons></p>
<p>Attributes can also live in namespaces, that is, attribute names are generally QNames. However, there are two very important aspects to consider. First, unprefixed attributes are not sensitive to default namespaces: unlike elements, the namespace of an unprefixed attribute is always absent even if there is a default namespace. Second, it is possible for two attributes to collide if they have the same local name, and different prefixes but associated with the same namespace (but again, we told you: do not do that!).</p>
<h2><span id="references">references</span></h2><p><a target="_blank" rel="noopener" href="https://ghislainfourny.github.io/big-data-textbook/">https://ghislainfourny.github.io/big-data-textbook/</a></p>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous is-invisible is-hidden-mobile"><a href="/categories/database/page/0/">Previous</a></div><div class="pagination-next"><a href="/categories/database/page/2/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link is-current" href="/categories/database/">1</a></li><li><a class="pagination-link" href="/categories/database/page/2/">2</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/tx.png" alt="s-serenity"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">s-serenity</p><p class="is-size-6 is-block">student</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Shanghai, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">72</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">9</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">21</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/s-serenity" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/s-serenity"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://github.com/s-serenity" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/algorithm/"><span class="level-start"><span class="level-item">algorithm</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/computer-system/"><span class="level-start"><span class="level-item">computer system</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/database/"><span class="level-start"><span class="level-item">database</span></span><span class="level-end"><span class="level-item tag">12</span></span></a></li><li><a class="level is-mobile" href="/categories/development-tools/"><span class="level-start"><span class="level-item">development tools</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/others/"><span class="level-start"><span class="level-item">others</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/practice/"><span class="level-start"><span class="level-item">practice</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/programming-language/"><span class="level-start"><span class="level-item">programming language</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/theory/"><span class="level-start"><span class="level-item">theory</span></span><span class="level-end"><span class="level-item tag">16</span></span></a></li><li><a class="level is-mobile" href="/categories/web-development/"><span class="level-start"><span class="level-item">web development</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-12-12T14:22:00.000Z">2023-12-12</time></p><p class="title"><a href="/2023/12/12/bigdata12/">bigdata - Cube Data</a></p><p class="categories"><a href="/categories/database/">database</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-12-12T14:21:07.000Z">2023-12-12</time></p><p class="title"><a href="/2023/12/12/bigdata11/">bigdata - Graph Database</a></p><p class="categories"><a href="/categories/database/">database</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-12-07T14:26:41.000Z">2023-12-07</time></p><p class="title"><a href="/2023/12/07/pai10/">pai10</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-12-07T14:26:12.000Z">2023-12-07</time></p><p class="title"><a href="/2023/12/07/pai9/">pai - Model-free Approximate Reinforcement Learning</a></p><p class="categories"><a href="/categories/theory/">theory</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-11-28T20:58:26.000Z">2023-11-28</time></p><p class="title"><a href="/2023/11/28/bigdata10/">bigdata - JSONiq</a></p><p class="categories"><a href="/categories/database/">database</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2023/"><span class="level-start"><span class="level-item">2023</span></span><span class="level-end"><span class="level-item tag">44</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/"><span class="level-start"><span class="level-item">2022</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/"><span class="level-start"><span class="level-item">2021</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/"><span class="level-start"><span class="level-item">2020</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/"><span class="level-start"><span class="level-item">2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/"><span class="level-start"><span class="level-item">2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Algorithm/"><span class="tag">Algorithm</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Elasticsearch/"><span class="tag">Elasticsearch</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux/"><span class="tag">Linux</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Markdown/"><span class="tag">Markdown</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SQL/"><span class="tag">SQL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/algorithm/"><span class="tag">algorithm</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/big-data/"><span class="tag">big data</span><span class="tag">12</span></a></div><div class="control"><a class="tags has-addons" href="/tags/computer-vision/"><span class="tag">computer vision</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/deep-learning/"><span class="tag">deep learning</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/git/"><span class="tag">git</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/graph/"><span class="tag">graph</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/hexo/"><span class="tag">hexo</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/java/"><span class="tag">java</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/machine-learning/"><span class="tag">machine learning</span><span class="tag">16</span></a></div><div class="control"><a class="tags has-addons" href="/tags/probability/"><span class="tag">probability</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python/"><span class="tag">python</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/reinforcement-learning/"><span class="tag">reinforcement learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/speech/"><span class="tag">speech</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/tools/"><span class="tag">tools</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="s-serenity" height="28"></a><p class="is-size-7"><span>&copy; 2023 s-serenity</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2019</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>