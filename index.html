<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>s-serenity</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="s-serenity"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="s-serenity"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Recording and sharing my learning process."><meta property="og:type" content="website"><meta property="og:title" content="s-serenity"><meta property="og:url" content="http://yoursite.com/"><meta property="og:site_name" content="s-serenity"><meta property="og:description" content="Recording and sharing my learning process."><meta property="og:locale" content="en_US"><meta property="og:image" content="http://yoursite.com/img/og_image.png"><meta property="article:author" content="s-serenity"><meta property="article:tag" content="Algorithm study development"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://yoursite.com/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://yoursite.com"},"headline":"s-serenity","image":["http://yoursite.com/img/og_image.png"],"author":{"@type":"Person","name":"s-serenity"},"publisher":{"@type":"Organization","name":"s-serenity","logo":{"@type":"ImageObject"}},"description":"Recording and sharing my learning process."}</script><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.1.0"></head><body class="is-1-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/">s-serenity</a></div><div class="navbar-menu"><div class="navbar-end"></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-12"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-10-12T08:54:35.000Z" title="2023-10-12 10:54:35 ├F10: AM┤">2023-10-12</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-10-12T08:54:35.988Z" title="2023-10-12 10:54:35 ├F10: AM┤">2023-10-12</time></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/10/12/pai1/">pai1</a></p><div class="content"></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-10-11T20:50:40.000Z" title="2023-10-11 10:50:40 ├F10: PM┤">2023-10-11</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-10-11T20:50:41.004Z" title="2023-10-11 10:50:41 ├F10: PM┤">2023-10-11</time></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/10/11/bigdata3/">bigdata3</a></p><div class="content"></div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-09-19T12:48:13.000Z" title="2023-9-19 2:48:13 ├F10: PM┤">2023-09-19</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-10-12T08:53:52.143Z" title="2023-10-12 10:53:52 ├F10: AM┤">2023-10-12</time></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/09/19/bigdata1/">bigdata - introduction</a></p><div class="content"><h2><span id="big-data">big data</span></h2><p>Big Data is a portfolio of technologies that were designed to store, manage and analyze data that is too large to fit on a single machine while accommodat-ing for the issue of growing discrepancy between capacity, throughput and latency.<br><img src="/2023/09/19/bigdata1/image1.png" alt="Alt text"></p>
<h2><span id="data-independence">data independence</span></h2><p>Data independence means that the logical view on the data is cleanly separated, decoupled, from its physical storage.</p>
<h2><span id="architecture">architecture</span></h2><p>Stack: storage, compute, model ,language</p>
<h3><span id="data-model">data model</span></h3><p>what data looks like and what you can do with it.</p>
<h2><span id="data-shapes">data shapes</span></h2><p>tables, trees, cubes</p>
<h3><span id="table">table</span></h3><p>Row(Tuple), Column(Attribute), Primary Key,Value.</p>
<h4><span id="relational-tables">relational tables</span></h4><p>schema: A set of attributes.<br>extension: A set&#x2F;bag&#x2F;list of tuples.<br>Three constraints: Relational integrity, domain integrity and atomic integrity.<br>Superkey, Candidate key(minimal superkey).</p>
<h4><span id="database-normalization">Database Normalization</span></h4><p>In database management systems (DBMS), normal forms are a series of guidelines that help to ensure that the design of a database is efficient, organized, and free from data anomalies.<br>First Normal Form (1NF):In 1NF, each table cell should contain only a single value, and each column should have a unique name.<br>Second Normal Form (2NF): 2NF eliminates redundant data by requiring that each non-key attribute be dependent on the primary key.<br>Third Normal Form (3NF): 3NF builds on 2NF by requiring that all non-key attributes are independent of each other. </p>
<h4><span id="denormalization">Denormalization</span></h4><p>Denormalization is a database optimization technique in which we add redundant data to one or more tables. This can help us avoid costly joins in a relational database. Note that denormalization does not mean ‘reversing normalization’ or ‘not to normalize’. It is an optimization technique that is applied after normalization.</p>
<h2><span id="sql">SQL</span></h2><p>SQL was originally named SEQUEL, for Structured English QUEry Language. SQL is a declarative language, which means that the user specifies what they want, and not how to compute it: it is up to the underlying system to figure out how to best execute the query.</p>
<h3><span id="view-and-table">View and Table</span></h3><p>The view is a result of an SQL query and it is a virtual table, whereas a Table is formed up of rows and columns that store the information of any object and be used to retrieve that data whenever required. A view contains no data of its own but it is like a ‘window’ through which data from tables can be viewed or changed. The view is stored as a SELECT statement in the data dictionary. Creating a view fulfills the requirement without storing a separate copy of the data because a view does not store any data of its own and always takes the data from a base table. as the data is taken from the base table, accurate and up-to-date information is required.</p>
<p>SQL:1999 added the with clause to define “statement scoped views”. They are not stored in the database schema: instead, they are only valid in the query they belong to. This makes it possible to improve the structure of a statement without polluting the global namespace.With is not a stand alone command like create view is: it must be followed by select. </p>
<h3><span id="natural-join-and-inner-join">Natural Join and Inner Join</span></h3><p>Natural Join joins two tables based on the same attribute name and datatypes. The resulting table will contain all the attributes of both the table but keep only one copy of each common column while Inner Join joins two tables on the basis of the column which is explicitly specified in the ON clause. The resulting table will contain all the attributes from both tables including the common column also.</p>
<h2><span id="data-storage">data storage</span></h2><p>Stack: Storage, Encoding, Syntax, Data models, Validation, Processing, Indexing, Data stores,Querying, User interfaces.</p>
<h3><span id="database-and-data-lake">database and data lake</span></h3><p>two main paradigms for storing and retrieving data:database and data lake. Data can be imported into the database (this is called ETL, for Extract-Transform-Load. ETL is often used as a verb).The data is internally stored as a proprietary format that is optimized to make queries faster. This includes in particular building indices on the data.<br>On the other hand, data can also just be stored on some file system.This paradigm is called the data lake paradigm and gained a lot of popularity in the past two decades. It is slower, however users can start querying their data without the effort of ETLing.</p>
<h3><span id="scaling-up-and-scaling-out">scaling up and scaling out</span></h3><p>First, one can buy a bigger machine: more memory, more or faster CPU cores, a larger disk, etc. This is called scaling up. Second, one can buy more, similar machines and share the work across them. This is called scaling out.</p>
<h3><span id="object-stores">Object stores</span></h3><p>Amazon’s object storage system is called Simple Storage Service, abbreviated S3. From a logical perspective, S3 is extremely simple: objects are organized in buckets. Buckets are identified with a bucket ID, and each object within a bucket is identified with an Object ID.</p>
<h3><span id="cap-theorem">CAP theorem</span></h3><p>Consistency: at any point in time, the same request to any server returns the same result, in order words, all nodes see the same data;<br>Availability: the system is available for requests at all times with very high availability.<br>Partition tolerance: the system continues to function even if the network linking its machines is occasionally partitioned.<br>The CAP theorem is basically an impossibility triangle: a system cannot guarantee at the same time: usually are CP,AP or AC.</p>
<h3><span id="rest-apis">REST APIs</span></h3><p>REST (Representational State Transfer) is an architectural style for designing networked applications.RESTful services often use HTTP as the communication protocol. A client and server communicated with the HTTP protocol interact in terms of methods applied to resources.A resource is referred to with what is called a URI. URI stands for Uniform Resource Identifier. A client can act on resources by invoking methods, with an optional body. The most important methods are: GET, PUT,DELETE,POST.</p>
<p>REST is not a standard or protocol, this is an approach to or architectural style for writing API.REST is an architectural style, and RESTful is the interpretation of it. That is, if your back-end server has REST API and you make client-side requests (from a website&#x2F;application) to this API, then your client is RESTful. All requests you make have their HTTP status codes. There are a lot of them and they are divided into 5 classes. The first number indicates which of them a code belongs to:<br>1xx - informational<br>2xx - success<br>3xx - redirection<br>4xx - client error<br>5xx - server error</p>
<h3><span id="amazon-s3-and-azure-blob-storage">Amazon S3 and Azure Blob Storage</span></h3><p><img src="/2023/09/19/bigdata1/image.png" alt="Alt text"></p>
<h3><span id="key-value-store">Key-value store</span></h3><p>A key-value store differs from a typical relational database in three aspects: • Its API is considerably simpler than that of a relational database (which comes with query languages) • It does not ensure atomic consistency; instead, it guarantees eventual consistency, which we covered earlier in this Chapter. • A key-value store scales out well, in that it is very fast also at large scales.</p>
<h4><span id="amazon-dynamo">Amazon Dynamo</span></h4><p>It is itself based (with some modifications) on the Chord protocol, which is a Distributed Hash Table.On the physical level, a distributed hash table is made of nodes (the machines we have in a data center, piled up in racks) that work following a few design principles. The first design principle is incremental stability. This means that new nodes can join the system at any time, and nodes can leave the system at any time, sometimes gracefully, sometimes in a sudden crash.The second principle is symmetry: no node is particular in any way The third principle is decentralization: there is no “central node” that orchestrates the others.The fourth principle is heterogeneity: the nodes may have different CPU power, amounts of memory, etc.</p>
<p>A central aspect of the design of a distributed hash table, and part in particular of the Chord protocol, is that every logical key is hashed to bits that we will call IDs. In the case of Dynamo, the hash is made of 128 bits (7 bytes).In the chord protocol, a technology called a finger table is used. Each node knows the next node clockwise, and the second node, and the 4th node, and the 8th node. Dynamo changes this design to so-called “preference lists”: each node knows, for every key (or key range), which node(s) are responsible (and hold a copy) of it. This is done by associating every key (key range) with a list of nodes, by decreasing priority (going down the ring clockwise).</p>
<p>Distributed hash tables, including Dynamo, are typically AP. A fundamental conceptual tool in AP systems is the use of vector clocks.Vector clocks are a way to annotate the versions when they follow a DAG structure.A vector clock can logically be seen as a map from nodes (machines) to integers, i.e., the version number is incremented per machine rather than globally.</p>
<h5><span id="vector-clock">vector clock</span></h5><h6><span id="lamports-logical-clock">Lamport’s logical clock</span></h6><p>Lamport’s Logical Clock was created by Leslie Lamport. It is a procedure to determine the order of events occurring. It provides a basis for the more advanced Vector Clock Algorithm. Due to the absence of a Global Clock in a Distributed Operating System Lamport Logical Clock is needed.</p>
<p>Implementation Rules：<br>[IR1]: If a -&gt; b [‘a’ happened before ‘b’ within the same process] then, Ci(b)  &#x3D;Ci(a) + d<br>[IR2]: Cj &#x3D; max(Cj, tm + d) [If there’s more number of processes, then tm &#x3D; value of Ci(a), Cj &#x3D; max value between Cj and tm + d]</p>
<h6><span id="vector-clocks-in-distributed-systems">Vector Clocks in Distributed Systems</span></h6><p>Vector Clock is an algorithm that generates partial ordering of events and detects causality violations in a distributed system.<br>How does the vector clock algorithm work : </p>
<p>Initially, all the clocks are set to zero.<br>Every time, an Internal event occurs in a process, the value of the processes’s logical clock in the vector is incremented by 1.<br>Every time, a process receives a message, the value of the processes’s logical clock in the vector is incremented by 1, and moreover, each element is updated by taking the maximum of the value in its own vector clock and the value in the vector in the received message (for every element). </p>
<p>To sum up, Vector clocks algorithms are used in distributed systems to provide a causally consistent ordering of events but the entire Vector is sent to each process for every message sent, in order to keep the vector clocks in sync.</p>
<h6><span id="partial-order-relation">partial order relation</span></h6><p>Note that vector clocks can be compared to each other with a partial order relation ≤. A partial order relation is any relation that is reflexive, antisymmetric, and transitive. A total order relation is a partial order in which every element of the set is comparable with every other element of the set. All total order relations are partial order relations, but the converse is not always true.</p>
<h2><span id="references">references</span></h2><p><a target="_blank" rel="noopener" href="https://www.geeksforgeeks.org/normal-forms-in-dbms/">https://www.geeksforgeeks.org/normal-forms-in-dbms/</a><br><a target="_blank" rel="noopener" href="https://www.geeksforgeeks.org/denormalization-in-databases/">https://www.geeksforgeeks.org/denormalization-in-databases/</a><br><a target="_blank" rel="noopener" href="https://www.geeksforgeeks.org/difference-between-view-and-table/">https://www.geeksforgeeks.org/difference-between-view-and-table/</a><br><a target="_blank" rel="noopener" href="https://modern-sql.com/feature/with">https://modern-sql.com/feature/with</a><br><a target="_blank" rel="noopener" href="https://www.geeksforgeeks.org/sql-natural-join/">https://www.geeksforgeeks.org/sql-natural-join/</a><br><a target="_blank" rel="noopener" href="https://mlsdev.com/blog/81-a-beginner-s-tutorial-for-understanding-restful-api">https://mlsdev.com/blog/81-a-beginner-s-tutorial-for-understanding-restful-api</a><br><a target="_blank" rel="noopener" href="https://ghislainfourny.github.io/big-data-textbook/">https://ghislainfourny.github.io/big-data-textbook/</a><br><a target="_blank" rel="noopener" href="https://www.geeksforgeeks.org/lamports-logical-clock/">https://www.geeksforgeeks.org/lamports-logical-clock/</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-09-19T12:48:13.000Z" title="2023-9-19 2:48:13 ├F10: PM┤">2023-09-19</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-10-12T08:08:27.754Z" title="2023-10-12 10:08:27 ├F10: AM┤">2023-10-12</time></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/09/19/bigdata2/">bigdata - Distributed file systems</a></p><div class="content"><h1><span id="distributed-file-systems">Distributed file systems</span></h1><h2><span id="requirements-of-a-distributed-file-system">requirements of a distributed file system</span></h2><p>Going back to our capacity-throughput-latency view of storage, a distributed file system is designed so that, in cruise mode, its bottleneck will be the data flow (throughput), not the latency. We saw that capacity increased much faster than throughput, and that this can be solved with parallelism. We saw that throughput increased much faster than latency decreased, and that this can be solved with batch processing. Distributed file systems support both parallelism and batch processing natively, forming the core part of the ideal storage system accessed by MapReduce or Apache Spark.The origins of such a system come back to the design of GoogleFS, the Google File System. Later on, an open source version of it was released as part of the Hadoop project, initiated by Doug Cutting at Yahoo, and called HDFS, for Hadoop Distributed File System.</p>
<h2><span id="hdfs">HDFS</span></h2><p>HDFS does not follow a key-value model: instead, an HDFS cluster organizes its files as a hierarchy, called the file namespace. Files are thus organized in directories, similar to a local file system. Unlike in S3, HDFS files are furthermore not stored as monolithic blackboxes, but HDFS exposes them as lists of blocks. As for the block size: HDFS blocks are typically 64 MB or 128 MB large, and are thus considerably larger than blocks on a local hard drive (around 4 kB).HDFS is designed to a run on a cluster of machines. </p>
<h3><span id="architecture">architecture</span></h3><p>HDFS is implemented on a fully centralized architecture, in which one node is special and all others are interchangeable and connected to it.<br>In the case of HDFS, the central node is called the NameNode and the other nodes are called the DataNodes. Every file is divided into chunks called blocks. All blocks have a size of exactly 128 MB, except the last one which is usually smaller. Each one of the blocks is then replicated and stored on several DataNodes. How many times? This is a parameter called the replication factor. By default, it is 3. </p>
<p>The NameNode is responsible for the system-wide activity of the HDFS cluster. It store in particular three things: • the file namespace, that is, the hierarchy of directory names and file names, as well as any access control (ACL) information similar to Unix-based systems. • a mapping from each file to the list of its blocks. Each block, in this list, is represented with a 64-bit identifier; the content of the blocks is not on the NameNode. • a mapping from each block, represented with its 64-bit identifier, to the locations of its replicas, that is, the list of the DataNodes that store a copy of this block. The DataNodes store the blocks themselves. These blocks are stored on their local disks. DataNodes send regular heartbeats to the NameNode. The frequency of these heartbeats is configurable and is by default a few seconds (e.g., 3s, but this value may change across releases). This is a way to let the NameNode know that everything is alright.Finally, the DataNode also sends, every couple of hours (e.g., 6h, but this value may change across releases), a full report including all the blocks that it contains. A NameNode never initiates a connection to a DataNode.</p>
<p>Finally, DataNodes are also capable of communicating with each other by forming replication pipelines. A pipeline happens whenever a new HDFS file is created. The client does not send a copy of the block to all the destination DataNodes, but only to the first one. This first DataNode is then responsible for creating the pipeline and propagating the block to its counterparts. When a replication pipeline is ongoing and a new block is being written to the cluster, the content of the block is not sent in one single 128 MB packet. Rather, it is sent in smaller packets (e.g., 64 kB) in a streaming fashion via a network protocol.</p>
<h3><span id="replicas">replicas</span></h3><p>Having this in mind, the first replica of the block, by default, gets written to the same machine that the client is running on. The second replica is written on a DataNode sitting in a different rack than the client, that we call B. The third replica is written to another DataNode on the same rack B.And further replicas are written mostly at random, but respecting two simple rules for resilience: at most one replica per node, and at most two replicas per rack.</p>
<h3><span id="fault-tolerance">Fault tolerance</span></h3><p>HDFS has a single point of failure: the NameNode. If the metadata stored on it is lost, then all the data on the cluster is lost, because it is not possible to reassemble the blocks into files any more.For this reason, the metadata is backed up. More precisely, the file namespace containing the directory and file hierarchy as well as the mapping from files to block IDs is backed up to a so-called snapshot. What is done is that updates to the file system arriving after the snapshot has been made are instead stored in a journal, called edit log, that lists the updates sorted by time of arrival. The snapshot and edit log are stored either locally or on a networkattached drive (not HDFS itself).</p>
<h3><span id="logging-and-importing-data">Logging and importing data</span></h3><p>Two tools are worth mentioning: Apache Flume lets you collect, aggregate and move log data to HDFS. Apache Sqoop lets you import data from a relational database management system to HDFS.</p>
<h2><span id="references">references</span></h2><p><a target="_blank" rel="noopener" href="https://ghislainfourny.github.io/big-data-textbook/">https://ghislainfourny.github.io/big-data-textbook/</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-08-10T09:08:31.000Z" title="2023-8-10 11:08:31 ├F10: AM┤">2023-08-10</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-08-10T09:14:32.076Z" title="2023-8-10 11:14:32 ├F10: AM┤">2023-08-10</time></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/08/10/opencv/">opencv</a></p><div class="content"><h1><span id="basic-functions">basic functions</span></h1><h2><span id="reading-and-writing">reading and writing</span></h2><p>cv2.VideoWriter_fourcc(‘M’, ‘P’, ‘4’, ‘V’)<br>cv2.VideoWriter(filename,fourcc,fps,frameSize[,isColor])<br>cv2.VideoWriter.write(image)</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-08-03T07:41:10.000Z" title="2023-8-3 9:41:10 ├F10: AM┤">2023-08-03</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-08-26T20:34:45.648Z" title="2023-8-26 10:34:45 ├F10: PM┤">2023-08-26</time></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/08/03/object-tracking/">object tracking</a></p><div class="content"><h1><span id="object-tracking">object tracking</span></h1><p>Multiple Object Tracking(MOT) is the task of detecting various objects of interest in a video, tracking these detected objects in subsequent frames by assigning them a unique ID, and maintaining these unique IDs as the objects move around in a video in successive frames.Generally, multiple object tracking happens in two stages: object detection and object association. Object detection is the process of identifying all potential objects of interest in the current frame using object detectors such as Faster-RCNN or YOLO. Object association is the process of linking objects detected in the current frame with its corresponding objects from previous frames, referred to as tracklets. Object or instance association is usually done by predicting the object’s location at the current frame based on previous frames’ tracklets using the Kalman Filter followed by one-to-one linear assignment typically using the Hungarian Algorithm to minimise the total differences between the matching results.</p>
<h2><span id="metrics">Metrics</span></h2><h3><span id="motp-multiple-object-tracking-precision">MOTP (Multiple Object Tracking Precision)</span></h3><p>MOTP (Multi-Object Tracking Precision) expresses how well exact positions of the object are estimated. It is the total error in estimated position for matched ground truth-hypothesis pairs over all frames, averaged by the total number of matches made. This metric is not responsible for recognizing object configurations and evaluating object trajectories.</p>
<h3><span id="mota-multiple-object-tracking-accuracy">MOTA (Multiple Object Tracking Accuracy)</span></h3><p>MOTA (Multi-Object Tracking Accuracy) shows how many errors the tracker system has made in terms of Misses, False Positives, Mismatch errors, etc. Therefore, it can be derived from three error ratios: the ratio of Misses, the ratio of False positives, and the ratio of Mismatches over all the frames.</p>
<h3><span id="idf1-score-idf1">IDF1 score (IDF1)</span></h3><p>IDF1 score (IDF1) is the ratio of correctly identified detections over the average of ground truth and predicted detections.</p>
<h2><span id="benchmarks">Benchmarks</span></h2><h3><span id="otb">OTB</span></h3><h3><span id="kitti">KITTI</span></h3><h3><span id="mot16">MOT16</span></h3><h2><span id="methodsmodels">Methods(models)</span></h2><h3><span id="iou-tracker">IOU tracker</span></h3><p>The Intersection-Over-Union (IOU) tracker uses the IOU values among the detector’s bounding boxes between the two consecutive frames to perform the association between them or assign a new target ID if no match found.</p>
<h3><span id="simple-online-and-realtime-tracking-sort">Simple Online And Realtime Tracking (SORT)</span></h3><p>Simple Online And Realtime Tracking (SORT) is a lean implementation of a tracking-by detection framework.SORT uses the position and size of the bounding boxes for both motion estimation and data association through frames. SORT combines location and motion cues by adopting a Kalman filter to predict the location of the tracklets in the new frame, then computes the IoU between the detection boxes and the predicted boxes as the similarity.</p>
<h3><span id="deepsort">DeepSORT</span></h3><p>DeepSORT replaces the association metric with a more informed metric that combines motion and appearance information. In particular, a “deep appearance” distance metric is added. The core idea is to obtain a vector that can be used to represent a given image. DeepSort adopts a stand-alone RE-ID model to extract appearance features from the detection boxes. After similarity computation matching strategy assigns identities to the objects. This can be done by the Hungarian Algorithm or greedy assignment.</p>
<h3><span id="fairmot">FairMOT</span></h3><p>FairMOT is a new tracking approach built on top of the anchor-free object detection architecture CenterNet.It has a simple network structure that consists of two homogeneous branches for detecting objects and extracting re-ID features.</p>
<h3><span id="transmot">TransMOT</span></h3><p>TransMOT is a new spatial-temporal graph Transformer that solves all these issues. It arranges the trajectories of all the tracked objects as a series of sparse weighted graphs that are constructed using the spatial relationships of the targets. TransMOT then uses these graphs to create a spatial graph transformer encoder layer, a temporal transformer encoder layer, and a spatial transformer decoder layer to model the spatial-temporal relationships of the objects.</p>
<h3><span id="bytetrack">ByteTrack</span></h3><p>BYTE is an effective association method that utilizes all detection boxes from high scores to low ones in the matching process.BYTE is built on the premise that the similarity with tracklets provides a strong cue to distinguish the objects and background in low score detection boxes. BYTE first matches the high score detection boxes to the tracklets based on motion similarity. It uses Kalman Filter to predict the location of the tracklets in the new frame. The motion similarity is computed by the IoU of the predicted box and the detection box. Then, it performs the second matching between the unmatched tracklets.</p>
<p>The primary innovation of BYTETrack is keeping non-background low confidence detection boxes which are typically discarded after the initial filtering of detections and use these low-score boxes for a secondary association step. Typically, occluded detection boxes have lower confidence scores than the threshold, but still contain some information about the objects which make their confidence score higher than purely background boxes. Hence, these low confidence boxes are still meaningful to keep track of during the association stage.</p>
<h3><span id="comparison-of-deepsort-and-bytetrack">Comparison of DeepSort and ByteTrack</span></h3><p>DeepSort uses a pre-trained object detection model to detect objects in each frame and a Siamese network to match the detected objects based on their appearance features. It also uses Kalman filters to predict the locations of the objects in the next frame. ByteTrack, on the other hand, uses a lightweight Siamese network architecture that takes in two input frames and outputs a similarity score. It also uses a simple but effective data augmentation technique to improve its performance on challenging datasets.</p>
<h2><span id="using-bytetrack">using ByteTrack</span></h2><p>ByteTracker initiates a new tracklet only if a detection is not matched with any previous tracklet and the bounding box score is higher than a threshold.  </p>
<h2><span id="references">references</span></h2><p><a target="_blank" rel="noopener" href="https://www.datature.io/blog/introduction-to-bytetrack-multi-object-tracking-by-associating-every-detection-box">https://www.datature.io/blog/introduction-to-bytetrack-multi-object-tracking-by-associating-every-detection-box</a><br><a target="_blank" rel="noopener" href="https://pub.towardsai.net/multi-object-tracking-metrics-1e602f364c0c">https://pub.towardsai.net/multi-object-tracking-metrics-1e602f364c0c</a><br><a target="_blank" rel="noopener" href="https://learnopencv.com/object-tracking-and-reidentification-with-fairmot/">https://learnopencv.com/object-tracking-and-reidentification-with-fairmot/</a><br><a target="_blank" rel="noopener" href="https://medium.com/augmented-startups/top-5-object-tracking-methods-92f1643f8435">https://medium.com/augmented-startups/top-5-object-tracking-methods-92f1643f8435</a><br><a target="_blank" rel="noopener" href="https://medium.com/@pedroazevedo6/object-tracking-state-of-the-art-2022-fe9457b77382">https://medium.com/@pedroazevedo6/object-tracking-state-of-the-art-2022-fe9457b77382</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-20T10:22:48.000Z" title="2023-7-20 12:22:48 ├F10: PM┤">2023-07-20</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-07-26T16:58:41.089Z" title="2023-7-26 6:58:41 ├F10: PM┤">2023-07-26</time></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/07/20/Neural-Networks/">Neural Networks</a></p><div class="content"><h2><span id="training-nerual-networks">training nerual networks</span></h2><h3><span id="suggestions">suggestions</span></h3><p>The first step to training a neural net is to not touch any neural net code at all and instead begin by thoroughly inspecting your data. </p>
<p> Our next step is to set up a full training + evaluation skeleton and gain trust in its correctness via a series of experiments. At this stage it is best to pick some simple model that you couldn’t possibly have screwed up somehow.<br> Tips &amp; tricks for this stage:<br>Always use a fixed random seed to guarantee that when you run the code twice you will get the same outcome.<br>Make sure to disable any unnecessary fanciness. As an example, definitely turn off any data augmentation at this stage.<br>When plotting the test loss run the evaluation over the entire (large) test set. Do not just plot test losses over batches and then rely on smoothing them in Tensorboard. We are in pursuit of correctness and are very willing to give up time for staying sane.<br>Monitor metrics other than loss that are human interpretable and checkable (e.g. accuracy). Whenever possible evaluate your own (human) accuracy and compare to it.<br>Train an input-independent baseline, (e.g. easiest is to just set all your inputs to zero). This should perform worse than when you actually plug in your data without zeroing it out. Does it? i.e. does your model learn to extract any information out of the input at all?</p>
<p>The approach I like to take to finding a good model has two stages: first get a model large enough that it can overfit (i.e. focus on training loss) and then regularize it appropriately (give up some training loss to improve the validation loss). The reason I like these two stages is that if we are not able to reach a low error rate with any model at all that may again indicate some issues, bugs, or misconfiguration.</p>
<p> In the early stages of setting baselines I like to use Adam with a learning rate of 3e-4. In my experience Adam is much more forgiving to hyperparameters, including a bad learning rate.</p>
<h2><span id="references">references</span></h2><p><a target="_blank" rel="noopener" href="http://karpathy.github.io/2019/04/25/recipe/">http://karpathy.github.io/2019/04/25/recipe/</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-19T17:56:52.000Z" title="2023-7-19 7:56:52 ├F10: PM┤">2023-07-19</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-07-21T13:34:23.750Z" title="2023-7-21 3:34:23 ├F10: PM┤">2023-07-21</time></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/07/19/transformer/">transformer</a></p><div class="content"><h1><span id="introduction">Introduction</span></h1><p>The Transformer is a deep learning architecture introduced in the paper “Attention is All You Need” by Vaswani et al., published in 2017.<br>The Transformer is based on the self-attention mechanism, which allows it to capture long-range dependencies in sequences more effectively than traditional recurrent neural networks (RNNs) or convolutional neural networks (CNNs). The key components of the Transformer are:Self-Attention Mechanism,Encoder-Decoder Architecture,Multi-Head Attention,Positional Encoding,Feed-Forward Neural Networks.</p>
<h2><span id="self-attention-mechanism">Self-Attention Mechanism</span></h2><p>The self-attention mechanism allows the model to weigh the importance of different words in a sentence while encoding the sequence. It computes the attention scores for each word in the input sequence based on its relationships with other words. By attending to relevant words, the model can focus on the most informative parts of the sequence.</p>
<p>The first step in calculating self-attention is to create three vectors from each of the encoder’s input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process.</p>
<p>The second step in calculating self-attention is to calculate a score. Say we’re calculating the self-attention for the first word in this example, “Thinking”. We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position.<br>The score is calculated by taking the dot product of the query vector with the key vector of the respective word we’re scoring. So if we’re processing the self-attention for the word in position #1, the first score would be the dot product of q1 and k1. The second score would be the dot product of q1 and k2.</p>
<p>The third and fourth steps are to divide the scores by 8 (the square root of the dimension of the key vectors used in the paper – 64. This leads to having more stable gradients. There could be other possible values here, but this is the default), then pass the result through a softmax operation. Softmax normalizes the scores so they’re all positive and add up to 1.</p>
<p>The fifth step is to multiply each value vector by the softmax score.</p>
<p>The sixth step is to sum up the weighted value vectors.This produces the output of the self-attention layer at this position (for the first word).</p>
<h3><span id="multi-head-attention">Multi-Head Attention</span></h3><p>To capture different types of dependencies and relationships, the Transformer uses multi-head attention. It performs self-attention multiple times with different learned projection matrices, allowing the model to attend to various aspects of the input.</p>
<p>With multi-headed attention we have not only one, but multiple sets of Query&#x2F;Key&#x2F;Value weight matrices (the Transformer uses eight attention heads, so we end up with eight sets for each encoder&#x2F;decoder). Each of these sets is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders&#x2F;decoders) into a different representation subspace.If we do the same self-attention calculation we outlined above, just eight different times with different weight matrices, we end up with eight different Z matrices.We concat the matrices then multiply them by an additional weights matrix WO to condense these eight down into a single matrix.</p>
<h3><span id="sequence-to-sequence-model">sequence-to-sequence model</span></h3><p>A sequence-to-sequence model is a model that takes a sequence of items (words, letters, features of an images…etc) and outputs another sequence of items.the model is composed of an encoder and a decoder.The encoder processes each item in the input sequence, it compiles the information it captures into a vector (called the context). After processing the entire input sequence, the encoder sends the context over to the decoder, which begins producing the output sequence item by item.The context is a vector (an array of numbers, basically) in the case of machine translation. The encoder and decoder tend to both be recurrent neural networks.By design, a RNN takes two inputs at each time step: an input (in the case of the encoder, one word from the input sentence), and a hidden state. The word, however, needs to be represented by a vector. To transform a word into a vector, we turn to the class of methods called “word embedding” algorithms.The context vector turned out to be a bottleneck for these types of models. It made it challenging for the models to deal with long sentences.A solution was proposed in Bahdanau et al., 2014 and Luong et al., 2015. These papers introduced and refined a technique called “Attention”.Attention allows the model to focus on the relevant parts of the input sequence as needed.</p>
<h3><span id="attention">attention</span></h3><p>An attention model differs from a classic sequence-to-sequence model in two main ways:First, the encoder passes a lot more data to the decoder. Instead of passing the last hidden state of the encoding stage, the encoder passes all the hidden states to the decoder;Second, an attention decoder does an extra step before producing its output. In order to focus on the parts of the input that are relevant to this decoding time step, the decoder does the following:Look at the set of encoder hidden states it received,Give each hidden state a score,Multiply each hidden state by its softmaxed score, thus amplifying hidden states with high scores, and drowning out hidden states with low scores. </p>
<h2><span id="encoder-decoder-architecture">Encoder-Decoder Architecture</span></h2><p>The Transformer architecture consists of two main components: the encoder and the decoder. The encoder takes an input sequence and processes it, while the decoder generates an output sequence based on the encoded representation.</p>
<p>One detail in the architecture of the encoder that we need to mention before moving on, is that each sub-layer (self-attention, ffnn) in each encoder has a residual connection around it, and is followed by a layer-normalization step.</p>
<p>The encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence.The following steps repeat the process until a special symbol is reached indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did. And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word.</p>
<p>In the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to -inf) before the softmax step in the self-attention calculation.<br>The “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.</p>
<p>The decoder stack outputs a vector of floats. How do we turn that into a word? That’s the job of the final Linear layer which is followed by a Softmax Layer.The Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector.The softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.</p>
<h3><span id="layer-normalization">Layer Normalization</span></h3><p>In traditional normalization techniques like Batch Normalization, the activations of a layer are normalized by computing the mean and variance over a batch of examples. This normalization helps stabilize and accelerate the training process, especially for deeper networks. However, it introduces a dependency on the batch size during training, which can be problematic in scenarios where batch sizes vary or during inference when processing individual samples.</p>
<p>Layer Normalization addresses this dependency by computing the mean and variance across all the units within a single layer for each training example. This means that normalization is done independently for each sample and does not rely on batch statistics.</p>
<h2><span id="positional-encoding">Positional Encoding</span></h2><p>Since Transformers do not inherently have positional information like RNNs, positional encodings are added to the input embeddings. These positional encodings provide the model with information about the order of the elements in the input sequence,or the distance between different words in the sequence. </p>
<h2><span id="training">Training</span></h2><h3><span id="loss-function">loss function</span></h3><h4><span id="cross-entropy">cross entropy</span></h4><p>The cross-entropy loss calculates the negative log-likelihood of the true class’s predicted probability.</p>
<h4><span id="kullbackleibler-divergence">Kullback–Leibler divergence</span></h4><p>Kullback-Leibler (KL) divergence, also known as relative entropy, is a measure of how one probability distribution diverges from another.KL divergence measures the average amount of information lost when using Q to approximate P. It is not symmetric.</p>
<h3><span id="decoding">decoding</span></h3><h4><span id="greedy-decoding">greedy decoding</span></h4><p>In greedy decoding, at each step of sequence generation, the model selects the most likely output token based on its predicted probability distribution. It chooses the token with the highest probability without considering the impact on future decisions. This means that the model makes locally optimal choices at each step without considering the global context of the entire sequence.For example, in machine translation, a model using greedy decoding will predict each target word one at a time, selecting the word with the highest probability given the source sentence and previously generated words. The process continues iteratively until an end-of-sentence token is generated.</p>
<h4><span id="beam-search">Beam search</span></h4><p>In beam search, instead of selecting only the most likely token at each step, the algorithm maintains a fixed-size list, known as the “beam,” containing the most promising candidate sequences. The beam size determines how many candidate sequences are considered at each decoding step.<br>At the beginning of the decoding process, the beam is initialized with a single token representing the start of the sequence. At each step, the model generates the probabilities for the next possible tokens and expands the beam with the top-k most likely candidate sequences based on their cumulative probabilities. The k represents the beam size, and higher values of k result in a more diverse exploration of possibilities.</p>
<h2><span id="references">references</span></h2><p><a target="_blank" rel="noopener" href="http://fancyerii.github.io/2019/03/09/transformer-illustrated/#%E6%A6%82%E8%BF%B0">http://fancyerii.github.io/2019/03/09/transformer-illustrated/#%E6%A6%82%E8%BF%B0</a><br><a target="_blank" rel="noopener" href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/</a><br><a target="_blank" rel="noopener" href="http://jalammar.github.io/illustrated-transformer/">http://jalammar.github.io/illustrated-transformer/</a><br><a target="_blank" rel="noopener" href="https://colah.github.io/posts/2015-09-Visual-Information/">https://colah.github.io/posts/2015-09-Visual-Information/</a><br><a target="_blank" rel="noopener" href="https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained">https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-18T15:33:42.000Z" title="2023-7-18 5:33:42 ├F10: PM┤">2023-07-18</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-07-18T15:34:17.207Z" title="2023-7-18 5:34:17 ├F10: PM┤">2023-07-18</time></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/07/18/segmentation/">segmentation</a></p><div class="content"><h1><span id="image-segmentation">Image segmentation</span></h1><p>Image segmentation is a sub-domain of computer vision and digital image processing which aims at grouping similar regions or segments of an image under their respective class labels. </p>
<h2><span id="semantic-segmentation">Semantic segmentation</span></h2><p>Semantic segmentation refers to the classification of pixels in an image into semantic classes.</p>
<h2><span id="instance-segmentation">Instance segmentation</span></h2><p>Instance segmentation models classify pixels into categories on the basis of “instances” rather than classes. </p>
<h2><span id="panoptic-segmentation">Panoptic segmentation</span></h2><p>Panoptic segmentation can be expressed as the combination of semantic segmentation and instance segmentation where each instance of an object in the image is segregated and the object’s identity is predicted. </p>
<p>Neural networks that perform segmentation typically use an encoder-decoder structure where the encoder is followed by a bottleneck and a decoder or upsampling layers directly from the bottleneck (like in the FCN).</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-07-11T16:58:49.000Z" title="2023-7-11 6:58:49 ├F10: PM┤">2023-07-11</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-07-11T17:10:28.614Z" title="2023-7-11 7:10:28 ├F10: PM┤">2023-07-11</time></span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/07/11/Regular-Expressions/">Regular Expressions</a></p><div class="content"><h2><span id="regular-expressions">Regular expressions</span></h2><p>A formal language for specifying text strings</p>
<h3><span id="rules">rules</span></h3><p>Disjunctions:<br>Letters inside square brackets[]: [A-Z]<br>pipe |: a|b|c<br>Negation in Disjunction: [^Ss]</p>
<p>?:  When placed after a character or a group, the question mark makes it optional, meaning that the character or group can occur zero or one time.<br>When placed after a quantifier, such as *, +, or ?, it modifies the quantifier to be non-greedy or lazy. A non-greedy quantifier matches as few characters as possible, while a greedy quantifier matches as many characters as possible.<br>*:0 or more of previous char<br>+:1 or more of previous char<br>.:any char</p>
<p>Anchors:<br>^: The begining. $: The end.</p>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous is-invisible is-hidden-mobile"><a href="/page/0/">Previous</a></div><div class="pagination-next"><a href="/page/2/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link is-current" href="/">1</a></li><li><a class="pagination-link" href="/page/2/">2</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/6/">6</a></li></ul></nav></div><!--!--><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/">s-serenity</a><p class="is-size-7"><span>&copy; 2023 s-serenity</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p></div><div class="level-end"></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><!--!--><script src="/js/main.js" defer></script><!--!--></body></html>