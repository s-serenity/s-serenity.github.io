<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>s-serenity</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="s-serenity"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="s-serenity"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Recording and sharing my learning process."><meta property="og:type" content="blog"><meta property="og:title" content="s-serenity"><meta property="og:url" content="http://yoursite.com/"><meta property="og:site_name" content="s-serenity"><meta property="og:description" content="Recording and sharing my learning process."><meta property="og:locale" content="en_US"><meta property="og:image" content="http://yoursite.com/img/og_image.png"><meta property="article:author" content="s-serenity"><meta property="article:tag" content="Algorithm study development"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://yoursite.com/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://yoursite.com"},"headline":"s-serenity","image":["http://yoursite.com/img/og_image.png"],"author":{"@type":"Person","name":"s-serenity"},"publisher":{"@type":"Organization","name":"s-serenity","logo":{"@type":"ImageObject","url":"http://yoursite.com/img/logo.svg"}},"description":"Recording and sharing my learning process."}</script><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.1.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="s-serenity" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item is-active" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-01-25T16:14:35.000Z" title="2024-1-25 5:14:35 ├F10: PM┤">2024-01-25</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-02-03T12:11:22.353Z" title="2024-2-3 1:11:22 ├F10: PM┤">2024-02-03</time></span><span class="level-item"><a class="link-muted" href="/categories/theory/">theory</a></span><span class="level-item">20 minutes read (About 2965 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/01/25/pai11/">pai - review notes</a></p><div class="content"><h1><span id="fundamentals">fundamentals</span></h1><h2><span id="probability">probability</span></h2><h3><span id="sample-space-event-space-σ-algebra-and-probability-space">sample space, event space, σ-algebra and probability space</span></h3><p>A probability space is a mathematical construct that consists of three elements: the sample space (S), the event space (E), and a probability measure (P). Additionally, a sigma-algebra (σ-algebra) is associated with the event space.</p>
<p>The sample space is the set of all possible outcomes of an experiment.The event space is a collection of subsets of the sample space.<br>A sigma-algebra is a collection of subsets of the sample space. It includes the sample space, is closed under complementation, and is closed under countable unions. The probability measure is a function that assigns probabilities to events.</p>
<h3><span id="probability-mass-function-pmf-and-cumulative-distribution-function-cdfprobability-density-function-pdf">probability mass function (PMF) and cumulative distribution function (CDF),probability density function (PDF)</span></h3><p>The Probability Mass Function (PMF) is a function that gives the probability that a discrete random variable is exactly equal to a certain value. The Cumulative Distribution Function (CDF) of a random variable gives the probability that<br>X takes on a value less than or equal to x.<br>The Probability Density Function (PDF) is applicable to continuous random variables.The total area under the PDF curve is equal to 1.</p>
<h3><span id="continuous-distributions">Continuous Distributions</span></h3><p>normal distribution(Gaussian):The Gaussian CDF cannot be expressed in closed-form. Note that the mean of a Gaussian distribution coincides with the maximizer of its PDF, also called mode of a distribution.</p>
<h3><span id="joint-probability-conditional-probability-sum-rule-product-rulechain-rulethe-law-of-total-probability">Joint Probability, Conditional Probability, Sum rule, Product rule(chain rule),the law of total probability</span></h3><p>Joint probability refers to the probability of the occurrence of two or more events simultaneously. Conditional probability is the probability of one event occurring given that another event has already occurred. The sum rule, or addition rule, gives the probability of the union of two events. The product rule, also known as the chain rule, provides a way to express the joint probability of multiple events. The law of total probability is a way to express the probability of an event B by considering all possible ways in which<br>B can occur. </p>
<h3><span id="independence-conditional-independence">independence, conditional independence</span></h3><p>Conditional independence does not necessarily imply unconditional independence, and vice versa.</p>
<h3><span id="directed-graphical-modelsbayesian-networks">Directed Graphical Models(Bayesian networks)</span></h3><p>Directed graphical models (also called Bayesian networks) are often used to visually denote the (conditional) independence relationships of a large number of random variables.</p>
<h3><span id="expectation-covariance-and-variance-standard-deviationlaw-of-total-variance">Expectation, Covariance and Variance, Standard deviation,Law of total variance</span></h3><h3><span id="change-of-variables-formula">Change of variables formula</span></h3><h2><span id="probabilistic-inference">Probabilistic inference</span></h2><h3><span id="bayes-ruleconjugate-priors">Bayes’ rule,Conjugate Priors</span></h3><h3><span id="gaussianguassian-random-vector">gaussian,Guassian random vector</span></h3><p>Any affine transformation of a Gaussian random vector is a Gaussian random vector.</p>
<h2><span id="supervised-learning-and-point-estimates">Supervised Learning and Point Estimates</span></h2><h3><span id="maximum-likelihood-estimationmaximum-a-posteriori-estimation">Maximum Likelihood Estimation,Maximum a Posteriori Estimation,</span></h3><p>The MLE and MAP estimate can be seen as a naïve approximation of probabilistic inference, represented by a point density which “collapses” all probability mass at the mode of the posterior distribution.</p>
<h2><span id="exercise">exercise</span></h2><h3><span id="affine-transformationjacobian-matrix">affine transformation,Jacobian matrix,</span></h3><h1><span id="pml">PML</span></h1><h2><span id="linear-regression">linear regression</span></h2><h3><span id="linear-regressionmle-ridge-regressionmap">linear regression(MLE), ridge regression(MAP)</span></h3><h3><span id="ridgelasso">ridge,lasso</span></h3><p>least absolute shrinkage and selection operator (lasso): Laplace prior, L1 regularization.<br>Ridge: Gaussian prior, L2 regularization.</p>
<p>The primary difference lies in the penalty terms: L1 regularization uses the sum of absolute values, and L2 regularization uses the sum of squared values.<br>L1 regularization tends to result in exact zeros, leading to sparse solutions, whereas L2 regularization generally leads to smaller, non-zero coefficients.</p>
<h3><span id="bayesian-linear-regression-blr">Bayesian linear regression (BLR)</span></h3><h3><span id="aleatoric-and-epistemic-uncertainty">Aleatoric and Epistemic Uncertainty</span></h3><p>epistemic uncertainty: corresponds to the uncertainty about our model due to the lack of data. aleatoric uncertainty: “irreducible noise”, cannot be explained by the inputs and any model from the model class. </p>
<p>equation under the law of total variance. </p>
<h3><span id="kernel">kernel</span></h3><h2><span id="filtering">Filtering</span></h2><p>The process of keeping track of the state using noisy observations is also known as Bayesian filtering or recursive Bayesian estimation.</p>
<h3><span id="kalman-filter">Kalman filter</span></h3><p>A Kalman filter is simply a Bayes filter using a Gaussian distribution over the states and conditional linear Gaussians to describe the evolution of states and observations.</p>
<h2><span id="gaussian-process">Gaussian Process</span></h2><p>A Gaussian process is an infinite set of random variables such that any finite number of them are jointly Gaussian.</p>
<h3><span id="kernel-function-feature-space-rkhsstationarity-and-isotropy">kernel function, feature space, RKHS,Stationarity and isotropy</span></h3><p>A Gaussian process with a linear kernel is equivalent to Bayesian linear regression.</p>
<p>For ν = 1/2, the Matérn kernel is equivalent to the Laplace kernel. For ν → ∞, the Matérn kernel is equivalent to the Gaussian kernel.</p>
<p>Note that stationarity is a necessary condition for isotropy. In other words, isotropy implies stationarity.</p>
<p>skip 4.3.4</p>
<h2><span id="model-selection">model selection</span></h2><h3><span id="maximizing-the-marginal-likelihood">Maximizing the Marginal Likelihood</span></h3><p>Marginal likelihood maximization is an empirical Bayes method. Often it is simply referred to as empirical Bayes.<br>this approach typically avoids overfitting even though we do not use a separate training and validation set. maximizing the marginal likelihood naturally encourages trading between a large likelihood and a large prior.</p>
<h2><span id="approximations">Approximations</span></h2><h3><span id="random-fourier-featuresbochners-theoremuniform-convergence-of-fourier-features">random Fourier features,Bochner’s theorem,Uniform convergence of Fourier features</span></h3><h3><span id="inducing-points-methodsubset-of-regressors-sor-approximationfully-independent-training-conditional-fitc-approximation">inducing points method,subset of regressors (SoR) approximation,fully independent training conditional (FITC) approximation</span></h3><h2><span id="variational-inference">Variational Inference</span></h2><h3><span id="laplace-approximationbayesian-logistic-regression">Laplace Approximation,Bayesian Logistic Regression,</span></h3><p>The Laplace approximation matches the shape of the true posterior around its mode but may not represent it accurately elsewhere — often leading to extremely overconfident predictions.</p>
<h3><span id="variational-familymean-field-distribution">Variational family,mean-field distribution</span></h3><h3><span id="information-theorysurpriseentropyjensens-inequalitycross-entropykl-divergenceforward-and-reverse-kl-divergence">Information Theory,Surprise,entropy,Jensen’s Inequality,Cross-entropy,KL-divergence,Forward and Reverse KL-divergence</span></h3><p>The uniform distribution has the maximum entropy among all discrete distributions supported on {1, . . . , n}.</p>
<p>In words, KL(p∥q) measures the additional expected surprise when observing samples from p that is due to assuming the (wrong) distribution q.</p>
<p>It can be seen that the reverse KL-divergence tends to greedily select the mode and underestimating the variance which, in this case, leads to an overconfident prediction. The forward KL-divergence, in contrast, is more conservative.</p>
<p>Note, however, that reverse-KL is not greedy in the same sense as Laplace approximation, as it does still take the variance into account and does not purely match the mode of p.</p>
<h3><span id="minimizing-forward-kl-as-maximum-likelihood-estimationminimizing-forward-kl-as-moment-matching">Minimizing Forward-KL as Maximum Likelihood Estimation,Minimizing Forward-KL as Moment Matching</span></h3><p> First, we observe that minimizing the forward KL-divergence is equivalent to maximum likelihood estimation on an infinitely large sample size.</p>
<p>A Gaussian qλ minimizing KL(p∥qλ) has the same first and second moment as p.</p>
<h3><span id="evidence-lower-boundgaussian-vi-vs-laplace-approximation-gradient-of-evidence-lower-boundscore-gradients-and-reparameterization-trick">Evidence Lower Bound,Gaussian VI vs Laplace approximation, Gradient of Evidence Lower Bound(score gradients and Reparameterization trick)</span></h3><p>maximizing the ELBO coincides with minimizing reverse-KL.</p>
<p>maximizing the ELBO selects a variational distribution q that is close to the prior distribution p(·) while also maximizing the average likelihood of the data p(y1:n | x1:n, θ) for θ ∼ q.</p>
<p>Note that for a noninformative prior p(·) ∝ 1, maximizing the ELBO is equivalent to maximum likelihood estimation.</p>
<p>skip 5.5.2</p>
<h2><span id="markov-chain-monte-carlo-methods">Markov Chain Monte Carlo Methods</span></h2><p>The key idea of Markov chain Monte Carlo methods is to construct a Markov chain, which is efficient to simulate and has the stationary distribution p.</p>
<h3><span id="markov-chainsstationarityconvergence-markov-propertytime-homogeneous-markov-chains">Markov Chains(Stationarity,convergence), Markov property,time-homogeneous Markov chains</span></h3><p>Intuitively, the Markov property states that future behavior is independent of past states given the present state.</p>
<h3><span id="stationary-distributionaperiodicergodicityfundamental-theorem-of-ergodic-markov-chains">Stationary distribution,aperiodic,Ergodicity,Fundamental theorem of ergodic Markov chains</span></h3><p>After entering a stationary distribution π, a Markov chain will always remain in the stationary distribution.</p>
<p>It can be shown that there exists a unique stationary distribution π if the Markov chain is irreducible, that is, if every state is reachable from every other state with a positive probability when the Markov chain is run for enough steps.</p>
<p>Even if a Markov chain has a unique stationary distribution, it must not converge to it.</p>
<p>In words, a Markov chain is aperiodic iff for every state x, the transition graph has a closed path from x to x with length k for all k ∈ N greater than some k0 ∈ N.</p>
<p>A Markov chain is ergodic iff there exists a t ∈ N0 such that for any x, x′ ∈ S we have p(t)(x′ | x) &gt; 0.</p>
<p>A commonly used strategy to ensure that a Markov chain is ergodic is to add “self-loops” to every vertex in the transition graph.</p>
<p>An ergodic Markov chain has a unique stationary distribution π (with full support) irrespectively of the initial distribution q0. This naturally suggests constructing an ergodic Markov chain such that its stationary distribution coincides with the posterior distribution. If we then sample “sufficiently long”, Xt is drawn from a distribution that is “very close” to the posterior distribution.</p>
<h3><span id="how-quickly-does-a-markov-chain-convergetotal-variation-distance-and-mixing-time">How quickly does a Markov chain converge?(Total variation distance and Mixing time)</span></h3><h3><span id="detailed-balance-equationergodic-theorem">Detailed Balance Equation,Ergodic Theorem</span></h3><p>A Markov chain that satisfies the detailed balance equation with respect to π is called reversible with respect to π.</p>
<p>Ergodic Theorem is a way to generalize the (strong) law of large numbers to Markov chains.</p>
<h3><span id="elementary-sampling-methodsmetropolis-hastings-algorithmmetropolis-hastings-theorem-gibbs-sampling">Elementary Sampling Methods,Metropolis-Hastings Algorithm,Metropolis-Hastings theorem, Gibbs Sampling</span></h3><p>A popular example of a Metropolis-Hastings algorithm is Gibbs sampling. </p>
<h3><span id="sampling-as-optimizationgibbs-distributionlangevin-dynamicsmetropolis-adjusted-langevin-algorithm-mala-or-langevin-monte-carlo-lmcunadjusted-langevin-algorithm-ula-stochastic-gradient-langevin-dynamics-hamiltonian-monte-carlo">Sampling as Optimization,Gibbs distribution,Langevin Dynamics,Metropolis adjusted Langevin algorithm (MALA) or Langevin Monte Carlo (LMC),unadjusted Langevin algorithm (ULA), Stochastic Gradient Langevin Dynamics, Hamiltonian Monte Carlo</span></h3><p>A useful property is that Gibbs distributions always have full support. Observe that the posterior distribution can always be interpreted as a Gibbs distribution as long as prior and likelihood have full support.</p>
<p>Langevin dynamics adapts the Gaussian proposals of the Metropolis-Hastings algorithm to search the state space in an “informed” direction. The simple idea is to bias the sampling towards states with lower energy, thereby making it more likely that a proposal is accepted. A natural idea is to shift the proposal distribution perpendicularly to the gradient of the energy function. The resulting variant of Metropolis-Hastings is known as the Metropolis adjusted Langevin algorithm (MALA) or Langevin Monte Carlo (LMC).</p>
<p>The HMC algorithm is an instance of Metropolis-Hastings which uses momentum to propose distant points that conserve energy, with high acceptance probability.</p>
<h3><span id="path">path</span></h3><p>target: show that the stationary distribution of a Markov chain coincides with the posterior distribution.<br>base: Detailed Balance Equation which shows that A Markov chain that satisfies the detailed balance equation with respect to π is called reversible with respect to π and if the Markov chain is reversible with respect to π then π is a stationary distribution. With posterior distribution p(x) = 1/Z q(x), substitute the posterior for π in the detailed balance equation, we can remove z, so we do not need to know the true posterior p to check that the stationary distribution of our Markov chain coincides with p, it suffices to know the finite measure q. However, until now, this does not allow us to estimate expectations over the posterior distribution. Note that although constructing such a Markov chain allows us to obtain samples from the posterior distribution, they are not independent. Thus, the law of large numbers and Hoeffding’s inequality do not apply, but there is a way to generalize the (strong) law of large numbers to Markov chains, which is Ergodic theorem. </p>
<p>Next we need to consider how to construct Markov chain with the goal of approximating samples from the posterior distribution p. One way is Metropolis-Hastings Algorithm, in which we are given a proposal distribution and we use the acceptance distribution to decide whether to follow the proposal. Another way is Gibbs sampling. </p>
<p>MCMC techniques can be generalized to<br>continuous random variables / vectors. Observe that the posterior distribution can always be interpreted as a Gibbs distribution as long as prior and likelihood have full support.</p>
<p>Langevin dynamics adapts the Gaussian proposals of the Metropolis-Hastings algorithm to search the state space in an “informed” direction. The simple idea is to bias the sampling towards states with lower energy, thereby making it more likely that a proposal is accepted. A natural idea is to shift the proposal distribution perpendicularly to the gradient of the energy function. The resulting variant of Metropolis-Hastings is known as the Metropolis adjusted Langevin algorithm (MALA) or Langevin Monte Carlo (LMC).</p>
<p>skip 6.3.5</p>
<h2><span id="bdl">BDL</span></h2><h3><span id="artificial-neural-networksactivation-functions">Artificial Neural Networks,Activation Functions,</span></h3><p>Non-linear activation functions allow the network to represent arbitrary functions. This is known as the universal approximation theorem.</p>
<h3><span id="bayesian-neural-networks-heteroscedastic-noise-variational-inferencemarkov-chain-monte-carlo-swaswagdropout-and-dropconnect-probabilistic-ensembles">Bayesian Neural Networks, Heteroscedastic Noise, Variational Inference,Markov Chain Monte Carlo, SWA,SWAG,Dropout and Dropconnect, Probabilistic Ensembles</span></h3><p>Intuitively, variational inference in Bayesian neural networks can be interpreted as averaging the predictions of multiple neural networks drawn according to the variational posterior qλ.</p>
<h3><span id="calibrationexpected-calibration-error-ece-maximum-calibration-error-mce-histogram-binning-isotonic-regression-platt-scaling-temperature-scaling">Calibration,expected calibration error (ECE), maximum calibration error (MCE), Histogram binning, Isotonic regression, Platt scaling, Temperature scaling</span></h3><p>We say that a model is wellcalibrated if its confidence coincides with its accuracy across many predictions. Compare within each bin, how often the model thought the inputs belonged to the class (confidence) with how often the inputs actually belonged to the class (frequency).</p>
<h1><span id="sequential-decision-making">Sequential Decision-Making</span></h1><h2><span id="active-learning">Active Learning</span></h2><h3><span id="conditional-entropyjoint-entropy">Conditional Entropy,Joint entropy</span></h3><p>A very intuitive property of entropy is its monotonicity: when conditioning on additional observations the entropy can never increase.</p>
<h3><span id="mutual-informationinformation-gain-the-law-of-total-expectation-data-processing-inequality-interaction-informationsynergy-and-redundancy-submodularity-of-mutual-information-marginal-gain-submodularity-monotone">Mutual Information(information gain), the law of total expectation, data processing inequality, interaction information(Synergy and Redundancy), Submodularity of Mutual Information, Marginal gain, Submodularity, monotone,</span></h3><p>data processing inequality: which says that processing cannot increase the information contained in a signal.</p>
<p>F is submodular iff “adding” x to the smaller set A yields more marginal gain than adding x to the larger set B.</p>
<p>I is monotone submodular.</p>
<h3><span id="maximizing-mutual-information-greedy-submodular-function-maximization-uncertainty-sampling-marginal-gain-of-maximizing-mutual-informationbayesian-active-learning-by-disagreement-bald">Maximizing Mutual Information, Greedy submodular function maximization, Uncertainty Sampling, Marginal gain of maximizing mutual information,Bayesian active learning by disagreement (BALD)</span></h3><p>skip proof of Theorem 8.15</p>
<p>Therefore, if f is modeled by a Gaussian and we assume homoscedastic noise, greedily maximizing mutual information corresponds to simply picking the point x with the largest variance. This algorithm is also called uncertainty sampling.</p>
<h3><span id="experimental-design-entropy-search">Experimental Design, Entropy Search</span></h3><p>skip</p>
<h2><span id="bayesian-optimization">Bayesian Optimization</span></h2><h3><span id="exploration-exploitation-dilemmaonline-learning-and-bandits-multi-armed-bandits-regret-sublinear-regret-cesàro-mean">Exploration-Exploitation Dilemma,Online Learning and Bandits, Multi-Armed Bandits, Regret, sublinear regret, Cesàro mean</span></h3><p>Bayesian optimization can be interpreted as a variant of the MAB problem where there can be a potentially infinite number of actions (arms), but their rewards are correlated (because of the smoothness of the Gaussian process prior).</p>
<h3><span id="acquisition-functions-upper-confidence-bound-bayesian-confidence-intervals-regret-of-gp-ucb-information-gain-of-common-kernels-frequentist-confidence-intervals-probability-of-improvement-expected-improvement-ei-thompson-sampling-probability-matching-information-directed-sampling">Acquisition Functions, Upper Confidence Bound, Bayesian confidence intervals, Regret of GP-UCB, Information gain of common kernels, Frequentist confidence intervals, probability of improvement, expected improvement (EI), Thompson Sampling, probability matching, Information-Directed Sampling</span></h3><p>skip Information-Directed Sampling</p>
<h2><span id="markov-decision-processes">Markov Decision Processes</span></h2><p>Planning deals with the problem of deciding which action an agent should play in a (stochastic) environment. A key formalism for probabilistic planning in known environments are so-called Markov decision processes.</p>
<h3><span id="policydiscounted-payoffstate-value-functionstate-action-value-function-also-called-q-function">Policy,discounted payoff,state value function,state-action value function (also called Q-function)</span></h3><p>A policy is a function that maps each state x ∈ X to a probability distribution over the actions.</p>
<h3><span id="bellman-expectation-equation">Bellman Expectation Equation</span></h3><h3><span id="policy-evaluation-fixed-point-iteration-contractionbanach-fixed-point-theorem">Policy Evaluation, Fixed-point Iteration, contraction,Banach fixed-point theorem</span></h3><h3><span id="policy-optimization-greedy-policies-bellman-optimality-equation-bellmans-theorem-policy-iteration-value-iteration">Policy Optimization, Greedy Policies, Bellman Optimality Equation, Bellman’s theorem, Policy Iteration, Value Iteration,</span></h3><p>In particular, if for every state there is a unique action that maximizes the state-action value function, the policy π⋆ is deterministic and unique.</p>
<p>Intuitively, the Bellman optimality equations express that the value of a state under an optimal policy must equal the expected return for the best action from that state.</p>
<p>Value iteration converges to an ε-optimal solution in a polynomial number of iterations. Unlike policy iteration, value iteration does not converge to an exact solution in general.</p>
<h3><span id="partial-observability">Partial Observability,</span></h3><p>Whereas MDPs are controlled Markov chains, POMDPs are controlled hidden Markov models.</p>
<h2><span id="tabular-reinforcement-learning">Tabular Reinforcement Learning</span></h2><h3><span id="trajectories-episodic-setting-continuous-setting-on-policy-and-off-policy-methods">Trajectories, episodic setting, continuous setting, On-policy and Off-policy Methods,</span></h3><p>on-policy methods are used when the agent has control over its own actions, in other words, the agent can freely choose to follow any policy. off-policy methods can be used even when the agent cannot freely choose its actions.</p>
<h3><span id="model-based-approaches-balancing-exploration-and-exploitation-ε-greedy-softmax-explorationboltzmann-exploration-rmax-algorithm">Model-based Approaches, Balancing Exploration and Exploitation, ε-greedy, Softmax Exploration(Boltzmann exploration), Rmax algorithm</span></h3><p>skip Remark 11.3: Asymptotic convergence</p>
<p>Note that ε-greedy is GLIE with probability 1 if the sequence (εt)t∈N0 satisfies the RM-conditions (A.56), e.g., if εt = 1/t.</p>
<p>A significant benefit to model-based reinforcement learning is that it is inherently off-policy. That is, any trajectory regardless of the policy used to obtain it can be used to improve the model of the underlying Markov decision process. In the model-free setting, this not necessarily true.</p>
<h3><span id="model-free-approaches-on-policy-value-estimation-bootstrapping-temporal-difference-learning-sarsa-off-policy-value-estimation-q-learning-optimistic-q-learning">Model-free Approaches, On-policy Value Estimation, bootstrapping, temporal-difference learning, SARSA, Off-policy Value Estimation, Q-learning, optimistic Q-learning</span></h3><h2><span id="model-free-reinforcement-learning">Model-free Reinforcement Learning</span></h2><h3><span id="value-function-approximation-dqn-experience-replay-maximization-bias-double-dqn">Value Function Approximation, DQN, experience replay, maximization bias, Double DQN</span></h3><h3><span id="policy-approximationpolicy-gradient-methods-policy-value-function-policy-gradient-score-gradient-estimator-score-gradients-with-baselines-downstream-returns-reinforce-algorithm">Policy Approximation(policy gradient methods),  policy value function, Policy Gradient, Score gradient estimator, Score gradients with baselines, Downstream returns, REINFORCE algorithm</span></h3><p>The main advantage of policy gradient methods such as REINFORCE is that they can be used in continuous action spaces. However, REINFORCE is not guaranteed to find an optimal policy. Even when operating in very small domains, REINFORCE can get stuck in local optima.</p>
<h3><span id="on-policy-actor-criticsadvantage-function-policy-gradient-theorem-actor-criticq-actor-criticonline-actor-critic-advantage-actor-critic-bias-variance-tradeoff-trust-region-policy-optimization-trpo-proximal-policy-optimization-ppo">On-policy Actor-Critics,Advantage Function, Policy Gradient Theorem, Actor-Critic(Q actor-critic,Online actor-critic), advantage actor-critic, bias-variance tradeoff, trust-region policy optimization (TRPO), Proximal policy optimization (PPO)</span></h3><p>One approach in the online setting (i.e., non-episodic setting), is to simply use SARSA for learning the critic. To learn the actor, we use stochastic gradient descent with gradients obtained using single samples.</p>
<h3><span id> </span></h3><h2><span id="model-based-reinforcement-learning">Model-based Reinforcement Learning</span></h2><h3><span id="planning-over-finite-horizons-receding-horizon-control-rhc-model-predictive-control-mpc-random-shooting-methods">Planning over Finite Horizons, receding horizon control (RHC), model predictive control (MPC), Random shooting methods</span></h3><h2><span id="cheat-sheet">cheat sheet</span></h2><h3><span id="from-book">from book</span></h3><p>conditional distribution for gaussian;<br>Gaussian process posterior;<br>the predictive posterior at the test point;<br>common kernels;<br>the Hessian of the logistic loss;<br>Surprise,entropy,Jensen’s Inequality,Cross-entropy,KL-divergence,<br>ELBO,<br>the law of large numbers and Hoeffding’s inequality,<br>Hoeffding bound, </p>
<h3><span id="from-exercise">from exercise</span></h3><p> Woodbury push-through identity;<br> Solution to problem 3.6;</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-01-03T15:06:23.000Z" title="2024-1-3 4:06:23 ├F10: PM┤">2024-01-03</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-01-22T23:11:01.055Z" title="2024-1-23 12:11:01 ├F10: AM┤">2024-01-23</time></span><span class="level-item"><a class="link-muted" href="/categories/database/">database</a></span><span class="level-item">an hour read (About 8850 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2024/01/03/bigdata13/">bigdata - review notes</a></p><div class="content"><h2><span id="data-cubes">data cubes</span></h2><h3><span id="slides">slides</span></h3><h4><span id="oltp-vs-olap">OLTP VS OLAP:</span></h4><p>record keeping vs decision support<br>read-intensive vs write-intensive<br>detailed individual records vs summarized data<br>Lots of transactions on small portions of data vs Large portions<br>of the data.<br>fully interactive vs slow interactive.<br><img src="/2024/01/03/bigdata13/image-117.png" alt="Alt text"><br>ETL: Extract Transform Load</p>
<h4><span id="slicing-and-dicingpivoting">slicing and dicing,Pivoting</span></h4><p>Slicing involves selecting a specific “slice” or subset of the data cube by fixing one or more dimensions at a particular value. Dicing involves creating a subcube by selecting specific values for two or more dimensions. It’s like slicing, but you are selecting a rectangular subset of the cube, cutting through more than one dimension. Pivoting is another operation often used alongside slicing and dicing. It involves rotating the data cube to view it from a different perspective by swapping the positions of dimensions. </p>
<h4><span id="aggregation-and-roll-up">Aggregation and roll-up</span></h4><p>These operations allow users to summarize and view data at different levels of granularity within a multidimensional dataset. Roll-up is a specific form of aggregation that involves moving from a lower level of detail to a higher level by collapsing one or more dimensions(moving up the hierarchy from a finer level of granularity (monthly) to a coarser level (quarterly)).</p>
<h4><span id="two-flavors-of-olap-rolap-and-molap">Two flavors of OLAP: ROLAP and MOLAP</span></h4><p>ROLAP (Relational Online Analytical Processing) and MOLAP (Multidimensional Online Analytical Processing) are two different approaches to organizing and processing data in the context of Online Analytical Processing (OLAP) systems. In summary, the main difference between ROLAP and MOLAP lies in how they store and organize data. ROLAP uses relational databases to store multidimensional data in tables, while MOLAP uses specialized databases with a cube structure optimized for efficient multidimensional analysis.</p>
<h4><span id="fact-table-and-satellite-table">fact table and Satellite table</span></h4><p>Fact tables are surrounded by dimension tables in a star schema. They usually have foreign key relationships with dimension tables, linking to various dimensions that provide context to the measures. Satellite tables are used to store additional details about dimension members that are not part of the core structure of the dimension table. Satellite tables typically have a foreign key relationship with the main dimension table, linking to the primary key of the dimension. They can be joined to the main dimension table to enrich the analysis with additional context.</p>
<h4><span id="star-schema-and-snow-flake-schema">star schema and snow-flake schema</span></h4><p> In a star schema, there is a central fact table surrounded by dimension tables. The fact table contains quantitative data (measures), and the dimension tables store descriptive data related to the measures. A snowflake schema is an extension of the star schema, where dimension tables are normalized into multiple related tables. This normalization leads to a structure resembling a snowflake when visualized, as opposed to the star-like structure of a star schema.</p>
<h4><span id="sql-querying-tables-and-mdx-querying-cubes">SQL querying tables and MDX querying Cubes</span></h4><p> MDX stands for Multi-Dimensional<br>eXpressions. </p>
<h4><span id="roll-up-and-drill-down">“Roll-up” and “drill down”</span></h4><p> Roll-up involves summarizing data at a higher level of aggregation or moving from a lower level of detail to a higher level. It’s a way of viewing data at a coarser level in the hierarchy. Drill down is the opposite of roll-up. It involves accessing more detailed information or moving from a higher level of aggregation to a lower, more detailed level.</p>
<h4><span id="grouping-sets-cube-rollup">GROUPING SETS, CUBE, ROLLUP</span></h4><p> GROUPING SETS is a SQL feature that allows you to specify multiple grouping sets within a single query.  The CUBE operation is an extension of GROUP BY that generates all possible combinations of grouped elements. ROLLUP is another extension of GROUP BY that generates subtotals and grand totals for a specified set of columns. It creates a hierarchy of grouping levels, calculating subtotals as it rolls up the hierarchy. Note that the column order in rollup is important.</p>
<h3><span id="book">book</span></h3><h3><span id="exercise">exercise</span></h3><p>concepts of fact tables;<br>pivot table;<br>SQL query with grouping set, cube…</p>
<h2><span id="graph-databases">Graph Databases</span></h2><h3><span id="sildes">sildes</span></h3><h4><span id="index-free-adjacency">Index-free adjacency</span></h4><p>With index-free adjacency, graph databases are designed in such a way that relationships between nodes are directly accessible without the need for an index structure. Instead of traversing indexes to find related nodes, the relationships are stored alongside the nodes, allowing for faster and more efficient traversal.Each node maintains direct pointers or references to its neighboring nodes, making it possible to navigate from one node to another without the need for an explicit index lookup. This design principle is particularly beneficial for scenarios where traversing relationships in a graph is a common and performance-critical operation.</p>
<h4><span id="property-graph-and-triple-stores-rdf">Property Graph and Triple stores (RDF)</span></h4><p>Labeled property graphs: ingredients: Nodes, Edges,Properties,Labels.<br>A property graph is a graph data model that consists of nodes, edges, and properties.RDF is a standard for representing data about resources in the form of subject-predicate-object triples.</p>
<h4><span id="cypher">Cypher</span></h4><p>Cypher is a query language commonly used with property graph databases like Neo4j.</p>
<h4><span id="sparql">SPARQL</span></h4><p>SPARQL is a query language commonly used with RDF triple stores.</p>
<h4><span id="neo4j">Neo4j</span></h4><p>Data replication;<br>sharding:neo4j fabric;<br>Caching and pages;</p>
<h4><span id="rdf">RDF</span></h4><p>IRI (=URI for all practical purposes);<br><img src="/2024/01/03/bigdata13/image-118.png" alt="Alt text"><br>RDF formats:RDF/XML;Turtle;JSON-LD;RDFa;N-Triples.</p>
<h3><span id="book">book</span></h3><h4><span id="labeled-property-graph-model-vs-triple-stores">Labeled property graph model vs. triple stores</span></h4><p>labeled property graphs enhance mathematical graphs with extra ingredients: properties, and labels.<br>Labels are some sort of “tags”, in the form of a string, that can be attached to a node or an edge.<br>Each node and each edge can be associated with a map from strings to values, which represents its properties.</p>
<p>Triple stores are a different and simpler model. It views the graph as nodes and edges that all have labels, but without any properties.<br>The graph is then represented as a list of edges, where each edge is a triple with the label of the origin node (called the subject), the label of the edge (called the property), and the label of the destination node (called the object).</p>
<p>Labels can be:URIs,Literals, that is, atomic values,Literals are only allowed as objects. Absent, in which case the node is called a blank node. Blank nodes are only allowed as subjects or objects, but not as properties.</p>
<h4><span id="cypher">cypher</span></h4><p>Other graph databases have other means of querying data. Many, including Neo4j, support the RDF query language SPARQL and the imperative, path-based query language Gremlin.</p>
<p>Cypher enables a user (or an application acting on behalf of a user) to ask the database to find data that matches a specific pattern. Colloquially, we ask the database to “find things like this.” And the way we describe what “things like this” look like is to draw them, using ASCII art.</p>
<p>The MATCH clause is at the heart of most Cypher queries. </p>
<h4><span id="native-graph-processing">native graph processing</span></h4><p>To understand why native graph processing is so much more efficient than graphs based on heavy indexing, consider the following. Depending on the implementation, index lookups could be O(log n) in algorithmic complexity versus O(1) for looking up immediate relationships. To traverse a network of m steps, the cost of the indexed approach, at O(m log n), dwarfs the cost of O(m) for an implementation that uses index-free adjacency.</p>
<p>With index-free adjacency, bidirectional joins are effectively precomputed and stored in the database as relationships</p>
<h4><span id="stores">stores</span></h4><p>Neo4j stores graph data in a number of different store files. Each store file contains the data for a specific part of the graph (e.g., there are separate stores for nodes, relationships, labels, and properties). Like most of the Neo4j store files, the node store is a fixed-size record store, where each record is nine bytes in length. Fixed-size records enable fast lookups for nodes in the store file.</p>
<h4><span id="cyphertraverser-api-and-core-api">CYPHER,Traverser API and Core API</span></h4><p>Neo4j’s Core API is an imperative Java API that exposes the graph primitives of nodes, relationships, properties, and labels to the user. When used for reads, the API is lazily evaluated, meaning that relationships are only traversed as and when the calling code demands the next node. </p>
<p>The Traversal Framework is a declarative Java API. It enables the user to specify a set of constraints that limit the parts of the graph the traversal is allowed to visit.</p>
<p>Cypher can be more tolerant of structural changes—things such as variable-length paths help mitigate variation and change.</p>
<p> however, the Traversal Framework tends to perform marginally less well than a well-written Core API query.</p>
<p> Choosing between the Core API and the Traversal Framework is a matter of deciding whether the higher abstraction/lower coupling of the Traversal Framework is sufficient, or whether the close-to-the-metal/higher coupling of the Core API is in fact necessary for implementing an algorithm correctly and in accordance with our performance requirements.</p>
<h3><span id="exercise">exercise</span></h3><h2><span id="querying-trees">Querying trees</span></h2><h3><span id="slides">slides</span></h3><h4><span id="jsoniq">JSONiq</span></h4><p>Data independence with<br>heterogeneous, denormalized data.<br>JSONiq Data Model (JDM): Sequences of Items;</p>
<h4><span id="declarative-languagesfunctional-languages-and-set-based-languages">Declarative languages,Functional languages and Set-based languages</span></h4><p>Declarative languages focus on describing what the program should accomplish, rather than specifying how to achieve it. Functional languages treat computation as the evaluation of mathematical functions and avoid changing state or mutable data.Haskell, Lisp, and Erlang are functional programming languages.<br>Parts of JavaScript and Python support functional programming paradigms. Set-based languages are a subset of declarative languages that focus on manipulating sets of data.<br>It’s worth noting that languages can often belong to more than one category. For example, SQL is both declarative and set-based, and functional programming concepts can be integrated into languages that are not purely functional. </p>
<h4><span id="flwor-clauses">FLWOR clauses</span></h4><h4><span id="queryabstract-syntax-tree-expression-tree-iterator-tree">query,Abstract Syntax Tree, Expression Tree, Iterator Tree</span></h4><h4><span id="materialized-executionstreamed-executionparallel-execution">Materialized execution,Streamed execution,Parallel execution</span></h4><p>Materialized execution takes lots of space. Streamed execution takes lots of time.<br>Parallel execution can take lots of machines.<br>Execution modes determined statically for every expression and clause.<br><img src="/2024/01/03/bigdata13/image-119.png" alt="Alt text"><br><img src="/2024/01/03/bigdata13/image-120.png" alt="Alt text"></p>
<h4><span id="rumble">Rumble</span></h4><h4><span id="traditional-rdbmswarehouses-vs-data-lakes">Traditional RDBMS/warehouses vs. data lakes</span></h4><h3><span id="book">book</span></h3><h3><span id="exercise">exercise</span></h3><h2><span id="document-stores">Document Stores</span></h2><h3><span id="slides">slides</span></h3><h4><span id="json-and-xml">JSON and XML</span></h4><h4><span id="sql-and-nosql">SQL and NoSQL:</span></h4><p>NoSQL: validation after the data was populated.</p>
<h4><span id="crud">CRUD</span></h4><p>Create,Read,Update,Delete</p>
<h4><span id="mongodb">MongoDB</span></h4><p> projecting away: not selecting a field.</p>
<h4><span id="hash-indicestree-indices-b-treecompund-index">hash indices,Tree indices (B+-tree),compund index</span></h4><p> Limitations of hash indices:<br> No support for range queries,Hash function not perfect in real life, Space requirements for collision avoidance.<br> B+-tree: All leaves at same depth, All non-leaf nodes have between 3 and 5 children,But it’s fine if the root has less.</p>
<h3><span id="book">book</span></h3><h3><span id="exercise">exercise</span></h3><h2><span id="performance-at-large-scales">Performance at Large Scales</span></h2><h3><span id="slides">slides</span></h3><h4><span id="sources-of-bottleneck">sources of bottleneck</span></h4><p>Memory, CPU, Disk I/O, Network I/O.<br>Sweet Spot for MapReduce and Spark: Disk I/O.</p>
<h4><span id="latency-throughputresponse-time">Latency, Throughput,Response time</span></h4><p>Latency: When do I start receiving data.<br>Throughput:”How fast can we transmit data.<br>Response time=Latency + Transfer.</p>
<h4><span id="speedupamdahls-lawgustafsons-law">speedup,Amdahl’s law,Gustafson’s law</span></h4><p>speedup = latency(old)/latency(new).<br>Gustafson’s law:Constant computing power.<br>Amdahl’s law: Constant problem size.</p>
<h4><span id="scaling-outscaling-up">Scaling out,Scaling up</span></h4><h4><span id="tail-latency-sla">tail latency, SLA</span></h4><h3><span id="book">book</span></h3><h3><span id="exercise">exercise</span></h3><h2><span id="massive-parallel-processing-imapreduce">Massive Parallel Processing I(MapReduce)</span></h2><h3><span id="slides">slides</span></h3><h4><span id="combine-and-reduce">combine and reduce</span></h4><h4><span id="map-task-and-reduce-task-map-slot-and-reduce-slot-map-phase-amd-reduce-phase">map task and reduce task, map slot and reduce slot, map phase amd reduce phase</span></h4><p>no combine task and combine slot.<br>map slot = sequential map task; map task = sequential split map;</p>
<h4><span id="splitmapreduce-and-blockhdfs">split(mapreduce) and block(HDFS)</span></h4><p>1 split = 1 map task<br>split(mapreduce)!= block(HDFS)</p>
<h3><span id="book">book</span></h3><h4><span id="mapreduce-model">mapreduce model</span></h4><p>In MapReduce, the input data, intermediate data, and output data are all made of a large collection of key-value pairs (with the keys not necessarily unique, and not necessarily sorted by key).</p>
<h4><span id="mapreduce-architecture">MapReduce architecture</span></h4><p>In the original version of MapReduce, the main node is called JobTracker, and the worker nodes are called TaskTrackers.<br>In fact, the JobTracker typically runs on the same machine as the NameNode (and HMaster) and the TaskTrackers on the same machines as the DataNodes (and RegionServers). This is called “bring the query to the data.”</p>
<p>Note that shuffling can start before the map phase is over, but the reduce phase can only start after the map phase is over.</p>
<h4><span id="combine">combine</span></h4><p>Combining happens during the map phase.<br>In fact, in most of the cases, the combine function will be identical to the reduce function, which is generally possible if the intermediate key-value pairs have the same type as the output key-value pairs, and the reduce function is both associative and commutative.</p>
<h4><span id="mapreduce-programming-api">MapReduce programming API</span></h4><p>In Java, the user needs to define a so-called Mapper class that contains the map function, and a Reducer class that contains the reduce function.<br>A map function takes in particular a key and a value. Note that it outputs key-value pairs via the call of the write method on the context, rather than with a return statement.<br>A reduce function takes in particular a key and a list of values.</p>
<h4><span id="functiontaskslotphase">function,task,slot,phase</span></h4><p>A map function is a mathematical, or programmed, function that takes one input key-value pair and returns zero, one or more intermediate key-value pairs.<br>Then, a map task is an assignment (or “homework”, or “TODO”) that consists in a (sequential) series of calls of the map function on a subset of the input.</p>
<p>There is no such thing as a combine task. Calls of the combine function are not planned as a task, but is called ad-hoc during flushing and compaction.</p>
<p>The map tasks are processed thanks to compute and memory resources (CPU and RAM). These resources are called map slots. One map slot corresponds to one CPU core and some allocated memory. Each map slot then processes one map task at a time, sequentially.</p>
<p>The map phase thus consists of several map slots processing map tasks in parallel.</p>
<h4><span id="short-circuitingsplit-and-block">short-circuiting(split and block)</span></h4><p>This is because the DataNode process of HDFS and the TaskTracker process of MapReduce are on the same machine. Thus, getting a replica of the block containing the data necessary to the processing of the task is as simple as a local read. This is called short-circuiting.<br>split(logical level)!=HDFS block(physical level).</p>
<h3><span id="exercise">exercise</span></h3><h2><span id="resource-management">Resource management</span></h2><h3><span id="book">book</span></h3><h4><span id="yarn">YARN</span></h4><p>YARN means Yet Another Resource manager. It was introduced as an additional layer that specifically handles the management of CPU and memory resources in the cluster.<br>YARN, unsurprisingly, is based on a centralized architecture in which the coordinator node is called the ResourceManager, and the worker nodes are called NodeManagers. NodeManagers furthermore provide slots (equipped with exclusively allocated CPU and memory) known as containers.</p>
<p>YARN provides generic support for allocating resources to any application and is application-agnostic. When the user launches a new application, the ResourceManager assigns one of the container to act as the ApplicationMaster which will take care of running the application.</p>
<p>Version 2 of MapReduce works on top of YARN by leaving the job lifecycle management to an ApplicationMaster.<br>It is important to understand that, unlike the JobTracker, the ResourceManager does not monitor tasks, and will not restart slots upon failure. This job is left to the ApplicationMasters.</p>
<h4><span id="scheduling-strategies">Scheduling strategies</span></h4><p>FIFO scheduling,Capacity scheduling,Fair scheduling</p>
<h4><span id="dominant-resource-fairness-algorithm">Dominant Resource Fairness algorithm.</span></h4><p>The two (or more) dimensions are projected again to a single dimension by looking at the dominant resource for each user.</p>
<h2><span id="massive-parallel-processing-ii-spark">Massive Parallel Processing II (Spark)</span></h2><h3><span id="slides">slides</span></h3><h4><span id="yarn">YARN</span></h4><p>ResourceManager + NodeManager; ResourceManager allocates one NodeManager as application master. Application Master communicates with containers. ResourceManager<br>Does not monitor tasks and Does not restart upon failure. Fault tolerance is on the application master.</p>
<h4><span id="spark">spark</span></h4><p>Full-DAG query processing.distributed acyclic graph (DAG)<br>RDD: Resilient Distributed Dataset.</p>
<h4><span id="rdd">RDD</span></h4><p>lazy evaluation:Lazy evaluation means that the execution of transformations on RDDs is deferred until an action is triggered. Instead of immediately executing the transformations, Spark keeps track of the sequence of transformations in the form of a logical execution plan. The actual computation is only performed when an action is called.</p>
<p>A narrow dependency (also known as a narrow transformation) occurs when each partition of the parent RDD contributes to at most one partition of the child RDD. In other words, the number of partitions remains the same before and after the transformation, and each partition of the child RDD depends on a one-to-one relationship with partitions of the parent RDD.</p>
<p>A wide dependency (also known as a wide transformation) occurs when each partition of the parent RDD contributes to multiple partitions of the child RDD. This typically involves operations that require data shuffling or redistribution, such as groupByKey or reduceByKey.</p>
<h4><span id="data-frames">Data Frames</span></h4><h3><span id="book">book</span></h3><h4><span id="resilient-distributed-datasetsrdds">Resilient distributed datasets(RDDs)</span></h4><p>Resilient means that they remain in memory or on disk on a “best effort” basis, and can be recomputed if need be. Distributed means that, just like the collections of key-value pairs in MapReduce, they are partitioned and spread over multiple machines.<br>A major difference with MapReduce, though, is that RDDs need not be collections of pairs. Since a key-value pair is a particular example of possible value, RDDs are a generalization of the MapReduce model for input, intermediate input and output.</p>
<h4><span id="the-rdd-lifecycle">The RDD lifecycle</span></h4><p>Creation,Transformation(Mapping or reducing, in this model, become two very specific cases of transformations.),Action.<br>transformations:<br>unary transformations: The filter transformation,The map transformation,The flatMap transformation,The distinct transformation,The sample transformation<br>Binary transformations: taking the union of two RDDs, take the intersection, take the subtraction.<br>Pair transformations: Spark has transformations specifically tailored for RDDs of key-value pairs: The key transformation,The values transformation,The reduceByKey transformation, The groupByKey transformation,The sortByKey transformation,The mapValues transformation,The join transformation,The subtractByKey transformation.<br>Actions: The collect action,The count action,The countByValue action,The take action,The top action,The takeSample action,The reduce action,saveAsTextFile action,saveAsObjectFile action.</p>
<p>Note that Spark, at least in its RDD API, is not aware of any particular format or syntax, i.e., it is up to the user to parse and serialize values to the appropriate text or bytes.</p>
<p>Actions on Pair RDDs: There are actions specifically working on RDDs of key-value pairs:The countByKey action, The lookup action, </p>
<h4><span id="lazy-evaluation">Lazy evaluation</span></h4><p>It is only with an action that the entire computation pipeline is put into motion, leading to the computation of all the necessary intermediate RDDs all the way down to the final output corresponding to the action.</p>
<h4><span id="physical-architecture">Physical architecture</span></h4><p>There are two kinds of transformations: narrow-dependency transformations and wide-dependency transformations.</p>
<p>Such a chain of narrow-dependency transformations executed efficiently as a single set of tasks is called a stage, which would correspond to what is called a phase in MapReduce.</p>
<h4><span id="optimizations">Optimizations</span></h4><p>Pinning RDDs, Pre-partitioning.</p>
<h4><span id="dataframes-in-spark">DataFrames in Spark</span></h4><p>A DataFrame can be seen as a specific kind of RDD: it is an RDD of rows (equivalently: tuples, records) that has relational integrity, domain integrity, but not necessarily (as the name “Row” would otherwise fallaciously suggest) atomic integrity.</p>
<p>Note that Spark automatically infers the schema from discovering the JSON Lines file, which adds a static performance overhead that does not exist for raw RDDs: there is no free lunch.</p>
<p>Unlike the RDD transformation API, there is no guarantee that the execution will happen as written, as the optimizer is free to reorganize the actual computations.</p>
<h4><span id="spark-sql">Spark SQL</span></h4><p>both GROUP BY and ORDER BY will trigger a shuffle in the system.  The SORT BY clause can sort rows within each partition, but not across partitions, i.e., does not induce any shuffling. The DISTRIBUTE BY clause forces a repartition by putting all rows with the same value (for the specified field(s)) into the same new partition.</p>
<p>use both SORT and DISTRIBUTE = the use of another clause, CLUSTER BY.</p>
<p>A word of warning must be given on SORT, DISTRIBUTE and CLUSTER clauses: they are, in fact, a breach of data independence, because they expose partitions.</p>
<p>explode() and lateral view: Lateral views are more powerful and generic than just an explode() because they give more control, and they can also be used to go down several levels of nesting. </p>
<h3><span id="book">book</span></h3><h3><span id="exercise">exercise</span></h3><p>Spark’s RDDs are by default recomputed each time you run an action on them. Please note that both persist() and cache() are lazy operations themselves. The caching operation will, in fact, only take place when the first action is called. With successive action calls, the cached RDD will be used.</p>
<h2><span id="introduction">introduction</span></h2><h3><span id="book">book</span></h3><p>three Vs: Volume, Variety, Velocity.<br><img src="/2024/01/03/bigdata13/image-121.png" alt="Alt text"><br>four more shapes: trees, unstructured, cubes,graphs.<br>three factors: Capacity,Throughput, Latency.<br><img src="/2024/01/03/bigdata13/image-122.png" alt="Alt text"></p>
<h4><span id="partial-function-and-function">partial function and function</span></h4><p>A function is a strict mapping where every element in the domain is mapped to a unique element in the codomain.<br>A partial function is a mapping where not every element in the domain necessarily has a defined value in the codomain.<br>For a table, we need to throw in three additional constraints: relational integrity, domain integrity and atomic integrity.</p>
<h2><span id="lessons-learned-from-the-past">lessons learned from the past</span></h2><h3><span id="book">book</span></h3><h4><span id="natural-join-theta-join-and-outer-joinsemi-outer-join">natural join, theta join, and outer join,Semi-outer join</span></h4><p>A natural join is a type of join that combines rows from two tables based on columns with the same name and data type. The columns used for the join condition are not explicitly specified; instead, the database system automatically identifies the matching columns. A theta join is a generalization of the natural join, where the join condition is explicitly specified using a comparison operator. </p>
<h4><span id="normal-forms">Normal forms</span></h4><p>The first normal form was already covered earlier: it is in fact atomic integrity.<br>The second normal form takes it to the next level: it requires that each column in a record contains information on the entire record. The third normal form additionally forbids functional dependencies on anything else than the primary key. </p>
<h4><span id="sql">SQL</span></h4><p>SQL is a declarative language, which means that the user specifies what they want, and not how to compute it: it is up to the underlying system to figure out how to best execute the query. It is also a set-based language, in the sense that it manipulates sets of records at a time, rather than single values as is common in other languages. It is also, to some limited extent, a functional language in the sense that it contains expressions that can nest in each other (nested queries).</p>
<h4><span id="acid">ACID</span></h4><p>There are four main properties (often called ACID):Atomicity,Consistency,Isolation,Durability.</p>
<h3><span id="exercise">exercise</span></h3><h4><span id="sql">SQL</span></h4><p>1NF,2NF,3NF,BCNF;<br>intersection;</p>
<h2><span id="storing-data">Storing data</span></h2><h3><span id="book">book</span></h3><h4><span id="cap">CAP</span></h4><p>(Atomic) Consistency, Availability, Partition tolerance. </p>
<h2><span id="document-stores">document stores</span></h2><h3><span id="book">book</span></h3><p>Document stores provide a native database management system for semi-structured data. A document store typically specializes in either JSON or XML data, even though some companies (e.g., MarkLogic) offer support for both. It is important to understand that document stores are optimized for the typical use cases of many records of small to medium sizes. Typically, a collection can have millions or billions of documents, while each single document weighs no more than 16 MB (or a size in a similar magnitude).</p>
<h4><span id="mongodb">MongoDB</span></h4><p>In MongoDB, the format is a binary version of JSON called BSON.</p>
<p>The API of MongoDB, like many document stores, is based on the CRUD paradigm. CRUD means Create, Read, Update, Delete.</p>
<p>MongoDB automatically adds to every inserted document a special field called “ id” and associated with a value called an Object ID and with a type of its own.</p>
<h4><span id="hash-indices-and-tree-indices">hash indices and tree indices</span></h4><p>Hash indices are used to optimize point queries and more generally query that select on a specific value of a field.</p>
<h4><span id="secondary-indices">Secondary indices</span></h4><p>By default, MongoDB always builds a tree index for the id field. Users can request to build hash and tree indices for more fields. These indices are called secondary indices.</p>
<h3><span id="exercise">exercise</span></h3><p>By default, MongoDB creates the _id index, which is an ascending unique index on the _id field, for all collections when the collection is created. You cannot remove the index on the _id field.</p>
<h2><span id="querying-denormalized-data">Querying denormalized data</span></h2><h3><span id="book">book</span></h3><h4><span id="features-of-a-query-language">Features of a query language</span></h4><p>First, it is declarative. This means that the users do not focus on how the query is computed, but on what it should return.</p>
<p>Second, it is functional. This means that the query language is made of composable expressions that nest with each other, like a Lego game.</p>
<p>Finally, it is set-based, in the sense that the values taken and returned by expressions are not only single values (scalars), but are large sequences of items (in the case of SQL, an item is a row).</p>
<h4><span id="jsoniq">JSONiq</span></h4><p>It is possible to filter any sequence with a predicate, where <script type="math/tex">in the predicate refers to the current item being tested.
example: "json-doc("file.json").o[].a.b[][</script>.c = 3]”<br>To access the n-th member of an array, you can use double-squarebrackets: “json-doc(“file.json”).o[[2]].a”.</p>
<p>Do not confuse sequence positions (single square brackets) with array positions (double square brackets)!</p>
<p>The empty sequence enjoys special treatment: if one of the sides (or both) is the empty sequence, then the arithmetic expression returns an empty sequence (no error).</p>
<p>Note that unlike SQL, JSONiq logic expressions are two-valued and return either true or false.</p>
<h4><span id="general-comparison">general comparison</span></h4><p>universal and existential quantification: every and some;<br>JSONiq has a shortcut for existential quantification on value comparisons. This is called general comparison.</p>
<h4><span id="flwor-expressions">FLWOR expressions</span></h4><p>One of the most important and powerful features of JSONiq is the FLWOR expression. It corresponds to SELECT-FROM-WHERE queries in SQL.</p>
<h3><span id="exercise">exercise</span></h3><p>Accessing a JSON dataset can be done in two ways depending on the exact format:<br>If this is a file that contains a single JSON object spread over multiple lines, use json-doc(URL).<br>If this is a file that contains one JSON object per line (JSON Lines), use json-file(URL).</p>
<h2><span id="hdfs">HDFS</span></h2><h3><span id="book">book</span></h3><h4><span id="hdfs-data-model">HDFS data model</span></h4><p>HDFS does not follow a key-value model: instead, an HDFS cluster organizes its files as a hierarchy, called the file namespace. Files are thus organized in directories, similar to a local file system.</p>
<h4><span id="key-value-model-vs-file-hierarchy">key-value model vs file hierarchy</span></h4><p>The key-value model and file hierarchy are two different approaches to organizing and accessing data within storage systems. While the key-value model excels in flexibility and quick data access, file hierarchy provides a structured and predictable organization suitable for many traditional storage use cases.</p>
<h4><span id="object-storage-vs-block-storage">object storage vs block storage</span></h4><p>Organizes data as objects, each containing both data and metadata. Objects are stored in a flat address space.<br>Organizes data as fixed-size blocks, typically within storage volumes.Requires a file system to manage and organize data into files and directories.</p>
<h4><span id="object-storage-and-key-value-model">object storage and key-value model</span></h4><p>Object storage is a data storage architecture that manages data as objects, each containing both data and metadata. Objects are stored in a flat address space without the hierarchy found in traditional file systems. In the key-value model, data is organized as pairs of keys and values. Each key uniquely identifies a value, and the system allows for the efficient retrieval and storage of data based on these key-value pairs.<br>Object storage systems often use a key-value model internally to manage objects. Each object has a unique identifier (key), and the associated data and metadata form the corresponding value.</p>
<h4><span id="physical-architecture">physical architecture</span></h4><p>In the case of HDFS, the central node is called the NameNode and the other nodes are called the DataNodes. In fact, more precisely, the NameNode and DataNodes are processes running on these nodes.<br>The NameNode stores in particular three things:the file namespace,a mapping from each file to the list of its blocks, a mapping from each block, represented with its 64-bit identifier, to the locations of its replicas.</p>
<h3><span id="exercise">exercise</span></h3><h4><span id="object-storage-vs-block-storage">object storage vs block storage</span></h4><h2><span id="syntax">syntax</span></h2><h3><span id="book">book</span></h3><h4><span id="json">json</span></h4><p>JSON stands for JavaScript Object Notation.<br>JSON is made of exactly six building blocks: strings, numbers, Booleans, null, objects, and arrays.<br>in JSON, escaping is done with backslash characters (\).<br>The way a number appears in syntax is called a lexical representation, or a literal.<br>JSON places a few restrictions: a leading + is not allowed. Also, a leading 0 is not allowed except if the integer part is exactly 0 (in which case it is even mandatory).<br>Objects are simply maps from strings to values. The keys of an object must be strings.<br>The JSON standard recommends for keys to be unique within an object.</p>
<h4><span id="unicode">unicode</span></h4><p>Unicode is a standard that assigns a numeric code (called a code point) to each character in order to catalogue them across all languages of the world, even including emojis. The code point must be indicated in base 16. </p>
<h4><span id="xml">XML</span></h4><p>XML stands for eXtensible Markup Language.<br>XML’s most important building blocks are elements, attributes, text and comments.<br>Unlike JSON keys, element names can repeat at will.<br>At the top-level, a well-formed XML document must have exactly one element.<br>Attributes appear in any opening elements tag and are basically keyvalue pairs. Values can be either double-quoted or single-quoted. The key is never quoted, and it is not allowed to have unquoted values. Within the same opening tag, there cannot be duplicate keys. Attributes can never appear in a closing tag. It is not allowed to create attributes that start with XML or xml, or any case combination. because this is reserved for namespace declarations.<br>a single comment alone is not well-formed XML (remember: we need exactly one top-level element).<br>XML documents can be identified as such with an optional text declaration containing a version number and an encoding.<br>Another tag that might appear right below, or instead of, the text declaration is the doctype declaration. It must then repeat the name of the top-level element.<br>Remember that in JSON, it is possible to escape sequences with a backslash character. In XML, this is done with an ampersand (&amp;) character.<br>Escape sequences can be used anywhere in text, and in attribute values.there are a few places where they are mandatory: In text, &amp; and &lt; MUST be escaped. In double-quoted attribute values, ”, &amp; and &lt; MUST be escaped. In single-quoted attribute values, ’, &amp; and &lt; MUST be escaped. </p>
<h4><span id="namespaces-in-xml">Namespaces in XML</span></h4><p>A namespace is identified with a URI.<br>The triplet (namespace, prefix, localname) is called a QName (for “qualified name”).<br>For the purpose of the comparisons of two QNames (and thus of documents), the prefix is ignored: only the local name and the namespace are compared.<br>First, unprefixed attributes are not sensitive to default namespaces: unlike elements, the namespace of an unprefixed attribute is always absent even if there is a default namespace. </p>
<h3><span id="exercise">exercise</span></h3><h4><span id="xml-names">xml names</span></h4><p>Remember:<br>Element names are case-sensitive.<br>Element names must start with a letter or underscore.<br>Element names cannot start with the letters xml (or XML, or Xml, etc).<br>Element names can contain letters, digits, hyphens, underscores, and periods.<br>Element names cannot contain spaces.</p>
<h4><span id="json-key-names">JSON Key names</span></h4><p> The only restriction the JSON syntax imposes on the key names is that “ and \ must be escaped.</p>
<h2><span id="wide-column-stores">Wide column stores</span></h2><h3><span id="book">book</span></h3><h4><span id="hdfs-vs-wide-column-stores">HDFS VS Wide column stores</span></h4><p>The problem with HDFS is its latency: HDFS works well with very large files (at least hundreds of MBs so that blocks even start becoming useful), but will have performance issues if accessing millions of small XML or JSON files. Wide column stores were invented to provide more control over performance and in particular, in order to achieve high-throughput and low latency for objects ranging from a few bytes to about 10 MB, which are too big and numerous to be efficiently stored as so-called clobs (character large objects) or blobs (binary large objects) in a relational database system, but also too small and numerous to be efficiently accessed in a distributed file system. </p>
<h4><span id="object-storage-vs-wide-column-stores">object storage VS Wide column stores</span></h4><p>a wide column store has additional benefits: a wide column store will be more tightly integrated with the parallel data processing systems.wide column stores have a richer logical model than the simple key-value model behind object storage; wide column stores also handle very small values (bytes and kBs) well thanks to batch processing.</p>
<h4><span id="hbase">HBase</span></h4><p>From an abstract perspective, HBase can be seen as an enhanced keyvalue store, in the sense that:a key is compound and involves a row, a column and a version; keys are sortable; values can be larger (clobs, blobs), up to around 10 MB. On the logical level, the data is organized in a tabular fashion: as a collection of rows. Each row is identified with a row ID. Row IDs can be compared, and the rows are logically sorted by row ID.  Column qualifiers are arrays of bytes (rather than strings), and as for row IDs, there is a library to easily create column qualifiers from primitive values. </p>
<p>HBase supports four kinds of low-level queries: get, put, scan and delete. Unlike a traditional key-value store, HBase also supports querying ranges of row IDs and ranges of timestamps.</p>
<p>HBase offers a locking mechanism at the row level, meaning that different rows can be modified concurrently, but the cells in the same row cannot: only one user at a time can modify any given row.</p>
<p>A table in HBase is physically partitioned in two ways: on the rows and on the columns. The rows are split in consecutive regions. Each region is identified by a lower and an upper row key, the lower row key being included and the upper row key excluded. A partition is called a store and corresponds to the intersection of a region and of a column family. </p>
<p>All the cells within a store are eventually persisted on HDFS, in files that we will call HFiles. An HFile is, in fact, nothing else than a (boring) flat list of KeyValues, one per cell. What is important is that, in an HFile, all these KeyValues are sorted by key in increasing order.</p>
<h4><span id="hdfs-block-and-hblock">HDFS block and HBlock</span></h4><p>HBase uses index structures to quickly skip to the position of the HBase block which may hold the requested key. Note HBase block is not to be confused with HDFS block and the underlying file system block.By default, each HBase block is 64KB (configurable) in size and always contains whole key-value pairs, so, if a block needs more than 64KB to avoid splitting a key-value pair, it will just grow.</p>
<h4><span id="log-structured-merge-trees">Log-structured merge trees</span></h4><p>Upon flushing, all cells are written sequentially to a new HFile in ascending key order, HBlock by HBlock, concurrently building the index structure. In fact, sorting is not done in the last minute when flushing. Rather, what happens is that when cells are added to memory, they are added inside a data structure that maintains them in sorted order (such as tree maps) and then flushing is a linear traversal of the tree.</p>
<h4><span id="bloom-filters">Bloom filters</span></h4><p>HBase has a mechanism to avoid looking for cells in every HFile. This mechanism is called a Bloom filter. It is basically a black box that can tell with absolute certainty that a certain key does not belong to an HFile, while it only predicts with good probability (albeit not certain) that it does belong to it.</p>
<h3><span id="exercise">exercise</span></h3><h4><span id="bloom-filters">Bloom filters</span></h4><p>Bloom filters are a data structure used to speed up queries, useful in the case in which it’s likely that the value we are looking doesn’t exist in the collection we are querying. Their main component is a bit array with all values initially set to 0. When a new element is inserted in the collection, its value is first run through a certain number of (fixed) hash functions, and the locations in the bit array corresponding to the outputs of these functions are set to 1.</p>
<p>This means that when we query for a certain value, if the value has previously been inserted in the collection then all the locations corresponding to the hash function outputs will certainly already have been set to 1. On the contrary, if the element hasn’t been previously inserted, then the locations may or may not have already been set to 1 by other elements. Then, if prior to accessing the collection we run our queried value through the hash functions, check the locations corresponding to the outputs, and find any of them to be 0, we are guaranteed that the element is not present in the collection (No False Negatives), and we don’t have to waste time looking. If the corresponding locations are all set to 1, the element may or may not be present in the collection (possibility of False Positives), but in the worst case we’re just wasting time. </p>
<p>As you have seen in the task above, HBase has to check all HFiles, along with the MemStore, when looking for a particular key. As an optimisation, Bloom filters are used to avoid checking an HFile if possible. Before looking inside a particular HFile, HBase first checks the requested key against the Bloom filter associated with that HFile. If it says that the key does not exist, the file is not read.</p>
<p>a Bloom filter can produce false positive outcomes. Luckily, it never produces false negative outcomes.</p>
<h4><span id="log-structured-merge-tree-lsm-tree-optional">Log-structured merge-tree (LSM tree) (optional)</span></h4><p>As opposed to B+-tree which has a time complexity of O(log n) when inserting new elements, n being the total number of elements in the tree, LSM tree has O(1) for inserting, which is a constant cost. </p>
<h2><span id="data-models-and-validation">Data models and validation</span></h2><h3><span id="book">book</span></h3><p>A data model is an abstract view over the data that hides the way it is stored physically. For example, a CSV file should be abstracted logically as a table.</p>
<h4><span id="the-json-information-set">The JSON Information Set</span></h4><p>it is possible to take a tree and output it back to JSON syntax. This is called serialization.</p>
<h4><span id="the-xml-information-set">The XML Information Set</span></h4><p>A fundamental difference between JSON trees and XML trees is that for JSON, the labels (object keys) are on the edges connecting an object information item to each one of its children information items. In XML, the labels (these would be element and attribute names) are on the nodes (information items) directly.</p>
<h4><span id="item-types">Item types</span></h4><p>Also, all atomic types have in common that they have a logical value space and a lexical value space. Atomic types can be in a subtype relationship: a type is a subtype of another type if its logical value space is a subset of the latter.<br>However, in modern databases, it is customary to support unbounded integers.<br>Decimals correspond to real numbers that can be written as a finite sequence of digits in base 10, with an optional decimal period.Support for the entire decimal value space can be costly in performance. In order to address this issue, a floating-point standard (IEEE 754) was invented and is still very popular today.<br>Timestamp values are typically stored as longs (64-bit integers) expressing the number of milliseconds elapsed since January 1, 1970 by convention.<br>XML Schema, JSound and JSONiq follow the ISO 8601 standard.<br>The lexical representation of duration can vary, but there is a standard defined by ISO 8601 as well, starting with a P and prefixing sub-day parts with a T.<br>Maps (not be confused with records, which are similar) are maps from any atomic value to any value, i.e., generalize objects to keys that are not necessarily strings (e.g., numbers, dates, etc). However, unlike records, the type of the values must be the same for all keys.<br><img src="/2024/01/03/bigdata13/image-123.png" alt="Alt text"></p>
<h4><span id="jsound-and-json-schema">JSound and JSON Schema</span></h4><p>JSound is a schema language that was designed to be simple for 80% of the cases, making it particularly suitable in a teaching environment. It is independent of any programming language.<br>JSON Schema is another technology for validating JSON documents.<br>The type system of JSON Schema is thus less rich than that of JSound, but extra checks can be done with so-called formats, which include date, time, duration, email, and so on including generic regular expressions.<br>It is possible to require the presence of a key by adding an exclamation mark in JSound. in JSON Schema, which uses a “required” property associated with the list of required keys to express the same.<br>In the JSound compact syntax, extra keys are forbidden. Unlike JSound, in JSON Schema, extra properties are allowed by default. JSON Schema then allows to forbid extra properties with the “additionalProperties” property.<br>There are a few more features available in the compact JSound syntax (not in JSON Schema) with the special characters @, ? and =. The question mark (?) allows for null values (which are not the same as absent values). The arobase (@) indicates that one or more fields are primary keys for a list of objects that are members of the same array. The equal sign (=) is used to indicate a default value that is automatically populated if the value is absent.<br><img src="/2024/01/03/bigdata13/image-124.png" alt="Alt text"><br>Note that some values are quoted, which does not matter for validation: validation only checks whether lexical values are part of the type’s lexical space.</p>
<p>Accepting any values in JSound can be done with the type “item”, which contains all possible values. In JSON Schema, in order to declare a field to accept any values, you can use either true or an empty object in lieu of the type.</p>
<p>JSON Schema additionally allows to use false to forbid a field.</p>
<p>In JSON Schema, it is also possible to combine validation checks with Boolean combinations using “anyOf”.JSound schema allows defining unions of types with the vertical bar inside type strings. </p>
<p>In JSON Schema only (not in JSound), it is also possible to do a conjunction (logical and) with “allOf” as well as exclusive with “oneOf” as well as negation with “not”.</p>
<h4><span id="xml-schema">XML Schema</span></h4><p>all elements in an XML Schema are in a namespace, the XML Schema namespace. The namespace is prescribed by the XML Schema standard and must be this one. </p>
<h4><span id="dataframe">dataframe</span></h4><p>There is a particular subclass of semi-structured datasets that are very interesting: valid datasets, which are collections of JSON objects valid against a common schema, with some requirements on the considered schemas. The datasets belonging to this particular subclass are called data frames, or dataframes.<br>relational tables are data frames, while data frames are not necessarily relational tables: data frames can be (and are often) nested, but they are still relatively homogeneous to some extent.Thus, Data frames are a generalization of (normalized) relational tables allowing for (organized and structured) nestedness.</p>
<h4><span id="data-format">data format</span></h4><p>In fact, if the data is structured as a (valid) data frame, then there are many, many different formats that it can be stored in, and in a way that is much more efficient than JSON. These formats are highly optimized and typically stored in binary form, for example Parquet, Avro, Root, Google’s protocol buffers, etc.</p>
<p>Why is it possible to store the data more efficiently when it is valid and data-frame-friendly? One important reason is that the schema can be stored as a header in the binary format, and the data can be stored without repeating the fields in every record (as is done in textual JSON).</p>
<h3><span id="exercise">exercise</span></h3><h4><span id="dremel">Dremel</span></h4><p>Dremel is a query system developed at Google for deriving data stored in a nested data format such as XML, JSON, or Google Protocol Buffers into column storage, where it can be analyzed faster.</p>
<h1><span id="paper-notes">paper notes</span></h1><h2><span id="dynamo">Dynamo</span></h2><p>CAP: AP.<br><img src="/2024/01/03/bigdata13/image-125.png" alt="Alt text"></p>
<h3><span id="preference-list-and-coordinator">preference list and coordinator</span></h3><p>A node handling a read or write operation is known as the coordinator. Typically, this is the first among the top N nodes in the preference list.</p>
<h3><span id="quorum-like-system">quorum-like system</span></h3><p>To maintain consistency among its replicas, Dynamo uses a consistency protocol similar to those used in quorum systems. This protocol has two key configurable values: R and W. R is the minimum number of nodes that must participate in a successful read operation. W is the minimum number of nodes that must participate in a successful write operation. Setting R and W such that R + W &gt; N yields a quorum-like system.</p>
<p>To remedy this it does not enforce strict quorum membership and instead it uses a “sloppy quorum”; all read and write operations are performed on the first N healthy nodes from the preference list.</p>
<h3><span id="merkle-trees">Merkle Trees</span></h3><p>A hash tree or Merkle tree is a binary tree in which every leaf node gets as its label a data block and every non-leaf node is labelled with the cryptographic hash of the labels of its child nodes. Some KeyValue stores use Merkle trees for efficiently detecting inconsistencies in data between replicas.</p>
<h3><span id="vector-clock">vector clock</span></h3><h2><span id="dremel">Dremel</span></h2><p>In contrast to layers such as Pig19 and Hive,16 it executes queries natively without translating them into MR jobs. Lastly, and importantly, Dremel uses a column-striped storage representation, which enables it to read less data from secondary storage and reduce CPU cost due to cheaper compression.</p>
<h3><span id="repetition-and-definition-levels">Repetition and definition levels</span></h3><p>we define the repetition level as the number of repeated fields in the common prefix (including the first path element identifying the record). The definition level specifies the number of optional and repeated fields in the path (excluding the first path element).A definition level smaller than the maximal number of repeated and optional fields in a path denotes a NULL.</p>
<h2><span id="hdfs">HDFS</span></h2><p>Each block replica on a DataNode is represented by two files in the local host’s native file system. The first file contains the data itself and the second file is block’s metadata including checksums for the block data and the block’s generation stamp.</p>
<p>If the NameNode does not receive a heartbeat from a DataNode in ten minutes the NameNode considers the DataNode to be out of service and the block replicas hosted by that DataNode to be unavailable. The NameNode then schedules creation of new replicas of those blocks on other DataNodes. Heartbeats from a DataNode also carry information about total storage capacity, fraction of storage in use, and the number of data transfers currently in progress. These statistics are used for the NameNode’s space allocation and load balancing decisions. The NameNode does not directly call DataNodes. It uses replies to heartbeats to send instructions to the DataNodes.</p>
<p>A recently introduced feature of HDFS is the BackupNode. Like a CheckpointNode, the BackupNode is capable of creating periodic checkpoints, but in addition it maintains an inmemory, up-to-date image of the file system namespace that is always synchronized with the state of the NameNode.</p>
<p>A replica stored on a DataNode may become corrupted because of faults in memory, disk, or network. HDFS generates and stores checksums for each data block of an HDFS file. Checksums are verified by the HDFS client while reading to help detect any corruption caused either by client, DataNodes, or network. When a client creates an HDFS file, it computes the checksum sequence for each block and sends it to a DataNode along with the data. A DataNode stores checksums in a metadata file separate from the block’s data file. When HDFS reads a file, each block’s data and checksums are shipped to the client. The client computes the checksum for the received data and verifies that the newly computed checksums matches the checksums it received. If not, the client notifies the NameNode of the corrupt replica and then fetches a different replica of the block from another DataNode.</p>
<p>The design of HDFS I/O is particularly optimized for batch processing systems, like MapReduce, which require high throughput for sequential reads and writes.</p>
<p>Currently HDFS provides a configurable block placement policy interface so that the users and researchers can experiment and test any policy that’s optimal for their applications.</p>
<p>When a block becomes over replicated, the NameNode chooses a replica to remove. The NameNode will prefer not to reduce the number of racks that host replicas, and secondly prefer to remove a replica from the DataNode with the least amount of available disk space. When a block becomes under-replicated, it is put in the replication priority queue.  A background thread periodically scans the head of the replication queue to decide where to place new replicas.</p>
<p>HDFS block placement strategy does not take into account DataNode disk space utilization. This is to avoid placing new—more likely to be referenced—data at a small subset of the DataNodes. </p>
<p>Each DataNode runs a block scanner that periodically scans its block replicas and verifies that stored checksums match the block data.  Whenever a read client or a block scanner detects a corrupt block, it notifies the NameNode. The NameNode marks the replica as corrupt, but does not schedule deletion of the replica immediately. Instead, it starts to replicate a good copy of the block. Only when the good replica count reaches the replication factor of the block the corrupt replica is scheduled to be removed. So even if all replicas of a block are corrupt, the policy allows the user to retrieve its data from the corrupt replicas.</p>
<p>A present member of the cluster that becomes excluded is marked for decommissioning. Once a DataNode is marked as decommissioning, it will not be selected as the target of replica placement, but it will continue to serve read requests. The NameNode starts to schedule replication of its blocks to other DataNodes. Once the NameNode detects that all blocks on the decommissioning DataNode are replicated, the node enters the decommissioned state. Then it can be safely removed from the cluster without jeopardizing any data availability.</p>
<h2><span id="mapreduce">MapReduce</span></h2><p>The master pings every worker periodically.If no response isrecei ved from awork er in acertain amount of time, the master marks the worker as failed.</p>
<h2><span id="json">json</span></h2><p>JSON stands for JavaScript Object Notation and was inspired by the object literals of JavaScript.</p>
<p>A JSON value can be an object, array, number, string, true, false, or null.</p>
<p>The JSON syntax does not impose any restrictions on the strings used as names, does not require that name strings be unique, and does not assign any significance to the ordering of name/value pairs.</p>
<p>Numeric values that cannot be represented as sequences of digits (such as Infinity and NaN) are not permitted.</p>
<p>All strings in JSON must be double-quoted.</p>
<h2><span id="rumble">rumble</span></h2><p>a query execution engine for large, heterogeneous, and nested collections of JSON objects built on top of Apache Spark.</p>
<h2><span id="xml">xml</span></h2><p>XML, unlike HTML, is case-sensitive. <person> is not the same as <person> or <person>.</person></person></person></p>
<p>Every well-formed XML document has exactly one root element. </p>
<p>XML elements can have attributes. An attribute is a name-value pair attached to the element’s start-tag. Names are separated from values by an equals sign and optional whitespace. Values are enclosed in single or double quotation marks. </p>
<p>each element may have no more than one attribute with a given name.</p>
<p>Element and other XML names may contain essentially any alphanumeric character. This includes the standard English letters A through Z and a through z as well as the digits 0 through 9. They may also include these three punctuation characters: _ The underscore ,- The hyphen, . The period.Finally, all names beginning with the string “XML” (in any combination of case) are reserved for standardization in W3C XML-related specifications. XML names may only start with letters, ideograms, or the underscore character. They may not start with a number, hyphen, or period. </p>
<h1><span id="exam-notes">exam notes</span></h1><h2><span id="2022">2022</span></h2><p>HTTP command and status code.<br>Storage type choosing.</p>
<p>HDFS and random access:<br>For distributed data storage though, and for the use case at hand where we read a large dataset, analyze it, and write back the output as a new dataset, random access is not needed. A distributed file system is designed so that, in cruise mode, its bottleneck will be the data flow (throughput), not the latency. This aspect of the design is directly consistent with a full-scan pattern, rather than with a random access pattern, the latter being strongly latency-bound.</p>
<p>json and comments:In JSON (JavaScript Object Notation), comments are not officially supported. </p>
<p>Hbase and meta table.<br>if empty json valid against to some schema.<br>mapreduce split.<br>mapreduce function: emit.<br>if reduce function can be used as combine function.</p>
<p>1NF,2NF and 3NF:<br>A candidate key is a minimal set of attributes that determines the other attributes included in the relation. A non-prime attribute is an attribute that is not part of the candidate key.Informally, the second normal form states that all attributes must depend on the entire candidate key.In other words, non-prime attributes must be functionally dependent on the key(s), but they must not depend on another non-prime attribute. 3NF non-prime attributes depend on “nothing but the key”.</p>
<p>different type comparision in mongodb.</p>
<p>If you have n dimensions, the CUBE operation will generate 2^n combinations.</p>
<p>xml schema:<br>You have already seen the xs:sequence element, which dictates that the elements it contains must appear in exactly the same order in which they appear within the sequence element.</p>
<p>mongdb atomic operations:<br>In MongoDB, a write operation is atomic on the level of a single document, even if the operation modifies multiple embedded documents within a single document.<br>When a single write operation (e.g. db.collection.updateMany()) modifies multiple documents, the modification of each document is atomic, but the operation as a whole is not atomic.</p>
<p>Json schema:<br>tuple validation. </p>
<h2><span id="references">references</span></h2><p><a target="_blank" rel="noopener" href="https://vertabelo.com/blog/normalization-1nf-2nf-3nf/">https://vertabelo.com/blog/normalization-1nf-2nf-3nf/</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-12-12T14:22:00.000Z" title="2023-12-12 3:22:00 ├F10: PM┤">2023-12-12</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-12-20T13:20:09.141Z" title="2023-12-20 2:20:09 ├F10: PM┤">2023-12-20</time></span><span class="level-item"><a class="link-muted" href="/categories/database/">database</a></span><span class="level-item">3 minutes read (About 454 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/12/12/bigdata12/">bigdata - Cube Data</a></p><div class="content"><h1><span id="on-line-analytic-processingolap">On-Line Analytic Processing(OLAP)</span></h1><p>On-Line Analytic Processing generally involves highly complex queries that use one or more aggregations.</p>
<h2><span id="olap-and-data-warehouses">OLAP and Data Warehouses</span></h2><p>Data from many separate databases may be integrated into the warehouse. The warehouse is usually only updated overnight. Data warehouses play an important role in OLAP applications. First, the warehouse is necessary to organize and centralize data in a way that supports OLAP queries. Second, OLAP queries are usually complex and touching much of the data and take too much time to be executed in a transaction-processing system with high throughput requirements.</p>
<h2><span id="a-multidimensional-view-of-olap-data">A Multidimensional View of OLAP Data</span></h2><p>In typical OLAP applications there is a central relation or collection of data called the fact table. Often, it helps to think of the objects in the fact table as arranged in a multidimensional space. Two broad directions that have been taken by specialized systems that support cube-structured data for OLAP: ROLAP and MOLAP. ROLAP, which is Relational OLAP. In this approach, data may be stored in relations with a specialized structure called a “star schema”. MOLAP, which is Multidimensional OLAP. A specialized structure “data cude” is used to hold the data, including its aggregates.</p>
<h2><span id="star-schemas">Star Schemas</span></h2><p>A star schema consists of the schema for the fact table, which links to several other relations, called “dimension tables”. </p>
<h2><span id="slicing-and-dicing">Slicing and Dicing</span></h2><p>A choice of partition for each dimension “dices” the cude. The result is that the cude is divided into smaller cubes that represent groups of points whose statistics are aggregated by a query that performs this partitioning in its “group by” clause. Through the “where” clause, a query has the option of focusing on particular partitions alone one or more dimensions.(on a particular “slice” of the cube).<br><img src="/2023/12/12/bigdata12/image-82.png" alt="Alt text"></p>
<p><img src="/2023/12/12/bigdata12/image-83.png" alt="Alt text"></p>
<p><img src="/2023/12/12/bigdata12/image-84.png" alt="Alt text"></p>
<p><img src="/2023/12/12/bigdata12/image-85.png" alt="Alt text"></p>
<h1><span id="data-cubes">Data Cubes</span></h1><p>The formal data cube precomputes all possible aggregates in a systematic way. </p>
<h2><span id="the-cube-operator">The Cube Operator</span></h2><p>Given a fact table F, we can define an augmented table CUBE(F) that adds an additional value, denoted <em>, to each dimension. The </em> has the intuitive meaning “any,” and it represents aggregation along the dimension in which it appears. </p>
<h2><span id="the-cube-operator-in-sql">The Cube Operator in SQL</span></h2><p>SQL gives us a way to apply the cube operator within queries. If we add the term “WITH CUBE” to a group-by clause, then we get not only the tuple for each group, but also the tuples that represent aggregation along one or more of the dimensions along which we have grouped. These tuples appear in the result with NULL where we have used *.<br><img src="/2023/12/12/bigdata12/image-86.png" alt="Alt text"></p>
<p><img src="/2023/12/12/bigdata12/image-87.png" alt="Alt text"></p>
<p>However, SalesRollup would not contain tuples such as<br><img src="/2023/12/12/bigdata12/image-88.png" alt="Alt text"></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-12-12T14:21:07.000Z" title="2023-12-12 3:21:07 ├F10: PM┤">2023-12-12</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-12-20T14:12:57.777Z" title="2023-12-20 3:12:57 ├F10: PM┤">2023-12-20</time></span><span class="level-item"><a class="link-muted" href="/categories/database/">database</a></span><span class="level-item">12 minutes read (About 1783 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/12/12/bigdata11/">bigdata - Graph Database</a></p><div class="content"><h2><span id="why-graphs">Why graphs</span></h2><p>Now, we do know a way to avoid joins and studied it at length: denormalizing the data to (homogeneous) collections of trees is a way of “pre-computing” the joins statically, so that the data is already joined (via nesting) at runtime.</p>
<p>Now, why is it efficient? Because doing down a tree only necessitates following pointers in memory. But trees cannot have cycles. Graph databases provide a way of generalizing the use in-memory pointers to traverse data to the general case in which cycles are present: this is called “index-free adjacency.”</p>
<h2><span id="kinds-of-graph-databases">Kinds of graph databases</span></h2><p>There are many different graph database system products on the market, and they can be classified along several dimensions: Labeled property graph model vs. triple stores;Read-intensive vs. write-intensive;Local vs. distributed;Native vs. non-native. </p>
<h2><span id="graph-data-models">Graph data models</span></h2><h3><span id="labeled-property-graphs">Labeled property graphs</span></h3><p>Computer scientists need to go one step further and also design how to store graphs physically. One way of doing so is to create an associative array mapping each node to the list of nodes that it connects to via an edge (adjacency lists). Another storage form is with an adjacency matrix: each row and each column represent a node, and a 0 or a 1 indicate the absence or presence of an edge between the row node and the column node. </p>
<p>Now, this does not quite work for us, because labeled property graphs enhance mathematical graphs with extra ingredients: properties, and labels.<br>how to “convert” a relational table to a labeled property graph: labels can be seen as table names, nodes as records, and properties as the attribute values for the records. This shows that relational tables can be physically stored as labeled property graphs. Of course, this does not work the other way round: given a graph, it will often not be possible to convert it “back” to a table in this specific way.</p>
<h3><span id="triple-stores">Triple stores</span></h3><p>Triple stores are a different and simpler model. It views the graph as nodes and edges that all have labels, but without any properties. The graph is then represented as a list of edges, where each edge is a triple with the label of the origin node (called the subject), the label of the edge (called the property), and the label of the destination node (called the object).</p>
<p>Triple stores typically provide SPARQL capabilities to reason about and stored RDF data.</p>
<h2><span id="querying-graph-data">Querying graph data</span></h2><p>We will now have a look at query languages for querying graphs, with a focus on Cypher, which is neo4j’s query language.<br><img src="/2023/12/12/bigdata11/image-89.png" alt="Alt text"></p>
<h3><span id="cypher-philosophy">Cypher Philosophy</span></h3><p>Cypher enables a user (or an application acting on behalf of a user) to ask the database to find data that matches a specific pattern. Colloquially, we ask the database to “find things like this.” And the way we describe what “things like this” look like is to draw them, using ASCII art.</p>
<p>Like most query languages, Cypher is composed of clauses. The simplest queries consist of a MATCH clause followed by a RETURN clause. There are other clauses we can use in a Cypher query: WHERE,WITH…AS…,CREATE,MERGE,DELETE,SET,UNION,FORWACH and so on. </p>
<h3><span id="a-comparison-of-relational-and-graph-modeling">A Comparison of Relational and Graph Modeling</span></h3><p>Relational databases—with their rigid schemas and complex modeling characteristics—are not an especially good tool for supporting rapid change. What we need is a model that is closely aligned with the domain, but that doesn’t sacrifice performance, and that supports evolution while maintaining the integrity of the data as it undergoes rapid change and growth. That model is the graph model.</p>
<h3><span id="creating-a-graph">creating a graph</span></h3><p>In practice, we tend to use CREATE when we’re adding to the graph and don’t mind duplication, and MERGE when duplication is not permitted by the domain.</p>
<h3><span id="beginning-a-query">Beginning a Query</span></h3><p>In Cypher we always begin our queries from one or more well-known starting points in the graph—what are called bound nodes. Cypher uses any labels and property predicates supplied in the MATCH and WHERE clauses, together with metadata supplied by indexes and constraints, to find the starting points that anchor our graph patterns.</p>
<h3><span id="indexes-and-constraints">INDEXES AND CONSTRAINTS</span></h3><p>To support efficient node lookup, Cypher allows us to create indexes per label and property combinations. For unique property values we can also specify constraints that assure uniqueness.</p>
<h1><span id="neo4j">Neo4j</span></h1><p> Neo4j is a graph database with native processing capabilities as well as native graph storage.</p>
<h2><span id="native-graph-processing">Native Graph Processing</span></h2><p>Of the many different engine architectures, we say that a graph database has native processing capabilities if it exhibits a property called index-free adjacency.</p>
<p>A database engine that utilizes index-free adjacency is one in which each node maintains direct references to its adjacent nodes. Each node, therefore, acts as a micro-index of other nearby nodes, which is much cheaper than using global indexes. It means that query times are independent of the total size of the graph, and are instead simply proportional to the amount of the graph searched. A nonnative graph database engine, in contrast, uses (global) indexes to link nodes together.</p>
<p>Proponents for native graph processing argue that index-free adjacency is crucial for fast, efficient graph traversals. To understand why native graph processing is so much more efficient than graphs based on heavy indexing, consider the following. Depending on the implementation, index lookups could be O(log n) in algorithmic complexity versus O(1) for looking up immediate relationships. To traverse a network of m steps, the cost of the indexed approach, at O(m log n), dwarfs the cost of O(m) for an implementation that uses index-free adjacency.</p>
<h2><span id="native-graph-storage">Native Graph Storage</span></h2><p>Neo4j stores graph data in a number of different store files. Each store file contains the data for a specific part of the graph (e.g., there are separate stores for nodes, relationships, labels, and properties).The division of storage responsibilities—particularly the separation of graph structure from property data—facilitates performant graph traversals, even though it means the user’s view of their graph and the actual records on disk are structurally dissimilar.</p>
<p>Like most of the Neo4j store files, the node store is a fixed-size record store, where each record is nine bytes in length. Fixed-size records enable fast lookups for nodes in the store file. If we have a node with id 100, then we know its record begins 900 bytes into the file. Based on this format, the database can directly compute a record’s location, at cost O(1), rather than performing a search, which would be cost O(log n).</p>
<h2><span id="transactions">Transactions</span></h2><p>Transactions in Neo4j are semantically identical to traditional database transactions. Writes occur within a transaction context, with write locks being taken for consistency purposes on any nodes and relationships involved in the transaction. On successful completion of the transaction, changes are flushed to disk for durability, and the write locks released. These actions maintain the atomicity guarantees of the transaction. </p>
<h3><span id="core-api-traversal-framework-or-cypher">CORE API, TRAVERSAL FRAMEWORK, OR CYPHER?</span></h3><p>The Core API allows developers to fine-tune their queries so that they exhibit high affinity with the underlying graph. A well-written Core API query is often faster than any other approach. The downside is that such queries can be verbose, requiring considerable developer effort. Moreover, their high affinity with the underlying graph makes them tightly coupled to its structure. When the graph structure changes, they can often break. Cypher can be more tolerant of structural changes—things such as variable-length paths help mitigate variation and change.</p>
<p>The Traversal Framework is both more loosely coupled than the Core API (because it allows the developer to declare informational goals), and less verbose, and as a result a query written using the Traversal Framework typically requires less developer effort than the equivalent written using the Core API. Because it is a general-purpose framework, however, the Traversal Framework tends to perform marginally less well than a well-written Core API query.</p>
<h2><span id="exercise">exercise</span></h2><p>Neo4j system design is different from mongodb on the consistency. MongoDB is eventually consistent, but neo4j is strong consistency and obey the ACID rules.</p>
<p>Cypher: note that label and property are case sensitive but the clause is not case sensitive.</p>
<p>RDF: Turtle Syntax </p>
<p>Mannual grouping and grouping sets: need to know how to translate between them.<br>grouping with rollup: rollup(c1,c2,c3) is equal to grouping sets((c1,c2,c3),(c1,c2),(c1),()). The ordering is important.</p>
<p>difference between neo4j and RDF:<br>Data Model:<br>Neo4j: Neo4j is a graph database that uses the property graph data model. In this model, nodes represent entities, relationships represent connections between entities, and both nodes and relationships can have key-value pairs as properties.<br>RDF: RDF is a data model for representing knowledge in the form of subject-predicate-object triples. Each triple represents a statement, and these triples can be used to build a graph of linked data. RDF is more abstract and can be implemented using various storage formats.</p>
<p>Query Language:<br>Neo4j: Neo4j uses the Cypher query language, which is specifically designed for querying graph databases. Cypher allows users to express graph patterns and relationships in a concise and readable manner.<br>RDF: RDF data is typically queried using SPARQL (SPARQL Protocol and RDF Query Language). SPARQL is a query language for querying and manipulating RDF data, and it provides powerful capabilities for navigating the graph structure.</p>
<p>Graph Structure:<br>Neo4j: In Neo4j, the graph is explicit, with nodes, relationships, and properties forming a connected graph structure. The focus is on relationships between entities and their properties.<br>RDF: RDF represents a graph, but the graph structure is more implicit. Resources are identified by URIs, and relationships are expressed through triples, allowing for the creation of a distributed and linked data web.</p>
<p>Schema:<br>Neo4j: Neo4j supports a flexible schema where nodes and relationships can have dynamic properties. While it provides some level of schema flexibility, users can define constraints and indexes to enforce certain structures.<br>RDF: RDF is schema-agnostic, allowing for more dynamic and extensible data representation. Schemas can be defined using RDF vocabularies and ontologies, such as RDFS and OWL.</p>
<p>Use Cases:<br>Neo4j: Neo4j is often used for applications where relationships and graph structures are central, such as social networks, recommendation engines, and network analysis.<br>RDF: RDF is commonly used in the context of the Semantic Web for representing and linking diverse data sources, allowing for interoperability and knowledge representation.</p>
<h2><span id="references">references</span></h2><p>Robinson, I. et al. (2015). Graph Databases (2nd ed.)</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-12-07T14:26:41.000Z" title="2023-12-7 3:26:41 ├F10: PM┤">2023-12-07</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-12-14T19:56:26.614Z" title="2023-12-14 8:56:26 ├F10: PM┤">2023-12-14</time></span><span class="level-item"><a class="link-muted" href="/categories/theory/">theory</a></span><span class="level-item">a few seconds read (About 81 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/12/07/pai10/">pai - Model-based Approximate Reinforcement Learning</a></p><div class="content"><h1><span id="model-based-reinforcement-learning">model-based reinforcement learning</span></h1><p>We face three main challenges in model-based reinforcement learning. First, given a fixed model, we need to perform planning to decide on which actions to play. Second, we need to learn models f and r accurately and efficiently. Third, we need to effectively trade exploration and exploitation.</p>
<h2><span id="planning">Planning</span></h2><h3><span id="deterministic-dynamics">Deterministic Dynamics</span></h3><p>To begin with, let us assume that our dynamics model is deterministic and known. That is, given a state-action pair, we know the subsequent state.</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-12-07T14:26:12.000Z" title="2023-12-7 3:26:12 ├F10: PM┤">2023-12-07</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-12-17T10:21:48.974Z" title="2023-12-17 11:21:48 ├F10: AM┤">2023-12-17</time></span><span class="level-item"><a class="link-muted" href="/categories/theory/">theory</a></span><span class="level-item">10 minutes read (About 1539 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/12/07/pai9/">pai - Model-free Approximate Reinforcement Learning</a></p><div class="content"><h1><span id="model-free-approximate-reinforcement-learning">Model-free Approximate Reinforcement Learning</span></h1><h2><span id="tabular-reinforcement-learning-as-optimization">Tabular Reinforcement Learning as Optimization</span></h2><p>In particular, in the tabular setting (i.e., over a discrete domain), we can parameterize the value function exactly by learning a separate parameter for each state.<br><img src="/2023/12/07/pai9/image-77.png" alt="Alt text"></p>
<p><img src="/2023/12/07/pai9/image-78.png" alt="Alt text"></p>
<p>Now, we cannot compute this derivative because we cannot compute the expectation. Firstly, the expectation is over the true value function which is unknown to us. Secondly, the expectation is over the transition model which we are trying to avoid in model-free methods. To resolve the first issue, analogously to TD-learning, instead of learning the true value function vπ which is unknown, we learn the bootstrapping estimate Vπ. To resolve the first issue, analogously to TD-learning, instead of learning the true value function vπ which is unknown, we learn the bootstrapping estimate Vπ. we will use a Monte Carlo estimate using a single sample. Recall that this is only possible because the transitions are conditionally independent given the state-action pair. </p>
<p><img src="/2023/12/07/pai9/image-79.png" alt="Alt text"></p>
<p><img src="/2023/12/07/pai9/image-80.png" alt="Alt text"></p>
<p>Therefore, TD-learning is essentially performing stochastic gradient descent using the TD-error as an unbiased gradient estimate.<br>Stochastic gradient descent with a bootstrapping estimate is also called stochastic semi-gradient descent.</p>
<h2><span id="value-function-approximation">Value Function Approximation</span></h2><p>Our goal for large state-action spaces is to exploit the smoothness properties5 of the value function to condense the representation. </p>
<h3><span id="heuristics">Heuristics</span></h3><p>The vanilla stochastic semi-gradient descent is very slow.<br>There are mainly two problems.<br>As we are trying to learn an approximate value function that depends on the bootstrapping estimate, this means that the optimization target is “moving” between iterations. In practice, moving targets lead to stability issues. One such technique aiming to “stabilize” the optimization targets is called neural fitted Q-iteration or deep Q-networks (DQN). DQN updates the neural network used for the approximate bootstrapping estimate infrequently to maintain a constant optimization target across multiple episodes. One approach is to clone the neural network and maintain one changing neural network (“online network”) for the most recent estimate of the Q-function which is parameterized by θ, and one fixed neural network (“target network”) used as the target which is parameterized by θold and which is updated infrequently.</p>
<p><img src="/2023/12/07/pai9/image-90.png" alt="Alt text"></p>
<p>This technique is known as experience replay. Another approach is Polyak averaging where the target network is gradually “nudged” by the neural network used to estimate the Q function.</p>
<p>Now, observe that the estimates Q⋆ are noisy estimates of q⋆. The fact that the update rules can be affected by inaccuracies (i.e., noise in the estimates) of the learned Q-function is known as the “maximization bias”. Double DQN (DDQN) is an algorithm that addresses this maximization bias. Instead of picking the optimal action with respect to the old network, it picks the optimal action with respect to the new network. Intuitively, this change ensures that the evaluation of the target network is consistent with the updated Q-function, which makes it more difficult for the algorithm to be affected by noise. </p>
<p><img src="/2023/12/07/pai9/image-91.png" alt="Alt text"></p>
<h2><span id="policy-approximation">Policy Approximation</span></h2><p>Methods that find an approximate policy are also called policy search methods or policy gradient methods. Policy gradient methods use randomized policies for encouraging exploration.</p>
<h3><span id="estimating-policy-values">Estimating Policy Values</span></h3><p><img src="/2023/12/07/pai9/image-92.png" alt="Alt text"><br>The policy value function measures the expected discounted payoff of policy π. </p>
<p><img src="/2023/12/07/pai9/image-93.png" alt="Alt text"><br><img src="/2023/12/07/pai9/image-94.png" alt="Alt text"></p>
<h3><span id="reinforce-gradient">Reinforce Gradient</span></h3><p><img src="/2023/12/07/pai9/image-95.png" alt="Alt text"></p>
<p><img src="/2023/12/07/pai9/image-96.png" alt="Alt text"><br>In this context, however, we cannot apply the reparameterization trick. Fortunately, there is another way of estimating this gradient.</p>
<p><img src="/2023/12/07/pai9/image-97.png" alt="Alt text"></p>
<p><img src="/2023/12/07/pai9/image-99.png" alt="Alt text"></p>
<p>When using a neural network for the parameterization of the policy π, we can use automatic differentiation to obtain unbiased gradient estimates. However, it turns out that the variance of these estimates is very large. Using so-called baselines can reduce the variance dramatically.</p>
<p>The baseline essentially captures the expected or average value, providing a reference point. By subtracting this reference point, the updates become more focused on the deviations from the expected values, which can reduce the variance in these deviations.</p>
<p><img src="/2023/12/07/pai9/image-98.png" alt="Alt text"></p>
<p><img src="/2023/12/07/pai9/image-100.png" alt="Alt text"></p>
<p><img src="/2023/12/07/pai9/image-101.png" alt="Alt text"></p>
<p><img src="/2023/12/07/pai9/image-102.png" alt="Alt text"></p>
<p>Typically, policy gradient methods are slow due to the large variance in the score gradient estimates. Because of this, they need to take small steps and require many rollouts of a Markov chain. Moreover, we cannot reuse data from previous rollouts, as policy gradient methods are fundamentally on-policy.</p>
<h2><span id="actor-critic-methods">Actor-Critic Methods</span></h2><p>Actor-Critic methods reduce the variance of policy gradient estimates by using ideas from value function approximation. They use function approximation both to approximate value functions and to approximate policies.</p>
<h3><span id="advantage-function">Advantage Function</span></h3><p><img src="/2023/12/07/pai9/image-103.png" alt="Alt text"></p>
<p>Intuitively, the advantage function is a shifted version of the state-action function q that is relative to 0. It turns out that using this quantity instead, has numerical advantages.</p>
<h3><span id="policy-gradient-theorem">Policy Gradient Theorem</span></h3><p><img src="/2023/12/07/pai9/image-105.png" alt="Alt text"></p>
<p><img src="/2023/12/07/pai9/image-104.png" alt="Alt text"></p>
<p>Intuitively, ρθ(x) measures how often we visit state x when following policy πθ. It can be thought of as a “discounted frequency”. Importantly, ρθ is not a probability distribution, as it is not normalized to integrate to 1. Instead, ρθ is what is often called a finite measure. Therefore, eq. (12.57) is not a real expectation!</p>
<h3><span id="on-policy-actor-critics">On-policy Actor-Critics</span></h3><p><img src="/2023/12/07/pai9/image-106.png" alt="Alt text"></p>
<h4><span id="oac">OAC</span></h4><p><img src="/2023/12/07/pai9/image-107.png" alt="Alt text"><br>Due to the use of TD-learning for learning the critic, this algorithm is fundamentally on-policy. </p>
<h4><span id="a2c">A2C</span></h4><p><img src="/2023/12/07/pai9/image-108.png" alt="Alt text"></p>
<p>that the Q-function is an absolute quantity, whereas the advantage function is a relative quantity, where the sign is informative for the gradient direction. Intuitively, an absolute value is harder to estimate than the sign. Actor-Critic methods are therefore often implemented with respect to the advantage function rather than the Q-function. </p>
<h4><span id="gaegaae">GAE/GAAE</span></h4><p>Taking a step back, observe that the policy gradient methods such as REINFORCE generally have high variance in their gradient estimates. However, due to using Monte Carlo estimates of Gt, the gradient estimates are unbiased. In contrast, using a bootstrapped Q-function to obtain gradient estimates yields estimates with a smaller variance, but those estimates are biased. We are therefore faced with a bias-variance tradeoff. A natural approach is therefore to blend both gradient estimates to allow for effectively trading bias and variance. This leads to algorithms such as generalized advantage estimation (GAE/GAAE). </p>
<h4><span id="improving-sample-efficiencytrpoppo">Improving sample efficiency(TRPO/PPO)</span></h4><p>Actor-Critic methods generally suffer from low sample efficiency. One well-known variant that slightly improves the sample efficiency is trust-region policy optimization (TRPO). </p>
<p><img src="/2023/12/07/pai9/image-109.png" alt="Alt text"><br>Intuitively, taking the expectation with respect to the previous policy πθk , means that we can reuse data from rollouts within the same iteration. TRPO allows reusing past data as long as it can still be “trusted”. This makes TRPO “somewhat” off-policy. Fundamentally, though, TRPO is still an on-policy method. Proximal policy optimization (PPO) is a heuristic variant of TRPO that often works well in practice.</p>
<h3><span id="off-policy-actor-critics">Off-policy Actor-Critics</span></h3><p>These algorithms use the reparameterization gradient estimates, instead of score gradient estimators. </p>
<h4><span id="ddpg">DDPG</span></h4><p>As our method is off-policy, a simple idea in continuous action spaces is to add Gaussian noise to the action selected by πθ — also known as Gaussian noise “dithering”. This corresponds to an algorithm called deep deterministic policy gradients.<br>This algorithm is essentially equivalent to Q-learning with function approximation (e.g., DQN), with the only exception that we replace the maximization over actions with the learned policy πθ.<br><img src="/2023/12/07/pai9/image-110.png" alt="Alt text"></p>
<p>Twin delayed DDPG (TD3) is an extension of DDPG that uses two separate critic networks for predicting the maximum action and evaluating the policy. This addresses the maximization bias akin to Double-DQN. TD3 also applies delayed updates to the actor network, which increases stability.</p>
<h3><span id="off-policy-actor-critics-with-randomized-policies">Off-Policy Actor Critics with Randomized Policies</span></h3><p><img src="/2023/12/07/pai9/image-111.png" alt="Alt text"></p>
<p>The algorithm that uses eq. (12.81) to obtain gradients for the critic and reparameterization gradients for the actor is called stochastic value gradients (SVG).</p>
<h3><span id="off-policy-actor-critics-with-entropy-regularization">Off-policy Actor-Critics with Entropy Regularization</span></h3><p>In practice, algorithms like SVG often do not explore enough. A key issue with relying on randomized policies for exploration is that they might collapse to deterministic policies. </p>
<p>A simple trick that encourages a little bit of extra exploration is to regularize the randomized policies “away” from putting all mass on a single action. This approach is known as entropy regularization and it leads to an analogue of Markov decision processes called entropy-regularized Markov decision process, where suitably defined regularized state-action value functions (so-called soft value functions) are used.</p>
<h4><span id="soft-actor-criticsac">soft actor critic(SAC)</span></h4><p><img src="/2023/12/07/pai9/image-81.png" alt="Alt text"></p>
<h2><span id="references">references</span></h2><p><a target="_blank" rel="noopener" href="https://towardsdatascience.com/soft-actor-critic-demystified-b8427df61665">https://towardsdatascience.com/soft-actor-critic-demystified-b8427df61665</a><br><a target="_blank" rel="noopener" href="https://github.com/tsmatz/reinforcement-learning-tutorials/blob/master/06-sac.ipynb">https://github.com/tsmatz/reinforcement-learning-tutorials/blob/master/06-sac.ipynb</a><br><a target="_blank" rel="noopener" href="https://spinningup.openai.com/en/latest/algorithms/sac.html">https://spinningup.openai.com/en/latest/algorithms/sac.html</a><br><a target="_blank" rel="noopener" href="https://towardsdatascience.com/policy-gradients-in-a-nutshell-8b72f9743c5d">https://towardsdatascience.com/policy-gradients-in-a-nutshell-8b72f9743c5d</a><br><a target="_blank" rel="noopener" href="https://lilianweng.github.io/posts/2018-04-08-policy-gradient/">https://lilianweng.github.io/posts/2018-04-08-policy-gradient/</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-11-28T20:58:26.000Z" title="2023-11-28 9:58:26 ├F10: PM┤">2023-11-28</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-12-12T15:35:16.899Z" title="2023-12-12 4:35:16 ├F10: PM┤">2023-12-12</time></span><span class="level-item"><a class="link-muted" href="/categories/database/">database</a></span><span class="level-item">17 minutes read (About 2564 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/11/28/bigdata10/">bigdata - JSONiq</a></p><div class="content"><h1><span id="querying-denormalized-data">Querying denormalized data</span></h1><h2><span id="imperative-vs-declarative">imperative vs declarative</span></h2><p>Imperative programming is a programming paradigm that expresses computation as a series of statements that change a program’s state. In imperative programming, the focus is on describing how a program operates step by step. Common imperative languages include C, C++, Java, and Python.</p>
<p>In contrast to imperative programming, declarative programming focuses on describing what the program should accomplish rather than how to achieve it. In a declarative host language, the emphasis is on specifying the desired outcome or properties, and the language itself takes care of the underlying implementation details. Such as SQL, HTML.</p>
<h2><span id="motivation">motivation</span></h2><h3><span id="denormalized-data">Denormalized data</span></h3><p>it is characterized with two features: nestedness, and heterogeneity. In fact, denormalized datasets should not be seen as “broken tables pushed to their limits”, but rather as collections of trees. </p>
<h3><span id="features-of-a-query-language">Features of a query language</span></h3><p>A query language for datasets has three main features: Declarative, Functional, and Set-based. First, it is declarative. This means that the users do not focus on how the query is computed, but on what it should return. Being functional means that the query language is made of composable expressions that nest with each other, like a Lego game. Finally, it is set-based, in the sense that the values taken and returned by expressions are not only single values (scalars), but are large sequences of items (in the case of SQL, an item is a row).</p>
<h3><span id="query-languages-for-denormalized-data">Query languages for denormalized data</span></h3><p>For denormalized data though, sadly, the number of languages keeps increasing: the oldest ones being XQuery, JSONiq, but then now also JMESPath, SpahQL, JSON Query, PartiQL, UnQL, N1QL, ObjectPath, JSONPath, ArangoDB Query Language (AQL), SQL++, GraphQL, MRQL, Asterix Query Language (AQL), RQL.</p>
<h3><span id="jsoniq-as-a-data-calculator">JSONiq as a data calculator</span></h3><p>it can perform arithmetics, but also comparison and logic. It is, however, more powerful than a common calculator and supports more complex constructs, for example variable binding.<br>It also supports all JSON values. Any copy-pasted JSON value literally returns itself.<br>And, unlike a calculator, it can access storage. </p>
<h2><span id="the-jsoniq-data-model">The JSONiq Data Model</span></h2><p>Every expression of the JSONiq “data calculator” returns a sequence of items.An item can be either an object, an array, an atomic item, or a function item. A sequence can also be empty. Caution, the empty sequence is not the same logically as a null item.</p>
<h2><span id="navigation">Navigation</span></h2><p>The general idea of navigation is that it is possible to “dive” into the nested data with dots and square brackets (originally, these were slashes with XPath) – all in parallel: starting with an original collection of objects (or, possibly, a single document), each step (i.e., for each dot and each pair of square brackets) goes down the nested structure and returns another sequence of nested items. </p>
<h3><span id="object-lookups-dot-syntax">Object lookups (dot syntax)</span></h3><p>json-doc(“file.json”).o</p>
<h3><span id="array-unboxing-empty-square-bracket-syntax">Array unboxing (empty square bracket syntax)</span></h3><p>We can unbox the array, meaning, extract its members as a sequence of seven object items, with empty square brackets.json-doc(“file.json”).o[].</p>
<h3><span id="parallel-navigation">Parallel navigation</span></h3><p>The dot syntax, in fact, works on sequences, too and will extract the value associated with a key in every object of the sequence (anything else than an object is ignored and thrown away).</p>
<h3><span id="filtering-with-predicates-simple-square-bracket-syntax">Filtering with predicates (simple square bracket syntax)</span></h3><p>It is possible to filter any sequence with a predicate, where <script type="math/tex">in the predicate refers to the current item being tested. json-doc("file.json").o[].a.b[][</script>.c = 3].It is also possible to access the item at position n in a sequence with this same notation: json-doc(“file.json”).o[].a.b[][5].</p>
<h3><span id="array-lookup-double-square-bracket-syntax">Array lookup (double square bracket syntax)</span></h3><p>To access the n-th member of an array, you can use double-squarebrackets, like so:<br>json-doc(“file.json”).o[[2]].a.</p>
<h3><span id="a-common-pitfall-array-lookup-vs-sequence-predicates">A common pitfall: Array lookup vs. Sequence predicates</span></h3><p>Do not confuse sequence positions (single square brackets) with array positions (double square brackets)!<br>([1, 2], [3, 4])[2] -&gt; [ 3, 4 ]<br>([1, 2], [3, 4])[[2]] -&gt; 2 4</p>
<h2><span id="schema-discovery">Schema discovery</span></h2><h3><span id="collections">Collections</span></h3><p>many datasets are in fact found in the form of large collections of smaller objects (as in document stores). Such collections are access with a function call together with a name or (if reading from a data lake) a path.</p>
<h3><span id="getting-all-top-level-keys">Getting all top-level keys</span></h3><p>The keys function retrieves all keys. It can be called on the entire sequence of objects and will return all unique keys found (at the top level) in that collection.</p>
<h3><span id="getting-unique-values-associated-with-a-key">Getting unique values associated with a key</span></h3><p>With distinct-values, it is then possible to eliminate duplicates and look at unique values:<br>distinct-values(collection( “<a target="_blank" rel="noopener" href="https://www.rumbledb.org/samples/git-archive.jsonl">https://www.rumbledb.org/samples/git-archive.jsonl</a>“ ).type)</p>
<h3><span id="aggregations">Aggregations</span></h3><p>Aggregations can be made on entire sequences with a single function call:The five basic functions are count, sum, avg, min, max.<br>count(distinct-values(collection( “<a target="_blank" rel="noopener" href="https://www.rumbledb.org/samples/git-archive.jsonl">https://www.rumbledb.org/samples/git-archive.jsonl</a>“ ).type))</p>
<h2><span id="construction">Construction</span></h2><h3><span id="construction-of-atomic-values">Construction of atomic values</span></h3><p>Atomic values that are core to JSON can be constructed with exactly the same syntax as JSON.</p>
<h3><span id="construction-of-objects-and-arrays">Construction of objects and arrays</span></h3><p>In fact, one can copy-paste any JSON value, and it will always be recognized as a valid JSONiq query returning that value.</p>
<h3><span id="construction-of-sequences">Construction of sequences</span></h3><p>Sequences can be constructed (and concatenated) using commas.Increasing sequences of integers can also be built with the to keyword.</p>
<h2><span id="scalar-expressions">Scalar expressions</span></h2><p>Sequences of items can have any number of items. A few JSONiq expression (arithmetic, logic, value comparison…) work on the particular case that a sequence has zero or one items.</p>
<h3><span id="arithmetic">Arithmetic</span></h3><p>JSONiq supports basic arithmetic: addition (+), subtraction (-), division (div), integer division (idiv) and modulo (mod).If the data types are different, then conversions are made automatically. The empty sequence enjoys special treatment: if one of the sides (or both) is the empty sequence, then the arithmetic expression returns an empty sequence (no error). If one of the two sides is null (and the other side is not the empty sequence), then the arithmetic expression returns null. If one of the sides (or both) is not a number, null, or the empty sequence, then a type error is thrown.</p>
<h3><span id="string-manipulation">String manipulation</span></h3><p>String concatenation is done with a double vertical bar: “foo” || “bar”.<br>Most other string manipulation primitives are available from the rich JSONiq builtin function library:<br>concat(“foo”, “bar”),string-join((“foo”, “bar”, “foobar”), “-“),substr(“foobar”, 4, 3),<br>string-length(“foobar”). </p>
<h3><span id="value-comparison">Value comparison</span></h3><p>Sequences of one atomic item can be compared with eq (equal), ne (not equal), le (lower or equal), gt (greater or equal), lt (lower than) and gt (greater than).</p>
<h3><span id="logic">Logic</span></h3><p>JSONiq supports the three basic logic expressions and, or, and not. not has the highest precedence, then and, then or.<br>JSONiq also supports universal and existential quantification:<br>every $i in 1 to 10 satisfies $i gt 0, some $i in 1 to 10 satisfies $i gt 5. </p>
<p>If one of the two sides is not a sequence of a single Boolean item, then implicit conversions are made. This mechanism is called the Effective Boolean Value (EBV). For example, an empty sequence, or a sequence of one empty string, or a sequence of one zero integer, is considered false. A sequence of one non-empty string, or a sequence or one non-zero integer, or a sequence starting with one object (or array) is considered true.</p>
<h3><span id="general-comparison">General comparison</span></h3><p>JSONiq has a shortcut for existential quantification on value comparisons. This is called general comparison.</p>
<p>some $i in (1, 2, 3, 4, 5) satisfies $i eq 1 ==<br>(1, 2, 3, 4, 5) = 1.</p>
<h2><span id="composability">Composability</span></h2><h3><span id="data-flow">Data flow</span></h3><p>A few expressions give some control over the data flow by picking the output or this or that expression based on the value of another expression. This includes conditional expressions. This includes conditional expressions. This also includes try-catch expressions.</p>
<h2><span id="binding-variables-with-cascades-of-let-clauses">Binding variables with cascades of let clauses</span></h2><p>Variables in JSONiq start with a dollar sign. It is important to understand that this is not a variable assignment that would change the value of a variable. This is only a declarative binding. </p>
<h2><span id="flwor-expressions">FLWOR expressions</span></h2><p>One of the most important and powerful features of JSONiq is the FLWOR expression. It corresponds to SELECT-FROM-WHERE queries in SQL, however, it is considerably more expressive and generic than them in several aspects. In JSONiq the clauses can appear in any order with the exception of the first and last clause. JSONiq supports a let clause, which does not exist in SQL.<br>In SQL, when iterating over multiple tables in the FROM clause, they “do not see each other”. In JSONiq, for clauses (which correspond to FROM clauses in SQL), do see each other, meaning that it is possible to iterate in higher and higher levels of nesting by referring to a previous for variable.</p>
<h3><span id="for-clauses">For clauses</span></h3><p>It can thus be seen that the for clause is akin to the FROM clause in SQL, and the return is akin to the SELECT clause. Projection in JSONiq can be made with a project() function call, with the keys to keep.  It is possible to implement a join with a sequence of two for clauses and a predicate. Note that allowing empty can be used to perform a left outer join, i.e., to account for the case when there are no matching records in the second collection. </p>
<h3><span id="let-clauses">Let clauses</span></h3><p>A let clause outputs exactly one outgoing tuple for each incoming tuple (think of a map transformation in Spark). Let clauses also allow for joining the two datasets. </p>
<h3><span id="where-clauses">Where clauses</span></h3><p>Where clauses are used to filter variable bindings (tuples) based on a predicate on these variables. They are the equivalent to a WHERE clause in SQL.</p>
<h3><span id="order-by-clauses">Order by clauses</span></h3><p>Order by clauses are used to reorganize the order of the tuples, but without altering them. They are the same as ORDER BY clauses in SQL.It is also possible, like in SQL, to specify an ascending or a descending order. In case of ties between tuples, the order is arbitrary. But it is possible to sort on another variable in case there is a tie with the first one (compound sorting keys). It is possible to control what to do with empty sequences: they can be considered smallest or greatest.</p>
<h3><span id="group-by-clauses">Group by clauses</span></h3><p>Group by clauses organize tuples in groups based on matching keys, and then output only one tuple for each group, aggregating other variables (count, sum, max, min…). It is also possible to opt out of aggregating other (non-grouping-key) variables.</p>
<h2><span id="types">Types</span></h2><h3><span id="variable-types">Variable types</span></h3><p>Since every value in JSONiq is a sequence of item, a sequence type consists of two parts: an item type, and a cardinality.<br>Item types can be any of the builtin atomic types. as well as “object”, “array” and the most generic item type, “item”.<br>Cardinality can be one of the following four:Any number of items (suffix <em>); for example object</em>, One or more items (suffix +); for example array+,Zero or one item (suffix ?); for example integer,Exactly one item (no suffix); for example boolean?</p>
<h3><span id="type-expressions">Type expressions</span></h3><p>An “instance of” expression checks whether a sequences matches a sequence type, and returns true or false. A “cast as” expression casts single items to an expected item type.<br>A “castable as” expression tests whether a cast would succeed (in which case it returns true) or not (false).<br>A “treat as” expression checks whether its input sequence matches an expected type (like a type on a variable); if it does, the input sequence is returned unchanged. If not, an error is raised.</p>
<h3><span id="types-in-user-defined-functions">Types in user-defined functions</span></h3><p>JSONiq supports user-defined functions. Parameter types can be optionally specified, and a return type can also be optionally specified.</p>
<h3><span id="validating-against-a-schema">Validating against a schema</span></h3><p>It is possible to declare a schema, associating it with a user-defined type, and to validate a sequence of items against this user-defined type.</p>
<h2><span id="architecture-of-a-query-engine">Architecture of a query engine</span></h2><h3><span id="static-phase">Static phase</span></h3><p>When a query is received by an engine, it is text that needs to be parsed. The output of this is a tree structure called an Abstract Syntax Tree. An Abstract Syntax Tree, even though it already has the structure of a tree, is tightly tied to the original syntax. Thus, it needs to be converted into a more abstract Intermediate Representation called an expression tree. Every node in this tree corresponds to either an expression or a clause in the JSONiq language, making the design modular. At this point, static typing takes place, meaning that the engine infers the static type of each expression, that is, the most specific type possible expected at runtime (but without actually running the program). Engines like RumbleDB perform their optimization round on this Intermediate Representation. Once optimizations have been done, RumbleDB decides the mode with which each expression and clause will be evaluated (locally, sequentially, in parallel, in DataFrames, etc). The resulting expression tree is then converted to a runtime iterator tree; this is the query plan that will actually be evaluated by the engine.</p>
<h3><span id="dynamic-phase">Dynamic phase</span></h3><p>During the dynamic phase, the root of the tree is asked to produce a sequence of items, which is to be the final output of the query as a whole. Then, recursively, each node in the tree will ask its children to produce sequences of items (or tuple streams). Each node then combines the sequences of items (or tuple streams) it receives from its children in order to produce its own sequence of items according to its semantics, and pass it to its parent. </p>
<h4><span id="materialization">Materialization</span></h4><p>When a sequence of items is materialized, it means that an actual List (or Array, or Vector), native to the language of implementation (in this case Java) is stored in local memory, filled with the items. </p>
<h4><span id="streaming">Streaming</span></h4><p>When a sequence of items (or tuple stream) is produced and consumed in a streaming fashion, it means that the items (or tuples) are produced and consumed one by one, iteratively. But the whole sequence of items (or tuple stream) is never stored anywhere. The classical pattern for doing so is known as the Volcano iterator architecture.</p>
<p>However, there are two problems with this: first, it can take a lot of time to go through the entire sequence (imagine doing so with billions or trillions of items). Second, there are expressions or clauses that are not compatible with streaming (consider, for example, the group by or order by clause, which cannot be implemented without materializing their full input).</p>
<h4><span id="parallel-execution-with-rdds">Parallel execution (with RDDs)</span></h4><p>When a sequence becomes unreasonably large, RumbleDB switches to a parallel execution, leveraging Spark capabilities: the sequences of items are passed and processed as RDDs of Item objects.</p>
<h4><span id="parallel-execution-with-dataframes">Parallel execution (with DataFrames)</span></h4><p>The RDD implementation supports heterogeneous sequences by leveraging the polymorphism of Item objects. However, this is not efficient in the case that Items in the same sequence happen to have a regular structure. Thus, if the Items in a sequence are valid against a specific schema, or even against an array type or an atomic type, the underlying physical storage in memory relies on Spark DataFrames instead of RDDs.</p>
<p>To summarize, homogeneous sequences of the most common types are stored in DataFrames, and RDDs are used in all other cases.</p>
<h4><span id="parallel-execution-with-native-sql">Parallel execution (with Native SQL)</span></h4><p>In some cases (more in every release), RumbleDB is able to evaluate the query using only Spark SQL, compiling JSONiq to SQL directly instead of packing Java runtime iterators in UDFs. This leads to faster execution, because UDFs are slower than a native execution in SQL. This is because, to a SQL optimizer, UDFs are opaque and prevent automatic optimizations.</p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-11-28T13:19:33.000Z" title="2023-11-28 2:19:33 ├F10: PM┤">2023-11-28</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-12-12T15:35:34.301Z" title="2023-12-12 4:35:34 ├F10: PM┤">2023-12-12</time></span><span class="level-item"><a class="link-muted" href="/categories/database/">database</a></span><span class="level-item">17 minutes read (About 2537 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/11/28/bigdata9/">bigdata - MongoDB</a></p><div class="content"><h1><span id="document-stores">Document stores</span></h1><p>Can we rebuild a similar system for collections of trees, in the sense that we drop all three constraints: relational integrity, domain integrity, and atomic integrity? Document stores bring us one step in this direction.</p>
<h2><span id="challenges">Challenges</span></h2><h3><span id="schema-on-read">Schema on read</span></h3><p>In a relational database management system, it is not possible to populate a table without having defined its schema first. However, when encountering such denormalized data, in the real world, there is often no schema. In fact, one of the important features of a system that deals with denormalized data is the ability to discover a schema.</p>
<h3><span id="making-trees-fit-in-tables">Making trees fit in tables</span></h3><p>Several XML elements (or, likewise, several JSON objects) can be naturally mapped to a relational table with several rows if the collection is flat and homogeneous, but semi-structured data can generally be nested and heterogeneous. if we map nested and heterogeneous into a table,such mapping will at best have to be done for every single dataset, and requires in most cases a schema, whereas we are looking for a generic solution for semistructured data with no a-priori schema information.</p>
<h2><span id="document-stores">Document stores</span></h2><p>Document stores provide a native database management system for semi-structured data. Document stores work on collections of records, generalizing the way that relational tables can be seen as collections of rows. It is important to understand that document stores are optimized for the typical use cases of many records of small to medium sizes. Typically, a collection can have millions or billions of documents, while each single document weighs no more than 16 MB (or a size in a similar magnitude). Finally, a collection of documents need not have a schema: it is possible to insert random documents that have dissimilar structures with no problem at all. Most document stores, however, do provide the ability to add a schema. Document stores can generally do selection, projection, aggregation and sorting quite well, but many of them are typically not (yet) optimized for joining collections. In fact, often, their language or API does not offer any joining functionality at all, which pushes the burden to reimplement joins in a host language to the users. This is a serious breach of data independence.</p>
<h2><span id="implementations">Implementations</span></h2><p>There is a very large number of products in the document store space for both JSON and XML, let us mention for example MongoDB, CouchDB, ElasticSearch, Cloudant, existDB, ArangoDB, BaseX, MarkLogic, DocumentDB, CosmosDB, and so on. We will focus, as an example, on MongoDB.</p>
<h2><span id="physical-storage">Physical storage</span></h2><p>Just like the storage format is optimized for tabular data in a relational database management system, it is optimized for tree-like data in a document store. In MongoDB, the format is a binary version of JSON called BSON. BSON is basically based on a sequence of tokens that efficiently encode the JSON constructs found in a document. The immediate benefit of BSON is that it takes less space in storage than JSON stored as a text file: for example, null, true and false literals need four or five bytes in text format at best, while they can be efficiently encoded as single bytes in BSON. Furthermore, BSON supports additional types that JSON does not have, such as dates. </p>
<h2><span id="querying-paradigm-crud">Querying paradigm (CRUD)</span></h2><p>The API of MongoDB, like many document stores, is based on the CRUD paradigm. CRUD means Create, Read, Update, Delete and corresponds to low-level primitives similar to those for HBase. MongoDB supports several host languages to query collections via an API. This includes in particular JavaScript and Python, but many other languages are supported via drivers. We will use JavaScript here because this is the native host language of MongoDB. It is important to note that these APIs are not query languages. MongoDB also provides access to the data via a shall called mongo or, newly, mongosh. This is a simple JavaScript interface wrapped around the MongoDB’s node.js driver. </p>
<h3><span id="populating-a-collection">Populating a collection</span></h3><p>To create a collection, one can simply insert a document in it, and it will be automatically created if it does not exist.</p>
<p>MongoDB automatically adds to every inserted document a special field called “ id” and associated with a value called an Object ID and with a type of its own.Object IDs are convenient for deleting or updating a specific document with no ambiguity.</p>
<h3><span id="querying-a-collection">Querying a collection</span></h3><h4><span id="scan-a-collection">Scan a collection</span></h4><p>Asking for the contents of an entire collection is done with a simple find() call on the previous object:db.collection.find().This function does not in fact return the entire collection; rather, it returns some pointer, called a cursor, to the collection; the user can then iterate on the cursor in an imperative fashion in the host language.</p>
<h4><span id="selection">Selection</span></h4><p>It is possible to perform a selection on a collection by passing a parameter to find() that is a JSON object:<br>db.collection.find({ “Theory” : “Relativity” }). </p>
<p>A disjunction (OR) uses a special MongoDB keyword, prefixed with a dollar sign:<br>db.collection.find( { “$or” : [ { “Theory”:”Relativity” }, { “Last”:”Einstein” } ] } ).</p>
<p>MongoDB offers many other keywords, for example for comparison other than equality:<br>db.collection.find( { “Publications” : { “$gte” : 100 } } )</p>
<h4><span id="projection">Projection</span></h4><p>Projections are made with the second parameter of this same find() method. This is done in form of a JSON object associating all the desired keys in the projection with the value 1. db.scientists.find( { “Theory” : “Relativity” }, { “First” : 1, “Last”: 1 } ).</p>
<p>It is also possible to project fields away in the same way with 0s, however 1s and 0s cannot be mixed in the projection parameter, except in the specific above case of projecting away the object ID</p>
<h4><span id="counting">Counting</span></h4><p>Counting can be done by chaining a count() method call:<br>db.scientists.find( { “Theory” : “Relativity” } ).count().</p>
<h4><span id="sorting">Sorting</span></h4><p>Sorting can be done by chaining a sort() method call.<br>db.scientists.find( { “Theory” : “Relativity” }, { “First” : 1, “Last” : 1 } ).sort( { “First” : 1, “Name” : -1 } )</p>
<p>1 is for ascending order and -1 for descending order.It is also possible to add limits and offsets to paginate results also by chaining more method calls:<br>db.scientists.find( { “Theory” : “Relativity” }, { “First” : 1, “Last” : 1 } ).sort( { “First” : 1, “Name” : -1 } ).skip(30).limit(10).</p>
<p>Note that, contrary to intuition, the order of the calls does not matter, as this is really just the creation of a query plan by providing parameters (in any order).</p>
<h4><span id="duplicate-elimination">Duplicate elimination</span></h4><p>It is possible to obtain all the distinct values for one field with a distinct() call: db.scientists.distinct(“Theory”)</p>
<h3><span id="querying-for-heterogeneity">Querying for heterogeneity</span></h3><h4><span id="absent-fields">Absent fields</span></h4><p>Absent fields can be filtered with: db.scientists.find( { “Theory” : null } ). </p>
<h4><span id="filtering-for-values-across-types">Filtering for values across types</span></h4><p>db.collection.find( { “$or” : [ { “Theory”: “Relativity” }, { “Theory”: 42 }, { “Theory”: null } ] } )</p>
<p>db.scientists.find( { “Theory” : { “$in” : [“Relativity”, 42, null ] } } ).</p>
<p>MongoDB is also able to sort on fields that have heterogeneous data types. It does so by first order by type in some (arbitrary, but documented) order, and then within each type.</p>
<h3><span id="querying-for-nestedness">Querying for nestedness</span></h3><p>Nestedness in MongoDB is handled in several ad-hoc ways for specific use cases.</p>
<h4><span id="values-in-nested-objects">Values in nested objects</span></h4><p>We saw how to select documents based on values associated with a top-level keys. What about values that are not at the top-level, but in nested objects?<br>The first solution that might come to mind is something like this: db.scientists.find({ “Name” : { “First” : “Albert” } }) However, this query will not have the behavior many would have expected: instead of finding documents that have a value “Albert” associated with the key “First” in an object itself associated with the top-level key ”Name”, this query looks for an exact match of the entire object.<br>In order to include documents such as above, MongoDB uses a dot syntax: db.scientists.find({ “Name.First” : “Albert” }).</p>
<h4><span id="values-in-nested-arrays">Values in nested arrays</span></h4><p>MongoDB allows to filter documents based on whether a nested array contains a specific value, like so: db.scientists.find({ “Theories” : “Special relativity” }). </p>
<h4><span id="deleting-objects-from-a-collection">Deleting objects from a collection</span></h4><p>Objects can be deleted from a collection either one at a time with deleteOne(), or several at a time with deleteMany(): db.scientists.deleteMany( { “century” : “15” } ). </p>
<h4><span id="updating-objects-in-a-collection">Updating objects in a collection</span></h4><p>Documents can be updated with updateOne() and updateMany() by providing both a filtering criterion (with the same syntax as the first parameter of find()) and an update to apply. The command looks like so: db.scientists.updateMany( { “Last” : “Einstein” }, { $set : { “Century” : “20” } } ).</p>
<p>The granularity of updates is per document, that is, a single document can be updated by at most one query at the same time.However, within the same collection, several different documents can be modified concurrently by different queries in parallel.</p>
<h4><span id="complex-pipelines">Complex pipelines</span></h4><p>For grouping and such more complex queries, MongoDB provides an API in the form of aggregation pipelines.<br>db.scientists.aggregate( { $match : { “Century” : 20 }}, { $group : { “Year” : “$year”, “Count” : { “$sum” : 1 } } }, { $sort : { “Count” : -1 } }, { $limit : 5 } ).</p>
<h3><span id="limitations-of-a-document-store-querying-api">Limitations of a document store querying API</span></h3><p>Simple use cases are straightforward to handle, however more complex use cases require a lot of additional code in the host language, be it JavaScript or Python. An example is that joins must be taken care of by the end user in the host language: the burden of implementing more complex use cases is pushed to the end user.</p>
<h2><span id="architecture">Architecture</span></h2><p>The architecture of MongoDB follows similar principles to what we covered before: scaling out the hardware to multiple machine, and sharding as well as replicating the data.</p>
<h3><span id="sharding-collections">Sharding collections</span></h3><p>Collections in MongoDB can be sharded. Shards are determined by selecting one or several fields. (Lexicographically-ordered) intervals over these fields then determine the shards. The fields used to shard must be organized in a tree index structure.</p>
<h3><span id="replica-sets">Replica sets</span></h3><p>A replica set is a set of several nodes running the MongoDB server process. The nodes within the same replica set all have a copy of the same data.</p>
<p>Each shard of each collection is assigned to exactly one replica set. Note that this architecture is not the same as that of HDFS, in which the replicas are spread over the entire cluster with no notion of “walls” between replica sets and no two DataNodes having the exact same block replicas.</p>
<h3><span id="write-concerns">Write concerns</span></h3><p>When writing (be it delete, update or insert) to a collection, more exactly, to a specific shard of a collection, MongoDB checks that a specific minimum number of nodes (within the replica set that is responsible for the shard) have successfully performed the update.</p>
<h3><span id="motivation">Motivation</span></h3><p>A document store, unlike a data lake, manages the physical data layout. This has a cost: the need to import (ETL) data before it is possible to query it, but this cost comes with a nice benefit: index support, just like relational database management systems.</p>
<h3><span id="hash-indices">Hash indices</span></h3><p>Hash indices are used to optimize point queries and more generally query that select on a specific value of a field. The general idea is that all the values that a field takes in a specific collection can be hashed to an integer. The value, together with pointers to the corresponding documents, is then placed in a physical array in memory, at the position corresponding to this integer.</p>
<h3><span id="tree-indices">Tree indices</span></h3><p>Hash indices are great and fast, but have limitations: first, they consume space. The more one wants to avoid hash collisions, the larger the array needs to be. But more importantly, hash indices cannot support range queries. This is because hashes do not preserve the order of the values and distribute them “randomly” in the index array structure.</p>
<p>Range queries are supported with tree indices. Instead of an array, tree indices use some sort of tree structure in which they arrange the possible values of the indexed field, such that the values are ordered when traversing the tree in a depth-first-search manner. More precisely, the structure is called a B+-tree. Unlike a simple binary tree, nodes have a large number of children.</p>
<h3><span id="secondary-indices">Secondary indices</span></h3><p>By default, MongoDB always builds a tree index for the id field. Users can request to build hash and tree indices for more fields. These indices are called secondary indices.</p>
<p>The command for building a hash index looks like so: db.scientists.createIndex({ “Name.Last” : “hash” })</p>
<p>And for a tree index (1 means in ascending order, -1 would be descending): db.scientists.createIndex({ “Name.Last” : 1 }).</p>
<h3><span id="when-are-indices-useful">When are indices useful</span></h3><p>When building indices, it is important to get a feeling for whether a query will be faster or not with this index.</p>
<h3><span id="index-types">index types</span></h3><h4><span id="single-field-indexes">Single Field Indexes</span></h4><p>By default, all collections have an index on the _id field. You can add additional indexes to speed up important queries and operations. You can create a single-field index on any field in a document, including:Top-level document fields, Embedded documents ,Fields within embedded documents. When you create an index on an embedded document, only queries that specify the entire embedded document use the index. Queries on a specific field within the document do not use the index. In order for a dot notation query to use an index, you must create an index on the specific embedded field you are querying, not the entire embedded object. </p>
<h4><span id="compound-indexes">Compound Indexes</span></h4><p>Compound indexes collect and sort data from two or more fields in each document in a collection. Data is grouped by the first field in the index and then by each subsequent field.</p>
<p>The order of the indexed fields impacts the effectiveness of a compound index. Compound indexes contain references to documents according to the order of the fields in the index. To create efficient compound indexes, follow the ESR (Equality, Sort, Range) rule. The ESR (Equality, Sort, Range) Rule is to place fields that require exact matches first in your index. Sort follows equality matches because the equality matches reduce the number of documents that need to be sorted. Sorting after the equality matches also allows MongoDB to do a non-blocking sort. “Range” filters scan fields. The scan doesn’t require an exact match, which means range filters are loosely bound to index keys. To improve query efficiency, make the range bounds as tight as possible and use equality matches to limit the number of documents that must be scanned. MongoDB cannot do an index sort on the results of a range filter. Place the range filter after the sort predicate so MongoDB can use a non-blocking index sort.  </p>
<p>Compound indexes cannot support queries where the sort order does not match the index or the reverse of the index. </p>
<p>Index prefixes are the beginning subsets of indexed fields. Compound indexes support queries on all fields included in the index prefix. Index fields are parsed in order; if a query omits an index prefix, it is unable to use any index fields that follow that prefix.</p>
<h4><span id="multikey-indexes">Multikey Indexes</span></h4><p>Multikey indexes collect and sort data from fields containing array values. Multikey indexes improve performance for queries on array fields.</p>
<p>In a compound multikey index, each indexed document can have at most one indexed field whose value is an array. </p>
<h2><span id="exercise">exercise</span></h2><h3><span id="data-model">data model</span></h3><h4><span id="embedded-data-models">Embedded Data Models</span></h4><p>In general, embedding provides better performance for read operations, as well as the ability to request and retrieve related data in a single database operation. Embedded data models make it possible to update related data in a single atomic write operation.</p>
<h4><span id="normalized-data-models">Normalized Data Models</span></h4><p>Normalized data models describe relationships using references between documents.</p>
<h2><span id="references">references</span></h2><p><a target="_blank" rel="noopener" href="https://www.mongodb.com/docs/manual/core/data-model-design/">https://www.mongodb.com/docs/manual/core/data-model-design/</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-11-17T15:56:37.000Z" title="2023-11-17 4:56:37 ├F10: PM┤">2023-11-17</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-12-07T20:28:48.516Z" title="2023-12-7 9:28:48 ├F10: PM┤">2023-12-07</time></span><span class="level-item"><a class="link-muted" href="/categories/theory/">theory</a></span><span class="level-item">8 minutes read (About 1193 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/11/17/pai8/">pai - Tabular Reinforcement Learning</a></p><div class="content"><h1><span id="tabular-reinforcement-learning">Tabular Reinforcement Learning</span></h1><h2><span id="the-reinforcement-learning-problem">The Reinforcement Learning Problem</span></h2><p>Reinforcement learning is concerned with probabilistic planning in unknown environments. In this chapter, we will begin by considering reinforcement learning with small state and action spaces. This setting is often called the tabular setting, as the value functions can be computed exhaustively for all states and stored in a table.</p>
<p>Clearly, the agent needs to trade exploring and learning about the environment with exploiting its knowledge to maximize rewards. In fact, Bayesian optimization can be viewed as reinforcement learning with a fixed state and a continuous action space: In each round, the agent plays an action, aiming to find the action that maximizes the reward.Another key challenge of reinforcement learning is that the observed data is dependent on the played actions.</p>
<h3><span id="trajectories">Trajectories</span></h3><p><img src="/2023/11/17/pai8/image-64.png" alt="Alt text"></p>
<p>Crucially, the newly observed states xt+1 and the rewards rt (across multiple transitions) are conditionally independent given the previous states xt and actions at. This independence property is crucial for being able to learn about the underlying Markov decision process. Notably, this implies that we can apply the law of large numbers (1.83) and Hoeffding’s inequality (1.87) to our estimators of both quantities.</p>
<p>The collection of data is commonly classified into two settings. In the episodic setting, the agent performs a sequence of “training” rounds (called episodes). In the beginning of each round, the agent is reset to some initial state. In contrast, in the continuous setting (or non-episodic,or online setting), the agent learns online. </p>
<h3><span id="control">control</span></h3><p>Another important distinction in how data is collected, is the distinction between on-policy and off-policy control. As the names suggest, on-policy methods are used when the agent has control over its own actions, in other words, the agent can freely choose to follow any policy. In contrast, off-policy methods can be used even when the agent cannot freely choose its actions. Off-policy methods are therefore able to make use of observational data.Off-policy methods are therefore more sample-efficient than on-policy methods. This is crucial, especially in settings where conducting experiments (i.e., collecting new data) is expensive.</p>
<p>On-Policy learning algorithms are the algorithms that evaluate and improve the same policy which is being used to select actions. Off-Policy learning algorithms evaluate and improve a policy that is different from Policy that is used for action selection.</p>
<p>To understand the difference between on-policy learning and off-policy learning one must first understand the difference between the behavior policy (i.e., sampling policy) and the update policy. The behavior policy is the policy an agent follows when choosing which action to take in the environment at each time step. The update policy is how the agent updates the Q-function. On-policy algorithms attempt to improve upon the current behavior policy that is used to make decisions and therefore these algorithms learn the value of the policy carried out by the agent, Off-policy algorithms learn the value of the optimal policy and can improve upon a policy that is different from the behavior policy. Determining if the update and behavior policy are the same or different can give us insight into whether or not the algorithm is on-policy or off-policy.</p>
<h2><span id="model-based-approaches">Model-based Approaches</span></h2><p>Approaches to reinforcement learning are largely categorized into two classes. Model-based approaches aim to learn the underlying Markov decision process. In contrast, model-free approaches learn the value function directly.</p>
<h3><span id="learning-the-underlying-markov-decision-process">Learning the Underlying Markov Decision Process</span></h3><p>A natural first idea is to use maximum likelihood estimation to approximate transition and reward function.</p>
<p><img src="/2023/11/17/pai8/image-65.png" alt="Alt text"></p>
<h3><span id="ε-greedy-algorithm">ε-greedy Algorithm</span></h3><p><img src="/2023/11/17/pai8/image-66.png" alt="Alt text"></p>
<p>The key problem of ε-greedy is that it explores the state space in an uninformed manner. In other words, it explores ignoring all past experience. It thus does not eliminate clearly suboptimal actions.</p>
<h3><span id="rmax-algorithm">Rmax Algorithm</span></h3><p>A key principle in effectively trading exploration and exploitation is “optimism in the face of uncertainty”. Let us apply this principle to the reinforcement learning setting. The key idea is to assume that the dynamics and rewards model “work in our favor” until we have learned “good estimates” of the true dynamics and rewards. </p>
<p><img src="/2023/11/17/pai8/image-67.png" alt="Alt text"></p>
<p>How many transitions are “enough”? We can use Hoeffding’s inequality to get a rough idea!</p>
<p><img src="/2023/11/17/pai8/image-68.png" alt="Alt text"></p>
<h3><span id="challenges">challenges</span></h3><h2><span id="model-free-approaches">Model-free Approaches</span></h2><p>A significant benefit to model-based reinforcement learning is that it is inherently off-policy. That is, any trajectory regardless of the policy used to obtain it can be used to improve the model of the underlying Markov decision process. In the model-free setting, this not necessarily true.</p>
<h3><span id="on-policy-value-estimation">On-policy Value Estimation</span></h3><p><img src="/2023/11/17/pai8/image-71.png" alt="Alt text"></p>
<p>Note that to estimate this expectation we use a single(!) sample.However, there is one significant problem in this approximation. Our approximation of vπ does in turn depend on the (unknown) true value of vπ. The key idea is to use a bootstrapping estimate of the value function instead. That is, in place of the true value function vπ, we will use a “running estimate” Vπ. In other words, whenever observing a new transition, we use our previous best estimate of vπ to obtain a new estimate Vπ.</p>
<p>Crucially, using a bootstrapping estimate generally results in biased estimates of the value function. Moreover, due to relying on a single sample, the estimates tend to have very large variance. </p>
<h4><span id="td-learning">TD-learning</span></h4><p>The variance of the estimate is typically reduced by mixing new estimates of the value function with previous estimates using a learning rate αt. This yields the temporal-difference learning algorithm.</p>
<p><img src="/2023/11/17/pai8/image-70.png" alt="Alt text"></p>
<p>TD-learning is a fundamentally on-policy method. That is, for the estimates Vπ to converge to the true value function vπ, the transitions that are used for the estimation must follow policy π. </p>
<h4><span id="sarsa">SARSA</span></h4><p><img src="/2023/11/17/pai8/image-69.png" alt="Alt text"></p>
<h3><span id="off-policy-value-estimation">Off-policy Value Estimation</span></h3><p><img src="/2023/11/17/pai8/image-72.png" alt="Alt text"><br>This adapted update rule explicitly chooses the subsequent action a′ according to policy π whereas SARSA absorbs this choice into the Monte Carlo approximation. The algorithm has analogous convergence guarantees to those of SARSA. Crucially, this algorithm is off-policy. As noted, the key difference to the on-policy TD-learning and SARSA is that our estimate of the Qfunction explicitly keeps track of the next-performed action. It does so for any action in any state.</p>
<h3><span id="q-learning">Q-learning</span></h3><p><img src="/2023/11/17/pai8/image-73.png" alt="Alt text"><br>Crucially, the Monte Carlo approximation of eq. (11.21) does not depend on the policy. Thus, Q-learning is an off-policy method.<br><img src="/2023/11/17/pai8/image-74.png" alt="Alt text"></p>
<h3><span id="optimistic-q-learning">Optimistic Q-learning</span></h3><p><img src="/2023/11/17/pai8/image-76.png" alt="Alt text"></p>
<h3><span id="challenges">Challenges</span></h3><p>We have seen that both the model-based Rmax algorithm and the modelfree Q-learning take time polynomial in the number of states |X| and the number of actions |A| to converge. While this is acceptable in small grid worlds, this is completely unacceptable for large state and action spaces.</p>
<h2><span id="references">references</span></h2><p><a target="_blank" rel="noopener" href="https://core-robotics.gatech.edu/2022/02/28/bootcamp-summer-2020-week-4-on-policy-vs-off-policy-reinforcement-learning/">https://core-robotics.gatech.edu/2022/02/28/bootcamp-summer-2020-week-4-on-policy-vs-off-policy-reinforcement-learning/</a></p>
</div></article></div><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2023-11-17T15:56:14.000Z" title="2023-11-17 4:56:14 ├F10: PM┤">2023-11-17</time></span><span class="level-item">Updated&nbsp;<time dateTime="2023-12-01T20:35:17.631Z" title="2023-12-1 9:35:17 ├F10: PM┤">2023-12-01</time></span><span class="level-item"><a class="link-muted" href="/categories/theory/">theory</a></span><span class="level-item">4 minutes read (About 553 words)</span></div></div><p class="title is-3 is-size-4-mobile"><a class="link-muted" href="/2023/11/17/pai7/">pai - Markov Decision Processes</a></p><div class="content"><h1><span id="markov-decision-processes">Markov Decision Processes</span></h1><p>Planning deals with the problem of deciding which action an agent should play in a (stochastic) environment(An environment is stochastic as opposed to deterministic, when the outcome of actions is random.). A key formalism for probabilistic plan ning in known environments are so-called Markov decision processes.<br><img src="/2023/11/17/pai7/image-50.png" alt="Alt text"></p>
<p>Our fundamental objective is to learn how the agent should behave to optimize its reward. In other words, given its current state, the agent should decide (optimally) on the action to play. Such a decision map — whether optimal or not — is called a policy.</p>
<p><img src="/2023/11/17/pai7/image-51.png" alt="Alt text"></p>
<p><img src="/2023/11/17/pai7/image-52.png" alt="Alt text"></p>
<p>For the purpose of our discussion of Markov decision processes and reinforcement learning, we will focus on a very common reward called discounted payoff.</p>
<p><img src="/2023/11/17/pai7/image-53.png" alt="Alt text"></p>
<p><img src="/2023/11/17/pai7/image-54.png" alt="Alt text"></p>
<p>Because we assumed stationary dynamics, rewards, and policies, the discounted payoff starting from a given state x will be independent of the start time t.</p>
<h2><span id="bellman-expectation-equation">Bellman Expectation Equation</span></h2><p>Let us now see how we can compute the value function：</p>
<p><img src="/2023/11/17/pai7/image-55.png" alt="Alt text"></p>
<p>This equation is known as the Bellman expectation equation, and it shows a recursive dependence of the value function on itself. The intuition is clear: the value of the current state corresponds to the reward from the next action plus the discounted sum of all future rewards obtained from the subsequent states.</p>
<h2><span id="policy-evaluation">Policy Evaluation</span></h2><p>Bellman’s expectation equation tells us how we can find the value function vπ of a fixed policy π using a system of linear equations.</p>
<p><img src="/2023/11/17/pai7/image-56.png" alt="Alt text"></p>
<h3><span id="fixed-point-iteration">Fixed-point Iteration</span></h3><p><img src="/2023/11/17/pai7/image-57.png" alt="Alt text"></p>
<h2><span id="policy-optimization">Policy Optimization</span></h2><h3><span id="greedy-policies">Greedy Policies</span></h3><p><img src="/2023/11/17/pai7/image-58.png" alt="Alt text"></p>
<h3><span id="bellman-optimality-equation">Bellman Optimality Equation</span></h3><p><img src="/2023/11/17/pai7/image-59.png" alt="Alt text"></p>
<p><img src="/2023/11/17/pai7/image-60.png" alt="Alt text"><br>These equations are also called the Bellman optimality equations. Intuitively, the Bellman optimality equations express that the value of a state under an optimal policy must equal the expected return for the best action from that state. Bellman’s theorem is also known as Bellman’s optimality principle, which is a more general concept.</p>
<h3><span id="policy-iteration">Policy Iteration</span></h3><p><img src="/2023/11/17/pai7/image-61.png" alt="Alt text"></p>
<p>It can be shown that policy iteration converges to an exact solution in a polynomial number of iterations.Each iteration of policy iteration requires computing the value function, which we have seen to be of cubic complexity in the number of states. </p>
<h3><span id="value-iteration">Value Iteration</span></h3><p><img src="/2023/11/17/pai7/image-62.png" alt="Alt text"><br>Value iteration converges to an ε-optimal solution in a polynomial number of iterations. Unlike policy iteration, value iteration does not converge to an exact solution in general.an iteration of 7 Sparsity refers to the interconnectivity of the state space. When only few states are reachable from any state, we call an MDP sparse. value iteration can be performed in (virtually) constant time in sparse Markov decision processes.</p>
<h2><span id="partial-observability">Partial Observability</span></h2><p>In this section, we consider how Markov decision processes can be extended to a partially observable setting where the agent can only access noisy observations Yt of its state Xt.</p>
<p><img src="/2023/11/17/pai7/image-63.png" alt="Alt text"></p>
<p>Observe that a Kalman filter can be viewed as a hidden Markov model with conditional linear Gaussian motion and sensor models and a Gaussian prior on the initial state.</p>
<p>POMDPs can be reduced to a Markov decision process with an enlarged state space.</p>
</div></article></div><nav class="pagination" role="navigation" aria-label="pagination"><div class="pagination-previous is-invisible is-hidden-mobile"><a href="/page/0/">Previous</a></div><div class="pagination-next"><a href="/page/2/">Next</a></div><ul class="pagination-list is-hidden-mobile"><li><a class="pagination-link is-current" href="/">1</a></li><li><a class="pagination-link" href="/page/2/">2</a></li><li><span class="pagination-ellipsis">&hellip;</span></li><li><a class="pagination-link" href="/page/8/">8</a></li></ul></nav></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/tx.png" alt="s-serenity"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">s-serenity</p><p class="is-size-6 is-block">student</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Shanghai, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">74</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">9</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">21</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/s-serenity" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/s-serenity"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://github.com/s-serenity" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/algorithm/"><span class="level-start"><span class="level-item">algorithm</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/computer-system/"><span class="level-start"><span class="level-item">computer system</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/database/"><span class="level-start"><span class="level-item">database</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/development-tools/"><span class="level-start"><span class="level-item">development tools</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/others/"><span class="level-start"><span class="level-item">others</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/practice/"><span class="level-start"><span class="level-item">practice</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/programming-language/"><span class="level-start"><span class="level-item">programming language</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/theory/"><span class="level-start"><span class="level-item">theory</span></span><span class="level-end"><span class="level-item tag">18</span></span></a></li><li><a class="level is-mobile" href="/categories/web-development/"><span class="level-start"><span class="level-item">web development</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-01-25T16:14:35.000Z">2024-01-25</time></p><p class="title"><a href="/2024/01/25/pai11/">pai - review notes</a></p><p class="categories"><a href="/categories/theory/">theory</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-01-03T15:06:23.000Z">2024-01-03</time></p><p class="title"><a href="/2024/01/03/bigdata13/">bigdata - review notes</a></p><p class="categories"><a href="/categories/database/">database</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-12-12T14:22:00.000Z">2023-12-12</time></p><p class="title"><a href="/2023/12/12/bigdata12/">bigdata - Cube Data</a></p><p class="categories"><a href="/categories/database/">database</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-12-12T14:21:07.000Z">2023-12-12</time></p><p class="title"><a href="/2023/12/12/bigdata11/">bigdata - Graph Database</a></p><p class="categories"><a href="/categories/database/">database</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2023-12-07T14:26:41.000Z">2023-12-07</time></p><p class="title"><a href="/2023/12/07/pai10/">pai - Model-based Approximate Reinforcement Learning</a></p><p class="categories"><a href="/categories/theory/">theory</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2024/"><span class="level-start"><span class="level-item">2024</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/"><span class="level-start"><span class="level-item">2023</span></span><span class="level-end"><span class="level-item tag">44</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/"><span class="level-start"><span class="level-item">2022</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/"><span class="level-start"><span class="level-item">2021</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/"><span class="level-start"><span class="level-item">2020</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/"><span class="level-start"><span class="level-item">2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/"><span class="level-start"><span class="level-item">2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Algorithm/"><span class="tag">Algorithm</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Elasticsearch/"><span class="tag">Elasticsearch</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux/"><span class="tag">Linux</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Markdown/"><span class="tag">Markdown</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SQL/"><span class="tag">SQL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/algorithm/"><span class="tag">algorithm</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/big-data/"><span class="tag">big data</span><span class="tag">13</span></a></div><div class="control"><a class="tags has-addons" href="/tags/computer-vision/"><span class="tag">computer vision</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/deep-learning/"><span class="tag">deep learning</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/git/"><span class="tag">git</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/graph/"><span class="tag">graph</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/hexo/"><span class="tag">hexo</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/java/"><span class="tag">java</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/machine-learning/"><span class="tag">machine learning</span><span class="tag">18</span></a></div><div class="control"><a class="tags has-addons" href="/tags/probability/"><span class="tag">probability</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python/"><span class="tag">python</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/reinforcement-learning/"><span class="tag">reinforcement learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/speech/"><span class="tag">speech</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/tools/"><span class="tag">tools</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="s-serenity" height="28"></a><p class="is-size-7"><span>&copy; 2024 s-serenity</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2019</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>