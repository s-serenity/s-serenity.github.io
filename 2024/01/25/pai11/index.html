<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>pai - review notes - s-serenity</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="s-serenity"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="s-serenity"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="fundamentalsprobabilitysample space, event space, σ-algebra and probability spaceA probability space is a mathematical construct that consists of three elements: the sample space (S), the event space"><meta property="og:type" content="blog"><meta property="og:title" content="pai - review notes"><meta property="og:url" content="http://yoursite.com/2024/01/25/pai11/"><meta property="og:site_name" content="s-serenity"><meta property="og:description" content="fundamentalsprobabilitysample space, event space, σ-algebra and probability spaceA probability space is a mathematical construct that consists of three elements: the sample space (S), the event space"><meta property="og:locale" content="en_US"><meta property="og:image" content="http://yoursite.com/img/og_image.png"><meta property="article:published_time" content="2024-01-25T16:14:35.000Z"><meta property="article:modified_time" content="2024-02-03T12:11:22.353Z"><meta property="article:author" content="s-serenity"><meta property="article:tag" content="machine learning"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="http://yoursite.com/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://yoursite.com/2024/01/25/pai11/"},"headline":"pai - review notes","image":["http://yoursite.com/img/og_image.png"],"datePublished":"2024-01-25T16:14:35.000Z","dateModified":"2024-02-03T12:11:22.353Z","author":{"@type":"Person","name":"s-serenity"},"publisher":{"@type":"Organization","name":"s-serenity","logo":{"@type":"ImageObject","url":"http://yoursite.com/img/logo.svg"}},"description":"fundamentalsprobabilitysample space, event space, σ-algebra and probability spaceA probability space is a mathematical construct that consists of three elements: the sample space (S), the event space"}</script><link rel="canonical" href="http://yoursite.com/2024/01/25/pai11/"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.1.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.svg" alt="s-serenity" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-01-25T16:14:35.000Z" title="2024-1-25 5:14:35 ├F10: PM┤">2024-01-25</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-02-03T12:11:22.353Z" title="2024-2-3 1:11:22 ├F10: PM┤">2024-02-03</time></span><span class="level-item"><a class="link-muted" href="/categories/theory/">theory</a></span><span class="level-item">20 minutes read (About 2965 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">pai - review notes</h1><div class="content"><h1><span id="fundamentals">fundamentals</span></h1><h2><span id="probability">probability</span></h2><h3><span id="sample-space-event-space-σ-algebra-and-probability-space">sample space, event space, σ-algebra and probability space</span></h3><p>A probability space is a mathematical construct that consists of three elements: the sample space (S), the event space (E), and a probability measure (P). Additionally, a sigma-algebra (σ-algebra) is associated with the event space.</p>
<p>The sample space is the set of all possible outcomes of an experiment.The event space is a collection of subsets of the sample space.<br>A sigma-algebra is a collection of subsets of the sample space. It includes the sample space, is closed under complementation, and is closed under countable unions. The probability measure is a function that assigns probabilities to events.</p>
<h3><span id="probability-mass-function-pmf-and-cumulative-distribution-function-cdfprobability-density-function-pdf">probability mass function (PMF) and cumulative distribution function (CDF),probability density function (PDF)</span></h3><p>The Probability Mass Function (PMF) is a function that gives the probability that a discrete random variable is exactly equal to a certain value. The Cumulative Distribution Function (CDF) of a random variable gives the probability that<br>X takes on a value less than or equal to x.<br>The Probability Density Function (PDF) is applicable to continuous random variables.The total area under the PDF curve is equal to 1.</p>
<h3><span id="continuous-distributions">Continuous Distributions</span></h3><p>normal distribution(Gaussian):The Gaussian CDF cannot be expressed in closed-form. Note that the mean of a Gaussian distribution coincides with the maximizer of its PDF, also called mode of a distribution.</p>
<h3><span id="joint-probability-conditional-probability-sum-rule-product-rulechain-rulethe-law-of-total-probability">Joint Probability, Conditional Probability, Sum rule, Product rule(chain rule),the law of total probability</span></h3><p>Joint probability refers to the probability of the occurrence of two or more events simultaneously. Conditional probability is the probability of one event occurring given that another event has already occurred. The sum rule, or addition rule, gives the probability of the union of two events. The product rule, also known as the chain rule, provides a way to express the joint probability of multiple events. The law of total probability is a way to express the probability of an event B by considering all possible ways in which<br>B can occur. </p>
<h3><span id="independence-conditional-independence">independence, conditional independence</span></h3><p>Conditional independence does not necessarily imply unconditional independence, and vice versa.</p>
<h3><span id="directed-graphical-modelsbayesian-networks">Directed Graphical Models(Bayesian networks)</span></h3><p>Directed graphical models (also called Bayesian networks) are often used to visually denote the (conditional) independence relationships of a large number of random variables.</p>
<h3><span id="expectation-covariance-and-variance-standard-deviationlaw-of-total-variance">Expectation, Covariance and Variance, Standard deviation,Law of total variance</span></h3><h3><span id="change-of-variables-formula">Change of variables formula</span></h3><h2><span id="probabilistic-inference">Probabilistic inference</span></h2><h3><span id="bayes-ruleconjugate-priors">Bayes’ rule,Conjugate Priors</span></h3><h3><span id="gaussianguassian-random-vector">gaussian,Guassian random vector</span></h3><p>Any affine transformation of a Gaussian random vector is a Gaussian random vector.</p>
<h2><span id="supervised-learning-and-point-estimates">Supervised Learning and Point Estimates</span></h2><h3><span id="maximum-likelihood-estimationmaximum-a-posteriori-estimation">Maximum Likelihood Estimation,Maximum a Posteriori Estimation,</span></h3><p>The MLE and MAP estimate can be seen as a naïve approximation of probabilistic inference, represented by a point density which “collapses” all probability mass at the mode of the posterior distribution.</p>
<h2><span id="exercise">exercise</span></h2><h3><span id="affine-transformationjacobian-matrix">affine transformation,Jacobian matrix,</span></h3><h1><span id="pml">PML</span></h1><h2><span id="linear-regression">linear regression</span></h2><h3><span id="linear-regressionmle-ridge-regressionmap">linear regression(MLE), ridge regression(MAP)</span></h3><h3><span id="ridgelasso">ridge,lasso</span></h3><p>least absolute shrinkage and selection operator (lasso): Laplace prior, L1 regularization.<br>Ridge: Gaussian prior, L2 regularization.</p>
<p>The primary difference lies in the penalty terms: L1 regularization uses the sum of absolute values, and L2 regularization uses the sum of squared values.<br>L1 regularization tends to result in exact zeros, leading to sparse solutions, whereas L2 regularization generally leads to smaller, non-zero coefficients.</p>
<h3><span id="bayesian-linear-regression-blr">Bayesian linear regression (BLR)</span></h3><h3><span id="aleatoric-and-epistemic-uncertainty">Aleatoric and Epistemic Uncertainty</span></h3><p>epistemic uncertainty: corresponds to the uncertainty about our model due to the lack of data. aleatoric uncertainty: “irreducible noise”, cannot be explained by the inputs and any model from the model class. </p>
<p>equation under the law of total variance. </p>
<h3><span id="kernel">kernel</span></h3><h2><span id="filtering">Filtering</span></h2><p>The process of keeping track of the state using noisy observations is also known as Bayesian filtering or recursive Bayesian estimation.</p>
<h3><span id="kalman-filter">Kalman filter</span></h3><p>A Kalman filter is simply a Bayes filter using a Gaussian distribution over the states and conditional linear Gaussians to describe the evolution of states and observations.</p>
<h2><span id="gaussian-process">Gaussian Process</span></h2><p>A Gaussian process is an infinite set of random variables such that any finite number of them are jointly Gaussian.</p>
<h3><span id="kernel-function-feature-space-rkhsstationarity-and-isotropy">kernel function, feature space, RKHS,Stationarity and isotropy</span></h3><p>A Gaussian process with a linear kernel is equivalent to Bayesian linear regression.</p>
<p>For ν = 1/2, the Matérn kernel is equivalent to the Laplace kernel. For ν → ∞, the Matérn kernel is equivalent to the Gaussian kernel.</p>
<p>Note that stationarity is a necessary condition for isotropy. In other words, isotropy implies stationarity.</p>
<p>skip 4.3.4</p>
<h2><span id="model-selection">model selection</span></h2><h3><span id="maximizing-the-marginal-likelihood">Maximizing the Marginal Likelihood</span></h3><p>Marginal likelihood maximization is an empirical Bayes method. Often it is simply referred to as empirical Bayes.<br>this approach typically avoids overfitting even though we do not use a separate training and validation set. maximizing the marginal likelihood naturally encourages trading between a large likelihood and a large prior.</p>
<h2><span id="approximations">Approximations</span></h2><h3><span id="random-fourier-featuresbochners-theoremuniform-convergence-of-fourier-features">random Fourier features,Bochner’s theorem,Uniform convergence of Fourier features</span></h3><h3><span id="inducing-points-methodsubset-of-regressors-sor-approximationfully-independent-training-conditional-fitc-approximation">inducing points method,subset of regressors (SoR) approximation,fully independent training conditional (FITC) approximation</span></h3><h2><span id="variational-inference">Variational Inference</span></h2><h3><span id="laplace-approximationbayesian-logistic-regression">Laplace Approximation,Bayesian Logistic Regression,</span></h3><p>The Laplace approximation matches the shape of the true posterior around its mode but may not represent it accurately elsewhere — often leading to extremely overconfident predictions.</p>
<h3><span id="variational-familymean-field-distribution">Variational family,mean-field distribution</span></h3><h3><span id="information-theorysurpriseentropyjensens-inequalitycross-entropykl-divergenceforward-and-reverse-kl-divergence">Information Theory,Surprise,entropy,Jensen’s Inequality,Cross-entropy,KL-divergence,Forward and Reverse KL-divergence</span></h3><p>The uniform distribution has the maximum entropy among all discrete distributions supported on {1, . . . , n}.</p>
<p>In words, KL(p∥q) measures the additional expected surprise when observing samples from p that is due to assuming the (wrong) distribution q.</p>
<p>It can be seen that the reverse KL-divergence tends to greedily select the mode and underestimating the variance which, in this case, leads to an overconfident prediction. The forward KL-divergence, in contrast, is more conservative.</p>
<p>Note, however, that reverse-KL is not greedy in the same sense as Laplace approximation, as it does still take the variance into account and does not purely match the mode of p.</p>
<h3><span id="minimizing-forward-kl-as-maximum-likelihood-estimationminimizing-forward-kl-as-moment-matching">Minimizing Forward-KL as Maximum Likelihood Estimation,Minimizing Forward-KL as Moment Matching</span></h3><p> First, we observe that minimizing the forward KL-divergence is equivalent to maximum likelihood estimation on an infinitely large sample size.</p>
<p>A Gaussian qλ minimizing KL(p∥qλ) has the same first and second moment as p.</p>
<h3><span id="evidence-lower-boundgaussian-vi-vs-laplace-approximation-gradient-of-evidence-lower-boundscore-gradients-and-reparameterization-trick">Evidence Lower Bound,Gaussian VI vs Laplace approximation, Gradient of Evidence Lower Bound(score gradients and Reparameterization trick)</span></h3><p>maximizing the ELBO coincides with minimizing reverse-KL.</p>
<p>maximizing the ELBO selects a variational distribution q that is close to the prior distribution p(·) while also maximizing the average likelihood of the data p(y1:n | x1:n, θ) for θ ∼ q.</p>
<p>Note that for a noninformative prior p(·) ∝ 1, maximizing the ELBO is equivalent to maximum likelihood estimation.</p>
<p>skip 5.5.2</p>
<h2><span id="markov-chain-monte-carlo-methods">Markov Chain Monte Carlo Methods</span></h2><p>The key idea of Markov chain Monte Carlo methods is to construct a Markov chain, which is efficient to simulate and has the stationary distribution p.</p>
<h3><span id="markov-chainsstationarityconvergence-markov-propertytime-homogeneous-markov-chains">Markov Chains(Stationarity,convergence), Markov property,time-homogeneous Markov chains</span></h3><p>Intuitively, the Markov property states that future behavior is independent of past states given the present state.</p>
<h3><span id="stationary-distributionaperiodicergodicityfundamental-theorem-of-ergodic-markov-chains">Stationary distribution,aperiodic,Ergodicity,Fundamental theorem of ergodic Markov chains</span></h3><p>After entering a stationary distribution π, a Markov chain will always remain in the stationary distribution.</p>
<p>It can be shown that there exists a unique stationary distribution π if the Markov chain is irreducible, that is, if every state is reachable from every other state with a positive probability when the Markov chain is run for enough steps.</p>
<p>Even if a Markov chain has a unique stationary distribution, it must not converge to it.</p>
<p>In words, a Markov chain is aperiodic iff for every state x, the transition graph has a closed path from x to x with length k for all k ∈ N greater than some k0 ∈ N.</p>
<p>A Markov chain is ergodic iff there exists a t ∈ N0 such that for any x, x′ ∈ S we have p(t)(x′ | x) &gt; 0.</p>
<p>A commonly used strategy to ensure that a Markov chain is ergodic is to add “self-loops” to every vertex in the transition graph.</p>
<p>An ergodic Markov chain has a unique stationary distribution π (with full support) irrespectively of the initial distribution q0. This naturally suggests constructing an ergodic Markov chain such that its stationary distribution coincides with the posterior distribution. If we then sample “sufficiently long”, Xt is drawn from a distribution that is “very close” to the posterior distribution.</p>
<h3><span id="how-quickly-does-a-markov-chain-convergetotal-variation-distance-and-mixing-time">How quickly does a Markov chain converge?(Total variation distance and Mixing time)</span></h3><h3><span id="detailed-balance-equationergodic-theorem">Detailed Balance Equation,Ergodic Theorem</span></h3><p>A Markov chain that satisfies the detailed balance equation with respect to π is called reversible with respect to π.</p>
<p>Ergodic Theorem is a way to generalize the (strong) law of large numbers to Markov chains.</p>
<h3><span id="elementary-sampling-methodsmetropolis-hastings-algorithmmetropolis-hastings-theorem-gibbs-sampling">Elementary Sampling Methods,Metropolis-Hastings Algorithm,Metropolis-Hastings theorem, Gibbs Sampling</span></h3><p>A popular example of a Metropolis-Hastings algorithm is Gibbs sampling. </p>
<h3><span id="sampling-as-optimizationgibbs-distributionlangevin-dynamicsmetropolis-adjusted-langevin-algorithm-mala-or-langevin-monte-carlo-lmcunadjusted-langevin-algorithm-ula-stochastic-gradient-langevin-dynamics-hamiltonian-monte-carlo">Sampling as Optimization,Gibbs distribution,Langevin Dynamics,Metropolis adjusted Langevin algorithm (MALA) or Langevin Monte Carlo (LMC),unadjusted Langevin algorithm (ULA), Stochastic Gradient Langevin Dynamics, Hamiltonian Monte Carlo</span></h3><p>A useful property is that Gibbs distributions always have full support. Observe that the posterior distribution can always be interpreted as a Gibbs distribution as long as prior and likelihood have full support.</p>
<p>Langevin dynamics adapts the Gaussian proposals of the Metropolis-Hastings algorithm to search the state space in an “informed” direction. The simple idea is to bias the sampling towards states with lower energy, thereby making it more likely that a proposal is accepted. A natural idea is to shift the proposal distribution perpendicularly to the gradient of the energy function. The resulting variant of Metropolis-Hastings is known as the Metropolis adjusted Langevin algorithm (MALA) or Langevin Monte Carlo (LMC).</p>
<p>The HMC algorithm is an instance of Metropolis-Hastings which uses momentum to propose distant points that conserve energy, with high acceptance probability.</p>
<h3><span id="path">path</span></h3><p>target: show that the stationary distribution of a Markov chain coincides with the posterior distribution.<br>base: Detailed Balance Equation which shows that A Markov chain that satisfies the detailed balance equation with respect to π is called reversible with respect to π and if the Markov chain is reversible with respect to π then π is a stationary distribution. With posterior distribution p(x) = 1/Z q(x), substitute the posterior for π in the detailed balance equation, we can remove z, so we do not need to know the true posterior p to check that the stationary distribution of our Markov chain coincides with p, it suffices to know the finite measure q. However, until now, this does not allow us to estimate expectations over the posterior distribution. Note that although constructing such a Markov chain allows us to obtain samples from the posterior distribution, they are not independent. Thus, the law of large numbers and Hoeffding’s inequality do not apply, but there is a way to generalize the (strong) law of large numbers to Markov chains, which is Ergodic theorem. </p>
<p>Next we need to consider how to construct Markov chain with the goal of approximating samples from the posterior distribution p. One way is Metropolis-Hastings Algorithm, in which we are given a proposal distribution and we use the acceptance distribution to decide whether to follow the proposal. Another way is Gibbs sampling. </p>
<p>MCMC techniques can be generalized to<br>continuous random variables / vectors. Observe that the posterior distribution can always be interpreted as a Gibbs distribution as long as prior and likelihood have full support.</p>
<p>Langevin dynamics adapts the Gaussian proposals of the Metropolis-Hastings algorithm to search the state space in an “informed” direction. The simple idea is to bias the sampling towards states with lower energy, thereby making it more likely that a proposal is accepted. A natural idea is to shift the proposal distribution perpendicularly to the gradient of the energy function. The resulting variant of Metropolis-Hastings is known as the Metropolis adjusted Langevin algorithm (MALA) or Langevin Monte Carlo (LMC).</p>
<p>skip 6.3.5</p>
<h2><span id="bdl">BDL</span></h2><h3><span id="artificial-neural-networksactivation-functions">Artificial Neural Networks,Activation Functions,</span></h3><p>Non-linear activation functions allow the network to represent arbitrary functions. This is known as the universal approximation theorem.</p>
<h3><span id="bayesian-neural-networks-heteroscedastic-noise-variational-inferencemarkov-chain-monte-carlo-swaswagdropout-and-dropconnect-probabilistic-ensembles">Bayesian Neural Networks, Heteroscedastic Noise, Variational Inference,Markov Chain Monte Carlo, SWA,SWAG,Dropout and Dropconnect, Probabilistic Ensembles</span></h3><p>Intuitively, variational inference in Bayesian neural networks can be interpreted as averaging the predictions of multiple neural networks drawn according to the variational posterior qλ.</p>
<h3><span id="calibrationexpected-calibration-error-ece-maximum-calibration-error-mce-histogram-binning-isotonic-regression-platt-scaling-temperature-scaling">Calibration,expected calibration error (ECE), maximum calibration error (MCE), Histogram binning, Isotonic regression, Platt scaling, Temperature scaling</span></h3><p>We say that a model is wellcalibrated if its confidence coincides with its accuracy across many predictions. Compare within each bin, how often the model thought the inputs belonged to the class (confidence) with how often the inputs actually belonged to the class (frequency).</p>
<h1><span id="sequential-decision-making">Sequential Decision-Making</span></h1><h2><span id="active-learning">Active Learning</span></h2><h3><span id="conditional-entropyjoint-entropy">Conditional Entropy,Joint entropy</span></h3><p>A very intuitive property of entropy is its monotonicity: when conditioning on additional observations the entropy can never increase.</p>
<h3><span id="mutual-informationinformation-gain-the-law-of-total-expectation-data-processing-inequality-interaction-informationsynergy-and-redundancy-submodularity-of-mutual-information-marginal-gain-submodularity-monotone">Mutual Information(information gain), the law of total expectation, data processing inequality, interaction information(Synergy and Redundancy), Submodularity of Mutual Information, Marginal gain, Submodularity, monotone,</span></h3><p>data processing inequality: which says that processing cannot increase the information contained in a signal.</p>
<p>F is submodular iff “adding” x to the smaller set A yields more marginal gain than adding x to the larger set B.</p>
<p>I is monotone submodular.</p>
<h3><span id="maximizing-mutual-information-greedy-submodular-function-maximization-uncertainty-sampling-marginal-gain-of-maximizing-mutual-informationbayesian-active-learning-by-disagreement-bald">Maximizing Mutual Information, Greedy submodular function maximization, Uncertainty Sampling, Marginal gain of maximizing mutual information,Bayesian active learning by disagreement (BALD)</span></h3><p>skip proof of Theorem 8.15</p>
<p>Therefore, if f is modeled by a Gaussian and we assume homoscedastic noise, greedily maximizing mutual information corresponds to simply picking the point x with the largest variance. This algorithm is also called uncertainty sampling.</p>
<h3><span id="experimental-design-entropy-search">Experimental Design, Entropy Search</span></h3><p>skip</p>
<h2><span id="bayesian-optimization">Bayesian Optimization</span></h2><h3><span id="exploration-exploitation-dilemmaonline-learning-and-bandits-multi-armed-bandits-regret-sublinear-regret-cesàro-mean">Exploration-Exploitation Dilemma,Online Learning and Bandits, Multi-Armed Bandits, Regret, sublinear regret, Cesàro mean</span></h3><p>Bayesian optimization can be interpreted as a variant of the MAB problem where there can be a potentially infinite number of actions (arms), but their rewards are correlated (because of the smoothness of the Gaussian process prior).</p>
<h3><span id="acquisition-functions-upper-confidence-bound-bayesian-confidence-intervals-regret-of-gp-ucb-information-gain-of-common-kernels-frequentist-confidence-intervals-probability-of-improvement-expected-improvement-ei-thompson-sampling-probability-matching-information-directed-sampling">Acquisition Functions, Upper Confidence Bound, Bayesian confidence intervals, Regret of GP-UCB, Information gain of common kernels, Frequentist confidence intervals, probability of improvement, expected improvement (EI), Thompson Sampling, probability matching, Information-Directed Sampling</span></h3><p>skip Information-Directed Sampling</p>
<h2><span id="markov-decision-processes">Markov Decision Processes</span></h2><p>Planning deals with the problem of deciding which action an agent should play in a (stochastic) environment. A key formalism for probabilistic planning in known environments are so-called Markov decision processes.</p>
<h3><span id="policydiscounted-payoffstate-value-functionstate-action-value-function-also-called-q-function">Policy,discounted payoff,state value function,state-action value function (also called Q-function)</span></h3><p>A policy is a function that maps each state x ∈ X to a probability distribution over the actions.</p>
<h3><span id="bellman-expectation-equation">Bellman Expectation Equation</span></h3><h3><span id="policy-evaluation-fixed-point-iteration-contractionbanach-fixed-point-theorem">Policy Evaluation, Fixed-point Iteration, contraction,Banach fixed-point theorem</span></h3><h3><span id="policy-optimization-greedy-policies-bellman-optimality-equation-bellmans-theorem-policy-iteration-value-iteration">Policy Optimization, Greedy Policies, Bellman Optimality Equation, Bellman’s theorem, Policy Iteration, Value Iteration,</span></h3><p>In particular, if for every state there is a unique action that maximizes the state-action value function, the policy π⋆ is deterministic and unique.</p>
<p>Intuitively, the Bellman optimality equations express that the value of a state under an optimal policy must equal the expected return for the best action from that state.</p>
<p>Value iteration converges to an ε-optimal solution in a polynomial number of iterations. Unlike policy iteration, value iteration does not converge to an exact solution in general.</p>
<h3><span id="partial-observability">Partial Observability,</span></h3><p>Whereas MDPs are controlled Markov chains, POMDPs are controlled hidden Markov models.</p>
<h2><span id="tabular-reinforcement-learning">Tabular Reinforcement Learning</span></h2><h3><span id="trajectories-episodic-setting-continuous-setting-on-policy-and-off-policy-methods">Trajectories, episodic setting, continuous setting, On-policy and Off-policy Methods,</span></h3><p>on-policy methods are used when the agent has control over its own actions, in other words, the agent can freely choose to follow any policy. off-policy methods can be used even when the agent cannot freely choose its actions.</p>
<h3><span id="model-based-approaches-balancing-exploration-and-exploitation-ε-greedy-softmax-explorationboltzmann-exploration-rmax-algorithm">Model-based Approaches, Balancing Exploration and Exploitation, ε-greedy, Softmax Exploration(Boltzmann exploration), Rmax algorithm</span></h3><p>skip Remark 11.3: Asymptotic convergence</p>
<p>Note that ε-greedy is GLIE with probability 1 if the sequence (εt)t∈N0 satisfies the RM-conditions (A.56), e.g., if εt = 1/t.</p>
<p>A significant benefit to model-based reinforcement learning is that it is inherently off-policy. That is, any trajectory regardless of the policy used to obtain it can be used to improve the model of the underlying Markov decision process. In the model-free setting, this not necessarily true.</p>
<h3><span id="model-free-approaches-on-policy-value-estimation-bootstrapping-temporal-difference-learning-sarsa-off-policy-value-estimation-q-learning-optimistic-q-learning">Model-free Approaches, On-policy Value Estimation, bootstrapping, temporal-difference learning, SARSA, Off-policy Value Estimation, Q-learning, optimistic Q-learning</span></h3><h2><span id="model-free-reinforcement-learning">Model-free Reinforcement Learning</span></h2><h3><span id="value-function-approximation-dqn-experience-replay-maximization-bias-double-dqn">Value Function Approximation, DQN, experience replay, maximization bias, Double DQN</span></h3><h3><span id="policy-approximationpolicy-gradient-methods-policy-value-function-policy-gradient-score-gradient-estimator-score-gradients-with-baselines-downstream-returns-reinforce-algorithm">Policy Approximation(policy gradient methods),  policy value function, Policy Gradient, Score gradient estimator, Score gradients with baselines, Downstream returns, REINFORCE algorithm</span></h3><p>The main advantage of policy gradient methods such as REINFORCE is that they can be used in continuous action spaces. However, REINFORCE is not guaranteed to find an optimal policy. Even when operating in very small domains, REINFORCE can get stuck in local optima.</p>
<h3><span id="on-policy-actor-criticsadvantage-function-policy-gradient-theorem-actor-criticq-actor-criticonline-actor-critic-advantage-actor-critic-bias-variance-tradeoff-trust-region-policy-optimization-trpo-proximal-policy-optimization-ppo">On-policy Actor-Critics,Advantage Function, Policy Gradient Theorem, Actor-Critic(Q actor-critic,Online actor-critic), advantage actor-critic, bias-variance tradeoff, trust-region policy optimization (TRPO), Proximal policy optimization (PPO)</span></h3><p>One approach in the online setting (i.e., non-episodic setting), is to simply use SARSA for learning the critic. To learn the actor, we use stochastic gradient descent with gradients obtained using single samples.</p>
<h3><span id> </span></h3><h2><span id="model-based-reinforcement-learning">Model-based Reinforcement Learning</span></h2><h3><span id="planning-over-finite-horizons-receding-horizon-control-rhc-model-predictive-control-mpc-random-shooting-methods">Planning over Finite Horizons, receding horizon control (RHC), model predictive control (MPC), Random shooting methods</span></h3><h2><span id="cheat-sheet">cheat sheet</span></h2><h3><span id="from-book">from book</span></h3><p>conditional distribution for gaussian;<br>Gaussian process posterior;<br>the predictive posterior at the test point;<br>common kernels;<br>the Hessian of the logistic loss;<br>Surprise,entropy,Jensen’s Inequality,Cross-entropy,KL-divergence,<br>ELBO,<br>the law of large numbers and Hoeffding’s inequality,<br>Hoeffding bound, </p>
<h3><span id="from-exercise">from exercise</span></h3><p> Woodbury push-through identity;<br> Solution to problem 3.6;</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>pai - review notes</p><p><a href="http://yoursite.com/2024/01/25/pai11/">http://yoursite.com/2024/01/25/pai11/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>s-serenity</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2024-01-25</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2024-02-03</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/machine-learning/">machine learning</a></div><div class="notification is-danger">You need to set <code>install_url</code> to use ShareThis. Please set it in <code>_config.yml</code>.</div></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button donate" href="/" target="_blank" rel="noopener" data-type="afdian"><span class="icon is-small"><i class="fas fa-charging-station"></i></span><span>Afdian.net</span></a><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>Alipay</span><span class="qrcode"><img src="/" alt="Alipay"></span></a><a class="button donate" href="/" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>Buy me a coffee</span></a><a class="button donate" href="/" target="_blank" rel="noopener" data-type="patreon"><span class="icon is-small"><i class="fab fa-patreon"></i></span><span>Patreon</span></a><div class="notification is-danger">You forgot to set the <code>business</code> or <code>currency_code</code> for Paypal. Please set it in <code>_config.yml</code>.</div><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>Wechat</span><span class="qrcode"><img src="/" alt="Wechat"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2024/02/21/llm0/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Measuring sentence similarity</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2024/01/03/bigdata13/"><span class="level-item">bigdata - review notes</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">Comments</h3><div class="notification is-danger">You forgot to set the <code>shortname</code> for Disqus. Please set it in <code>_config.yml</code>.</div></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/tx.PNG" alt="s-serenity"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">s-serenity</p><p class="is-size-6 is-block">student</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Shanghai, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">81</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">9</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">21</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/s-serenity" target="_blank" rel="noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/s-serenity"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://github.com/s-serenity" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Github</span></span><span class="level-right"><span class="level-item tag">github.com</span></span></a></li></ul></div></div></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">Categories</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/algorithm/"><span class="level-start"><span class="level-item">algorithm</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/computer-system/"><span class="level-start"><span class="level-item">computer system</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/database/"><span class="level-start"><span class="level-item">database</span></span><span class="level-end"><span class="level-item tag">13</span></span></a></li><li><a class="level is-mobile" href="/categories/development-tools/"><span class="level-start"><span class="level-item">development tools</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/others/"><span class="level-start"><span class="level-item">others</span></span><span class="level-end"><span class="level-item tag">4</span></span></a></li><li><a class="level is-mobile" href="/categories/practice/"><span class="level-start"><span class="level-item">practice</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/programming-language/"><span class="level-start"><span class="level-item">programming language</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/categories/theory/"><span class="level-start"><span class="level-item">theory</span></span><span class="level-end"><span class="level-item tag">24</span></span></a></li><li><a class="level is-mobile" href="/categories/web-development/"><span class="level-start"><span class="level-item">web development</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-07-04T13:14:44.000Z">2024-07-04</time></p><p class="title"><a href="/2024/07/04/llm7/">Important papers</a></p><p class="categories"><a href="/categories/theory/">theory</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-07-04T12:57:56.000Z">2024-07-04</time></p><p class="title"><a href="/2024/07/04/llm6/">finetuning large language models</a></p><p class="categories"><a href="/categories/theory/">theory</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-05-18T15:08:37.000Z">2024-05-18</time></p><p class="title"><a href="/2024/05/18/llm5/">Batch processing for sequences</a></p><p class="categories"><a href="/categories/theory/">theory</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-05-18T13:17:07.000Z">2024-05-18</time></p><p class="title"><a href="/2024/05/18/llm4/">Tokenizers</a></p><p class="categories"><a href="/categories/theory/">theory</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-05-01T13:56:46.000Z">2024-05-01</time></p><p class="title"><a href="/2024/05/01/llm3/">Natural Language Inference(Recognizing Textual Entailment)</a></p><p class="categories"><a href="/categories/practice/">practice</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2024/"><span class="level-start"><span class="level-item">2024</span></span><span class="level-end"><span class="level-item tag">10</span></span></a></li><li><a class="level is-mobile" href="/archives/2023/"><span class="level-start"><span class="level-item">2023</span></span><span class="level-end"><span class="level-item tag">43</span></span></a></li><li><a class="level is-mobile" href="/archives/2022/"><span class="level-start"><span class="level-item">2022</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/archives/2021/"><span class="level-start"><span class="level-item">2021</span></span><span class="level-end"><span class="level-item tag">17</span></span></a></li><li><a class="level is-mobile" href="/archives/2020/"><span class="level-start"><span class="level-item">2020</span></span><span class="level-end"><span class="level-item tag">5</span></span></a></li><li><a class="level is-mobile" href="/archives/2019/"><span class="level-start"><span class="level-item">2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2018/"><span class="level-start"><span class="level-item">2018</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Algorithm/"><span class="tag">Algorithm</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Elasticsearch/"><span class="tag">Elasticsearch</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux/"><span class="tag">Linux</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Machine-Learning/"><span class="tag">Machine Learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Markdown/"><span class="tag">Markdown</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/NLP/"><span class="tag">NLP</span><span class="tag">10</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SQL/"><span class="tag">SQL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/algorithm/"><span class="tag">algorithm</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/big-data/"><span class="tag">big data</span><span class="tag">13</span></a></div><div class="control"><a class="tags has-addons" href="/tags/computer-vision/"><span class="tag">computer vision</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/deep-learning/"><span class="tag">deep learning</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/git/"><span class="tag">git</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/graph/"><span class="tag">graph</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/hexo/"><span class="tag">hexo</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/java/"><span class="tag">java</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/machine-learning/"><span class="tag">machine learning</span><span class="tag">18</span></a></div><div class="control"><a class="tags has-addons" href="/tags/probability/"><span class="tag">probability</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python/"><span class="tag">python</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/reinforcement-learning/"><span class="tag">reinforcement learning</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/speech/"><span class="tag">speech</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/tools/"><span class="tag">tools</span><span class="tag">1</span></a></div></div></div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">Subscribe for updates</h3><form action="https://feedburner.google.com/fb/a/mailverify" method="post" target="popupwindow" onsubmit="window.open(&#039;https://feedburner.google.com/fb/a/mailverify?uri=&#039;,&#039;popupwindow&#039;,&#039;scrollbars=yes,width=550,height=520&#039;);return true"><input type="hidden" value="" name="uri"><input type="hidden" name="loc" value="en_US"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div><div class="card widget"><div class="card-content"><div class="notification is-danger">You need to set <code>client_id</code> and <code>slot_id</code> to show this AD unit. Please set it in <code>_config.yml</code>.</div></div></div><div class="card widget" data-type="subscribe-email"><div class="card-content"><div class="menu"><h3 class="menu-label">follow.it</h3><form action="" method="post" target="_blank"><div class="field has-addons"><div class="control has-icons-left is-expanded"><input class="input" name="email" type="email" placeholder="Email"><span class="icon is-small is-left"><i class="fas fa-envelope"></i></span></div><div class="control"><input class="button" type="submit" value="Subscribe"></div></div></form></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.svg" alt="s-serenity" height="28"></a><p class="is-size-7"><span>&copy; 2024 s-serenity</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2019</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>